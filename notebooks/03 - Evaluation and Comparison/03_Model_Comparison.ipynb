{"cells":[{"cell_type":"markdown","metadata":{"id":"MocEEl6evMpO"},"source":["# Model Comparison"]},{"cell_type":"markdown","metadata":{"id":"hq6JnUhcvUFi"},"source":["This file contains all comparisons on the model outputs and was developed by Marc C. Hennig (mhennig@hm.edu)."]},{"cell_type":"markdown","metadata":{"id":"TQbWuHdWhhbN"},"source":["# Environment"]},{"cell_type":"markdown","metadata":{"id":"LKQT5Dhuhni0"},"source":["## Dependency installation"]},{"cell_type":"markdown","metadata":{"id":"RwEddQ2GrlRy"},"source":["### A. PIP Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0luyK5STpuf"},"outputs":[],"source":["!pip install ipdb\n","!pip freeze > requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"HBG__Y3whra8"},"source":["## Dependency Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3h6Sq8RoTiLC"},"outputs":[],"source":["# Python dependencies\n","import os\n","import sys\n","import re\n","import math\n","import datetime\n","import random\n","import json\n","import time\n","import shutil\n","import warnings\n","import functools\n","from pathlib import Path\n","from typing import List, Tuple, Union, Optional, Literal, Callable, Dict\n","from collections import namedtuple\n","from enum import Enum\n","\n","# Debugging\n","import ipdb\n","\n","# Colab dependencies\n","from google.colab import files, drive, output\n","\n","# Basic dependencies\n","import pandas as pd\n","import numpy as np\n","import scipy as sp\n","import statsmodels as sm\n","import statsmodels.api\n","import statsmodels.stats\n","import statsmodels.stats.contingency_tables\n","\n","# Plotting dependencies\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","# Machine learning depenencies\n","import sklearn as sl\n","import sklearn.metrics"]},{"cell_type":"markdown","metadata":{"id":"jSaBMrqDhuKF"},"source":["## Variables & Global Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7Tg7Nrrh1zG"},"outputs":[],"source":["# Assign a random seed for reproduceability\n","RANDOM_STATE = 1337\n","\n","os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n","random.seed(RANDOM_STATE)\n","np.random.seed(RANDOM_STATE)\n","\n","# Show all Pandas columns\n","pd.set_option(\"display.max_columns\", None)\n","\n","# Set Matplotlib and Seaborn color scheme\n","plt.rcParams[\"image.cmap\"] = \"Blues\"\n","sns.set_palette(\"Blues\")\n","\n","# Colab settings\n","output.enable_custom_widget_manager()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eh37EJKuirTG"},"outputs":[],"source":["# Google Drive folders\n","GDRIVE_INPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/TGN-AST/Eventlogs\"\n","GDRIVE_OUTPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/TGN-AST/Results\"\n","\n","# Local Colab folders\n","UTIL_DIR = os.path.join(\".\", \"Util\")\n","DATA_DIR = os.path.join(\".\", \"Data\")\n","INPUT_DATA_DIR = os.path.join(DATA_DIR, \"Input\")\n","INPUT_DATA_BPIC2013_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2013\")\n","INPUT_DATA_BPIC2014_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2014\")\n","INPUT_DATA_BPIC2015_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2015\")\n","INPUT_DATA_RESULT_DIR = os.path.join(INPUT_DATA_DIR, \"Results\")\n","INTERIM_DATA_DIR = os.path.join(DATA_DIR, \"Interim\")\n","OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"Output\")\n","\n","GRAPHIC_DIR = os.path.join(\".\", \"Graphics\")\n","MODEL_DIR = os.path.join(\".\", \"Models\")\n","\n","Path(DATA_DIR).mkdir(exist_ok=True)\n","Path(INTERIM_DATA_DIR).mkdir(exist_ok=True)\n","Path(OUTPUT_DATA_DIR).mkdir(exist_ok=True)\n","Path(GRAPHIC_DIR).mkdir(exist_ok=True)\n","Path(MODEL_DIR).mkdir(exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"3kDkI1AkTKNY"},"source":["## Common Functions"]},{"cell_type":"markdown","metadata":{"id":"-w25bjIuz_YN"},"source":["### Cleaning & Formatting Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWFMaStGWnnc"},"outputs":[],"source":["EVENTLOG_CASE = \"case:concept:name\"\n","EVENTLOG_ACTIVITY = \"concept:name\"\n","EVENTLOG_TIMESTAMP = \"time:timestamp\"\n","EVENTLOG_GROUP = \"org:group\"\n","EVENTLOG_RESOURCE = \"org:resource\"\n","EVENTLOG_ROLE = \"org:role\"\n","EVENTLOG_CASE_PREFIX = \"case:\"\n","EVENTLOG_LABEL_PREFIX = \"label:\"\n","\n","EVENTLOG_LABEL_REM_TIME = f\"{EVENTLOG_LABEL_PREFIX}time:timestamp:last\"\n","EVENTLOG_LABEL_NEXT_ACT = f\"{EVENTLOG_LABEL_PREFIX}concept:name:next\"\n","EVENTLOG_LABEL_NEXT_TIME = f\"{EVENTLOG_LABEL_PREFIX}time:timestamp:next\"\n","\n","EVENTLOG_FEAT_TIME_OF_YEAR_SUFFIX = \":timeofyear\"\n","EVENTLOG_FEAT_TIME_OF_MONTH_SUFFIX = \":timeofmonth\"\n","EVENTLOG_FEAT_TIME_OF_WEEK_SUFFIX = \":timeofweek\"\n","EVENTLOG_FEAT_TIME_OF_DAY_SUFFIX = \":timeofday\"\n","EVENTLOG_FEAT_TIME_ELAPSED_CYCLE_SUFFIX = \":elapsedcycle\"\n","EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX = \":elapsedprev\"\n","\n","TOKEN_PADDING = \"[PAD]\"\n","TOKEN_PADDING_NUM = 0\n","TOKEN_NA = \"[NA]\"\n","TOKEN_EOC = \"[EOC]\"\n","\n","def df_find_case_attributes(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, label_prefix: str = EVENTLOG_LABEL_PREFIX, exclude_labels: bool = False) -> List[str]:\n","  \"\"\"\n","  Identifies and returns a list of attributes from the DataFrame that have a constant value within each case. Attributes are considered 'case attributes' if they have the same single value for all rows associated with a given case.\n","\n","  Parameters:\n","    df (pd.DataFrame): A pandas DataFrame containing the event log data.\n","    case_col (str, optional): The name of the column in df that represents the case identifier. Defaults to `EVENTLOG_CASE`.\n","    label_prefix (str, optional): The prefix used to identify label columns within df. Defaults to `EVENTLOG_LABEL_PREFIX`.\n","    exclude_labels (bool, optional): If True, attributes that are considered labels (i.e., start with label_prefix) will be excluded from the result.\n","\n","  Returns:\n","    List[str]: A list of case attributes that are constant within each case. If `exclude_labels` is set to True, attributes considered as labels will not be included in the list.\n","  \"\"\"\n","  attrs = df.groupby(case_col).agg('nunique', dropna=False).agg('max', axis='rows')\n","  attrs = attrs.where(attrs == 1).dropna().index.to_list()\n","  if exclude_labels:\n","    attrs = [attr for attr in attrs if attr not in df_find_labels(df, label_prefix)]\n","  return attrs\n","\n","def df_find_event_attributes(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, label_prefix: str = EVENTLOG_LABEL_PREFIX, exclude_labels: bool = False) -> List[str]:\n","  \"\"\"\n","  Identifies and returns a list of attributes from the DataFrame that have varying values across events within the same case. Attributes are considered 'event attributes' if they do not have the same single value for all rows associated with a given case  (i.e., their value varies across events within the same case).\n","\n","  Parameters:\n","    df (pd.DataFrame): A pandas DataFrame containing the event log data.\n","    case_col (str, optional): The name of the column in df that represents the case identifier. Defaults to `EVENTLOG_CASE`.\n","    label_prefix (str, optional): The prefix used to identify label columns within df. Defaults to `EVENTLOG_LABEL_PREFIX`.\n","    exclude_labels (bool, optional): If True, attributes that are considered labels (i.e., start with label_prefix) will be excluded from the result.\n","\n","  Returns:\n","    List[str]: A list of event attributes that have more than one unique value within each case. If `exclude_labels` is set to True, attributes considered as labels will not be included in the list.\n","  \"\"\"\n","  attrs = df.groupby(case_col).agg('nunique', dropna=False).agg('max', axis='rows')\n","  attrs = attrs.where(attrs > 1).dropna().index.to_list()\n","  if exclude_labels:\n","    attrs = [attr for attr in attrs if attr not in df_find_labels(df, label_prefix)]\n","  return attrs\n","\n","\n","\n","def df_find_labels(df: pd.DataFrame, label_prefix: str = EVENTLOG_LABEL_PREFIX) -> List[str]:\n","  \"\"\"\n","  Identifies and returns a list of column names that are considered labels in the DataFrame based on a given prefix. Label columns are those that start with the specified `label_prefix`.\n","\n","  Parameters:\n","    df (pd.DataFrame): A pandas DataFrame containing the event log data or similar structured data.\n","    label_prefix (str, optional): The prefix used to identify label columns within df. Defaults to `EVENTLOG_LABEL_PREFIX`.\n","\n","  Returns:\n","    List[str]: A list of column names that are identified as labels based on the prefix.\n","  \"\"\"\n","  return [col for col in df.columns if col.startswith(label_prefix)]\n","\n","def df_separate_categoricals(df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n","  ordered_cols = []\n","  unordered_cols = []\n","  for col in df.select_dtypes(include='category').columns:\n","    if df[col].cat.ordered:\n","      ordered_cols.append(col)\n","    else:\n","      unordered_cols.append(col)\n","  return ordered_cols, unordered_cols\n","\n","def df_convert_datetimes(df: pd.DataFrame, cols: List[str] = [], dayfirst: bool = False, yearfirst: bool = False, tz: Optional[Union[str, datetime.tzinfo]] = None) -> pd.DataFrame:\n","  \"\"\"\n","  Converts specified columns of a DataFrame to datetime format and localizes the datetime objects to the specified timezone if provided. Attempts to parse the columns as date times, optionally interpreting the day first or year first. If the initial parsing fails, it retries with the assumption that the datetime is in UTC. After conversion, the datetime objects may be localized to a specific timezone if `tz` is not None.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame containing columns to be converted to datetime.\n","    cols (List[str], optional): The list of column names to convert to datetime. Defaults to an empty list.\n","    dayfirst (bool, optional): Boolean indicating if the day is the first number in the date string. Defaults to False.\n","    yearfirst (bool, optional): Boolean indicating if the year is the first number in the date string. Defaults to False.\n","    tz (Optional[Union[str, datetime.tzinfo]], optional): Optional timezone information to which the datetimes will be localized. Defaults to None.\n","\n","  Returns:\n","    pd.DataFrame: The DataFrame with the specified columns converted to datetime format.\n","\n","  Raises:\n","    ValueError: If parsing the dates fails even after assuming UTC.\n","  \"\"\"\n","  for col in cols:\n","    try:\n","      df[col] = pd.to_datetime(df[col], dayfirst=dayfirst, yearfirst=yearfirst)\n","    except ValueError:\n","      df[col] = pd.to_datetime(df[col], dayfirst=dayfirst, yearfirst=yearfirst, utc=True)\n","\n","    df[col] = df[col].dt.tz_localize(tz=tz)\n","  return df\n","\n","def df_convert_timedeltas(df: pd.DataFrame, cols: List[str] = [], unit: str = 'nanoseconds') -> pd.DataFrame:\n","  \"\"\"\n","  Converts specified columns of a DataFrame to timedelta format using the given time unit. Each value in the specified columns will be converted into a timedelta object, interpreting the value according to the specified unit.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame containing columns to be converted to timedelta.\n","    cols (List[str], optional): The list of column names to convert to timedelta. Defaults to an empty list.\n","    unit (str, optional): The time unit to interpret the values in `cols` when converting. Defaults to 'nanoseconds'.\n","\n","  Returns:\n","    pd.DataFrame: The DataFrame with the specified columns converted to timedelta format.\n","  \"\"\"\n","  for col in cols:\n","    df[col] = pd.to_timedelta(df[col], unit=unit)\n","  return df\n","\n","def df_convert_bools(df: pd.DataFrame, cols: list[str] = [], true_vals: Union[str, List[str]] = [], false_vals: Union[str, List[str]] = []) -> pd.DataFrame:\n","  \"\"\"\n","  Converts specified columns of a DataFrame to boolean format based on provided true and false values. String values from `true_vals` are mapped to `True`, while values from `false_vals` are mapped to `False`. All other values not included in `true_vals` or `false_vals` will be converted based on the presence of either list; if only `true_vals` is provided, all other values are considered `False`, and vice-versa.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame containing columns to be converted to boolean.\n","    cols (List[str], optional): The list of column names to convert to boolean. Defaults to an empty list.\n","    true_vals (Union[str, List[str]], optional): Values to be mapped to `True`. Can be a single string or a list of strings. Defaults to an empty list.\n","    false_vals (Union[str, List[str]], optional): Values to be mapped to `False`. Can be a single string or a list of strings. Defaults to an empty list.\n","\n","  Returns:\n","    pd.DataFrame: The DataFrame with the specified columns converted to boolean format.\n","\n","  \"\"\"\n","  if isinstance(true_vals, str):\n","    true_vals = [true_vals]\n","  if isinstance(false_vals, str):\n","    false_vals = [false_vals]\n","\n","  map = {true_val: True for true_val in true_vals} | {false_val: False for false_val in false_vals}\n","  if len(true_vals) == 0 and len(false_vals) > 0:\n","    # Replace unknown values with True\n","    map['__missing__'] = True\n","  elif len(false_vals) == 0 and len(true_vals) > 0:\n","    # Replace unknown values with False\n","    map['__missing__'] = False\n","  else:\n","    map[\"__missing__\"] = pd.NA\n","\n","  for col in cols:\n","    df[col] = df[col].map(map).astype('boolean')\n","\n","  return df\n","\n","def df_convert_bool_to_int(df: pd.DataFrame, cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n","  df = df.copy()\n","  if cols is None:\n","    cols = df.select_dtypes('boolean').columns\n","  elif isinstance(cols, str):\n","    cols = [cols]\n","\n","  for col in cols:\n","    df[col] = df[col].astype('Int8')\n","\n","  return df\n","\n","def df_convert_ordered_cat_to_int(df: pd.DataFrame, cols: Optional[Union[str, List[str]]] = None, relative: bool = False) -> pd.DataFrame:\n","  df = df.copy()\n","  if cols is None:\n","    cols, _ = df_separate_categoricals(df)\n","  elif isinstance(cols, str):\n","    cols = [cols]\n","\n","  for col in cols:\n","    max_code = df[col].cat.codes.astype('Int16').max()\n","    na_rows = df[df[col].isna()].index\n","\n","    df[col] = df[col].cat.codes.astype('Int16')\n","    df.loc[na_rows, col] = pd.NA\n","    if relative:\n","      df[col] = df[col] / max_code\n","\n","\n","\n","  return df\n","\n","def df_fillna_str(df: pd.DataFrame, val: str, cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n","  df = df.copy()\n","  if cols is None:\n","    cols = df.select_dtypes('object').columns\n","  elif isinstance(cols, str):\n","    cols = [cols]\n","\n","  for col in cols:\n","    df[col] = df[col].astype('string').fillna(val)\n","\n","  return df\n","\n","def df_fillna_num(df: pd.DataFrame, val: Union[int, float], cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n","  df = df.copy()\n","  if cols is None:\n","    cols = df.select_dtypes('number').columns\n","  elif isinstance(cols, str):\n","    cols = [cols]\n","\n","  for col in cols:\n","    df[col] = df[col].fillna(val)\n","\n","  return df\n","\n","def df_fillna_cat(df: pd.DataFrame, val: str, cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n","  df = df.copy()\n","  if cols is None:\n","    cols = df.select_dtypes('category').columns\n","  elif isinstance(cols, str):\n","    cols = [cols]\n","\n","  for col in cols:\n","    if val not in df[col].cat.categories.array:\n","      df[col] = df[col].cat.add_categories(val)\n","    df[col] = df[col].fillna(val)\n","\n","  return df\n","\n","def df_rename_cat_values(df: pd.DataFrame, cols: Union[str, List[str]], from_cats: Union[str, List[str]], to_cat: str) -> pd.DataFrame:\n","  \"\"\"\n","  Changes one or more categorical values to another specified value within the provided columns, and removes any categories that are no longer used.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame containing categorical columns where values will be renamed.\n","    cols (Union[str, List[str]]): A column name or list of column names to be modified.\n","    from_cats (Union[str, List[str]]): The category or list of categories to be changed.\n","    to_cat (str): The new category value that replaces the `from_cats`.\n","\n","  Returns:\n","    pd.DataFrame: The DataFrame with renamed categorical values and cleaned categories.\n","\n","  Raises:\n","    TypeError: If the columns specified are not categorical dtype.\n","  \"\"\"\n","  if isinstance(cols, str):\n","    cols = [cols]\n","  for col in cols:\n","    df[col] = df[col].cat.remove_categories(from_cats).fillna(to_cat)\n","    df[col] = df[col].cat.remove_unused_categories()\n","\n","  return df\n","\n","def df_drop_duplicate_rows(df: pd.DataFrame, inplace: bool = True, ignore_index: bool = True, keep: str = 'first') -> Union[pd.DataFrame, None]:\n","  \"\"\"\n","  Removes duplicate rows from the DataFrame, optionally updating the DataFrame in place and resetting the index.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame from which duplicate rows will be removed.\n","    inplace (bool, optional): If True, the DataFrame will be updated in place and None will be returned. Otherwise, a new DataFrame is returned. Defaults to True.\n","    ignore_index (bool, optional): If True, the index will be reset to the default integer index after dropping duplicates. Otherwise, the original index will be preserved. This parameter is ignored when inplace is True. Defaults to True.\n","    keep (str, optional): Determines which duplicates (if any) to keep.\n","        - 'first': Drop duplicates except for the first occurrence.\n","        - 'last': Drop duplicates except for the last occurrence.\n","        - False: Drop all duplicates.\n","        Defaults to 'first'.\n","\n","  Returns:\n","    Union[pd.DataFrame, None]: The DataFrame with duplicate rows removed if inplace is set to False, otherwise None.\n","  \"\"\"\n","  df = df.drop_duplicates(keep=keep, ignore_index=ignore_index, inplace=inplace)\n","  return df\n","\n","def df_drop_duplicate_cols(df: pd.DataFrame, keep: str = 'first') -> pd.DataFrame:\n","  \"\"\"\n","  Removes duplicate columns from the DataFrame while keeping the first occurrence by default. It also ensures that the data types of the remaining columns are preserved.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame from which duplicate columns will be removed.\n","    keep (str, optional): Determines which duplicates (if any) to keep.\n","        - 'first': Drop duplicates except for the first occurrence.\n","        - 'last': Drop duplicates except for the last occurrence.\n","        - False: Drop all duplicates.\n","        Defaults to 'first'.\n","\n","  Returns:\n","    pd.DataFrame: A new DataFrame with duplicate columns removed and original data types intact.\n","  \"\"\"\n","  dtypes = df.dtypes\n","  df = df.T.drop_duplicates(keep=keep).T\n","  dtypes.drop(dtypes.index[~dtypes.index.isin(df.columns)], inplace=True)\n","  return df.astype(dtypes)\n","\n","# Remove rows and columns that are completely empty\n","def df_drop_na_rows_and_cols(df: pd.DataFrame, inplace: bool = True) -> Union[pd.DataFrame, None]:\n","  \"\"\"\n","  Removes rows and columns from the DataFrame that are completely empty (all values are NaN).\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame from which completely empty rows and columns will be removed.\n","    inplace (bool, optional): If True, the DataFrame will be updated in place, which modifies the original DataFrame and returns None.\n","                              If False, a new DataFrame is returned with the empty rows and columns removed.\n","                              Defaults to True.\n","\n","  Returns:\n","    Union[pd.DataFrame, None]: None if inplace is True; otherwise, a new DataFrame with empty rows and columns removed.\n","  \"\"\"\n","  if inplace:\n","    df.dropna(how=\"all\", axis='index', inplace=inplace)\n","    df = df.dropna(how=\"all\", axis='columns', inplace=inplace)\n","  else:\n","    df = df.dropna(how=\"all\", axis='index', inplace=inplace).dropna(how=\"all\", axis='columns', inplace=inplace)\n","  return df\n","\n","def df_drop_single_val_cols(df: pd.DataFrame, inplace: bool = True) -> Union[pd.DataFrame, None]:\n","  \"\"\"\n","  Removes columns from the DataFrame that only contain a single unique value.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame from which columns with a single unique value will be removed.\n","    inplace (bool, optional): If True, the operation will be performed inplace and the function will return None.\n","                              If False, a new DataFrame with the specified columns removed will be returned.\n","                              Defaults to True.\n","\n","  Returns:\n","    Union[pd.DataFrame, None]: None if inplace is True; otherwise, a new DataFrame with columns that have a single unique value removed.\n","  \"\"\"\n","  df = df.drop(columns=df.columns[df.nunique(dropna=True) == 1], inplace=inplace)\n","  return df\n","\n","def df_drop_threshold_cols(df: pd.DataFrame, gte: float = sys.float_info.min, lt: float = sys.float_info.max, cols: List[str] = [], absolute: bool = False) -> Union[pd.DataFrame, None]:\n","  \"\"\"\n","  Removes columns from the DataFrame where all values meet a threshold condition. Greater than or equal to `gte` and less than `lt` thresholds can be set, and optionally, absolute value conditions can be considered.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame from which columns will be dropped based on threshold conditions.\n","    gte (float, optional): The 'greater than or equal to' threshold condition. Defaults to the smallest representable float.\n","    lt (float, optional): The 'less than' threshold condition. Defaults to the largest representable float.\n","    cols (List[str], optional): The list of columns to check for the threshold conditions. If empty, all numeric columns will be checked. Defaults to an empty list.\n","    absolute (bool, optional): If True, the absolute values of the column data will be considered for the thresholds. Defaults to False.\n","\n","  Returns:\n","    Union[pd.DataFrame, None]: The modified DataFrame with thresholded columns removed. As per the modification of the function, this will always return a new DataFrame and never None.\n","  \"\"\"\n","  df = df.copy()\n","  if cols is None or len(cols) == 0:\n","    cols = df.select_dtypes('number').columns.to_list()\n","\n","  if absolute:\n","    if gte != sys.float_info.min:\n","      drop_cols = df[cols].mask(df[cols].abs() >= abs(gte)).dropna(axis='columns', how='all').columns.to_list()\n","    elif lt != sys.float_info.max:\n","      drop_cols = df[cols].mask(df[cols].abs() < abs(lt)).dropna(axis='columns', how='all').columns.to_list()\n","  else:\n","    drop_cols = df[cols].mask((df[cols] >= gte) & (df[cols] < lt)).dropna(axis='columns', how='all').columns.to_list()\n","\n","  return df.drop(columns=drop_cols)\n","\n","def df_drop_threshold_na_cols(df: pd.DataFrame, threshold: Union[float, int], inplace: bool = True) -> pd.DataFrame:\n","  \"\"\"\n","  Removes columns from the DataFrame that have NaN values equal to or exceeding the specified threshold.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame from which columns with excessive NaN values will be removed.\n","    threshold (Union[float, int]): The threshold for NaN values (absolute number or percentage). If provided as a float,\n","                                   it is interpreted as a percentage of the total number of rows.\n","    inplace (bool, optional): If True, the DataFrame will be updated in place, and None will be returned.\n","                              If False, a new DataFrame with the specified columns removed will be returned.\n","                              Defaults to True.\n","\n","  Returns:\n","    pd.DataFrame: The DataFrame with columns removed if `inplace` is False. If `inplace` is True, the original DataFrame is modified and the function will return None.\n","\n","  Raises:\n","    ValueError: If the threshold is greater than the size of the DataFrame.\n","  \"\"\"\n","  if isinstance(threshold, float):\n","    threshold = threshold * len(df)\n","\n","  if threshold > len(df):\n","    raise ValueError(f\"Threshold {threshold} must be lower than or equal to the number of rows in the DataFrame {len(df)}\")\n","\n","  df_na = df.isna().sum()\n","  df_na = df_na[df_na >= threshold]\n","\n","  return df.drop(columns=df_na.index.array, inplace=inplace)\n","\n","def df_drop_threshold_rows(df: pd.DataFrame, gte: float = sys.float_info.min, lt: float = sys.float_info.max, cols: List[str] = [], absolute: bool = False) -> Union[pd.DataFrame, None]:\n","  \"\"\"\n","  Drops rows from the DataFrame where all numeric values in specified columns meet threshold conditions of either 'greater than or equal to' (`gte`) or 'less than' (`lt`). Optionally, absolute values can be considered for the thresholds.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame from which rows will be dropped.\n","    gte (float, optional): The 'greater than or equal to' threshold condition. Defaults to the smallest representable float.\n","    lt (float, optional): The 'less than' threshold condition. Defaults to the largest representable float.\n","    cols (List[str], optional): The list of column names to check against the threshold conditions. If empty, all numeric columns will be checked. Defaults to an empty list.\n","    absolute (bool, optional): If True, the absolute values of the data in the columns will be considered when comparing against the thresholds. Defaults to False.\n","\n","  Returns:\n","    pd.DataFrame: A DataFrame with the specified rows dropped.\n","  \"\"\"\n","  df = df.copy()\n","  if cols is None or len(cols) == 0:\n","    cols = df.select_dtypes('number').columns.to_list()\n","\n","  if absolute:\n","    if gte != sys.float_info.min:\n","      drop_rows = df[cols].mask(df[cols].abs() >= abs(gte)).dropna(axis='index', how='all').index.to_list()\n","    elif lt != sys.float_info.max:\n","      drop_rows = df[cols].mask(df[cols].abs() < abs(lt)).dropna(axis='index', how='all').index.to_list()\n","  else:\n","    drop_rows = df[cols].mask((df[cols] >= gte) & (df[cols] < lt)).dropna(axis='index', how='all').index.to_list()\n","\n","  return df.drop(index=drop_rows)\n","\n","def df_format_as_eventlog(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, activity_col: str = EVENTLOG_ACTIVITY, time_col: str = EVENTLOG_TIMESTAMP, resource_col: Optional[str] = None, group_col: Optional[str] = None, role_col: Optional[str] = None, inplace: Optional[bool] = True, sort: Union[bool, str] = True):\n","  col_map = {\n","    case_col: EVENTLOG_CASE,\n","    activity_col: EVENTLOG_ACTIVITY,\n","    time_col: EVENTLOG_TIMESTAMP\n","  }\n","  if resource_col is not None:\n","    col_map[resource_col] = EVENTLOG_RESOURCE\n","  if group_col is not None:\n","    col_map[group_col] = EVENTLOG_GROUP\n","  if role_col is not None:\n","    col_map[role_col] = EVENTLOG_ROLE\n","\n","  if sort and isinstance(sort, str):\n","    sort_cols = [case_col, sort, time_col, activity_col]\n","  else:\n","    sort_cols = [case_col, time_col, activity_col]\n","\n","  case_attrs = df_find_case_attributes(df, case_col)\n","  col_map = col_map | {attr: f\"{EVENTLOG_CASE_PREFIX}{attr}\" for attr in case_attrs if not attr.startswith(EVENTLOG_CASE_PREFIX)}\n","\n","  event_attrs = df_find_event_attributes(df, case_col)\n","  col_map = col_map | {attr: f\"{attr.replace(EVENTLOG_CASE_PREFIX, '', 1)}\" for attr in event_attrs if attr.startswith(EVENTLOG_CASE_PREFIX)}\n","\n","  if sort and inplace:\n","    df.sort_values(by=sort_cols, inplace=inplace, ignore_index=True)\n","  elif sort and not inplace:\n","    df = df.sort_values(by=sort_cols, inplace=inplace, ignore_index=True)\n","\n","  return df.rename(columns=col_map, inplace=inplace)\n","\n","def df_write_files(df: pd.DataFrame, filename: str, index: bool = False, skip_xes: bool = True) -> None:\n","  df.to_csv(f\"{filename}.csv\", index=index)\n","  df.to_pickle(f\"{filename}.pkl.gz\")\n","  try:\n","    if isinstance(df.columns, pd.MultiIndex):\n","      df = df.copy()\n","      df.columns = df.columns.to_flat_index()\n","    df.reset_index().to_feather(f\"{filename}.feather\")\n","  except Exception as e:\n","    print(f\"Skipping feather: {e}\")\n","  if not skip_xes:\n","    pm4py.write_xes(df, f\"{filename}.xes\")\n","\n","def df_datetime_to_numeric(df: pd.DataFrame, cols: Optional[Union[List[str], str]] = None, convert_datetime: Optional[bool] = True, convert_timedelta: Optional[bool] = True) -> pd.DataFrame:\n","  \"\"\"\n","  Converts datetime and timedelta columns in a DataFrame to a numeric representation. Datetime columns are converted to UNIX timestamps, and timedelta columns are converted to total seconds.\n","\n","  Parameters:\n","    df (pd.DataFrame): The DataFrame with columns to be converted.\n","    cols (Optional[Union[List[str], str]], optional): Columns to be converted. If None, all columns are considered. It can be a single column name or a list of column names. Defaults to None.\n","    convert_datetime (Optional[bool], optional): Flag indicating whether to convert datetime columns. Defaults to True.\n","    convert_timedelta (Optional[bool], optional): Flag indicating whether to convert timedelta columns. Defaults to True.\n","\n","  Returns:\n","    pd.DataFrame: A DataFrame with the specified datetime and timedelta columns converted to numeric values.\n","  \"\"\"\n","  df = df.copy()\n","  if cols is None:\n","    cols = df.columns.to_list()\n","  elif isinstance(cols, str):\n","    cols = [cols]\n","\n","  if convert_datetime:\n","    for col in df.select_dtypes(include='datetime').columns.to_list():\n","      if col in cols:\n","        df[col] = df[col].map(pd.Timestamp.timestamp, na_action='ignore')\n","\n","  if convert_timedelta:\n","    for col in df.select_dtypes(include='timedelta').columns.to_list():\n","      if col in cols:\n","        df[col] = df[col].dt.total_seconds()\n","\n","  return df\n","\n","def df_timedelta_to_unit(df: pd.DataFrame, timedelta_col: str, unit: Optional[Literal['days', \"day\", \"d\", \"hours\", \"hour\", \"hr\", \"h\", \"m\", \"minute\", \"min\", \"minutes\", \"t\", \"s\", \"seconds\", \"sec\", \"second\"]], floor: bool = False, na_token: Optional[pd.Timedelta] = pd.NA) -> pd.DataFrame:\n","  if not pd.isna(na_token):\n","    na_token = pd.Timedelta(na_token)\n","    df[timedelta_col].fillna(na_token, inplace=True)\n","\n","  if unit in [\"days\", \"day\", \"d\"]:\n","    df[timedelta_col] = df[timedelta_col].dt.total_seconds() / 60 / 60 / 24\n","  elif unit in [\"hours\", \"hour\", \"hr\", \"h\"]:\n","    df[timedelta_col] = df[timedelta_col].dt.total_seconds() / 60 / 60\n","  elif unit in [\"m\", \"minute\", \"min\", \"minutes\", \"t\"]:\n","    df[timedelta_col] = df[timedelta_col].dt.total_seconds() / 60\n","  elif unit in [\"s\", \"seconds\", \"sec\", \"second\"]:\n","    df[timedelta_col] = df[timedelta_col].dt.total_seconds()\n","  else:\n","    raise ValueError(f\"Invalid timedelta unit {unit}\")\n","\n","  if floor:\n","    df[timedelta_col] = df[timedelta_col].astype('Int64')\n","  return df\n","\n","def df_to_multiindex(df: pd.DataFrame, case_col: str = EVENTLOG_CASE) -> pd.DataFrame:\n","  df = df.copy()\n","  df = df.groupby(case_col).apply(lambda x: x.reset_index(drop=True))\n","  df.columns = pd.MultiIndex.from_arrays([df.columns, np.zeros(len(df.columns), dtype=int)])\n","  return df\n","\n","def df_to_flatindex(df: pd.DataFrame):\n","  df = df.copy()\n","  if df.columns.nlevels > 1:\n","    df.columns = [\"_\".join(map(str, col)) for col in df.columns.to_flat_index()]\n","  return df.reset_index()\n","\n","def np_filter_na(arr: np.ndarray, unsqueeze: bool = False) -> np.ndarray:\n","  if unsqueeze:\n","    return np.array([np.expand_dims(el[~pd.isna(el)], -1) for el in arr], dtype='object')\n","  else:\n","    return np.array([el[~pd.isna(el)] for el in arr], dtype='object')\n"]},{"cell_type":"markdown","metadata":{"id":"Paz01hy80HHR"},"source":["### Statistic & Visualization Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eaIHRDggTNqM"},"outputs":[],"source":["def df_naive_regression_metrics(df_train: pd.DataFrame, df_test: pd.DataFrame, label_col: str, method: Literal['median', 'mean', 'mode'] = 'median', return_df: bool = False) -> Union[Dict[str, float], pd.DataFrame]:\n","  if 'median' == method:\n","    y_pred = df_train[label_col].median()\n","  elif 'mean' == method:\n","    y_pred = df_train[label_col].mean()\n","  elif 'mode' == method:\n","    y_pred = df_train[label_col].mode()\n","  else:\n","    raise ValueError(f\"Unknown method {method}\")\n","\n","  y_pred = np.full(df_test[label_col].size, y_pred)\n","  y_true = df_test[label_col].to_numpy()\n","\n","  return evaluate_regression(y_true, y_pred, return_df=return_df)\n","\n","def df_naive_classification_metrics(df_train: pd.DataFrame, df_test: pd.DataFrame, label_col: str, return_df: bool = False) -> Union[Dict[str, float], pd.DataFrame]:\n","  y_pred = df_train[EVENTLOG_LABEL_NEXT_ACT].mode().iloc[0]\n","\n","  label_enc = sl.preprocessing.LabelEncoder()\n","  label_enc.fit(np.concatenate((df_train[label_col], df_test[label_col]), axis=None))\n","\n","  y_pred = label_enc.transform(np.full(df_test[label_col].size, y_pred))\n","  y_true = label_enc.transform(df_test[label_col])\n","\n","  return evaluate_classification(y_true, y_pred, return_df=return_df)\n","\n","def df_predictive_power_scores(df: pd.DataFrame, label_col: str, variable_cols: Optional[Union[str, List[str]]] = None, datetime_is_numeric: Optional [bool] = False, exclude_labels: Optional[bool] = True, threshold: float = 0.0) -> Tuple[pd.DataFrame, plt.Axes]:\n","  df = df.copy()\n","  if variable_cols is None:\n","    if exclude_labels:\n","      variable_cols = df.columns[~df.columns.str.startswith(EVENTLOG_LABEL_PREFIX) | (df.columns == label_col)].to_list()\n","    else:\n","      variable_cols = df.columns.to_list()\n","  elif isinstance(variable_cols, str):\n","    variable_cols = [variable_cols, label_col]\n","\n","  df = df[variable_cols]\n","\n","  if threshold is None or threshold < 0 or threshold > 1:\n","    raise ValueError(f\"threshold must be in the interval [0, 1] but was {threshold}\")\n","  if datetime_is_numeric:\n","    df = df_datetime_to_numeric(df)\n","\n","  df_predictors = pps.predictors(df, label_col, output=\"df\", sorted=True, catch_errors=False, random_seed=RANDOM_STATE, invalid_score=np.NaN, cross_validation=2)\n","  fig, ax = plt.subplots(figsize=(10,10))\n","  barplot = sns.barplot(data=df_predictors[df_predictors[\"is_valid_score\"] & df_predictors[\"ppscore\"] >= threshold], x=\"ppscore\", y=\"x\", orient=\"h\", ax=ax).set_title(f\"Predictive power score {label_col}\")\n","  return df_predictors, barplot\n","\n","def df_predictive_power_matrix(df: pd.DataFrame, datetime_is_numeric: Optional[bool] = False, threshold: float = 0.0) -> Tuple[pd.DataFrame, plt.Axes]:\n","  df = df.copy()\n","  if threshold is None or threshold < 0 or threshold > 1:\n","    raise ValueError(f\"threshold must be in the interval [0, 1] but was {threshold}\")\n","  if datetime_is_numeric:\n","    df = df_datetime_to_numeric(df)\n","\n","  df_matrix = pps.matrix(df, output=\"df\", sorted=True, catch_errors=False, random_seed=RANDOM_STATE, invalid_score=np.NaN, cross_validation=2)\n","  df_heatmap = df_matrix[df_matrix[\"is_valid_score\"] & df_matrix[\"ppscore\"] >= threshold][['x', 'y', 'ppscore']].pivot(columns='y', index='x', values='ppscore')\n","  fig, ax = plt.subplots(figsize=(30,30))\n","  heatmap = sns.heatmap(df_heatmap, vmin=0, vmax=1, linewidths=0.5, annot=True, ax=ax).set_title(\"Predictive power matrix\")\n","  return df_matrix, heatmap\n","\n","def df_correlation_matrix(df: pd.DataFrame, datetime_is_numeric: Optional[bool] = False, method: str = \"pearson\", threshold: float = 0.0) -> Tuple[pd.DataFrame, plt.Axes]:\n","  df = df.copy()\n","  if threshold is None or threshold < 0 or threshold > 1:\n","    raise ValueError(f\"threshold must be in the interval [0, 1] but was {threshold}\")\n","  if datetime_is_numeric:\n","    df = df_datetime_to_numeric(df)\n","\n","  ordered_cols, unordered_cols = df_separate_categoricals(df)\n","\n","  for ordinal_col in ordered_cols:\n","    df[ordinal_col] = df[ordinal_col].cat.codes\n","\n","  if len(unordered_cols) > 0:\n","    df = df.join(pd.get_dummies(df[unordered_cols], sparse=True)).drop(columns=unordered_cols)\n","\n","  df_corr = df.corr(method=method, numeric_only=True)\n","  fig, ax = plt.subplots(figsize=(50,50))\n","  heatmap = sns.heatmap(df_corr, vmin=-1, vmax=1, annot=True, ax=ax).set_title(f\"{method} correlation\")\n","\n","  return df_corr, heatmap\n","\n","def df_case_length_stats(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, result_col: str = \"Case Length\", percentiles: List[float] = np.arange(.05, 1, .05)) -> pd.DataFrame:\n","  df = df.groupby(case_col).size()\n","  return pd.DataFrame(data={result_col: df.describe(percentiles=percentiles)})\n","\n","def df_case_duration_stats(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, time_col: str = EVENTLOG_TIMESTAMP, result_col: str = \"Case Duration\", percentiles: List[float] = np.arange(.05, 1, .05)) -> pd.DataFrame:\n","  df = df.groupby(case_col)[time_col].max() - df.groupby(case_col)[time_col].min()\n","  return pd.DataFrame(data={result_col: df.describe(percentiles=percentiles)})\n","\n","def df_correlation_scores(df: pd.DataFrame, label_col: str, variable_cols: Optional[Union[str, List[str]]] = None, datetime_is_numeric: Optional [bool] = False, exclude_labels: Optional[bool] = True, method: str = \"pearson\", threshold: float = 0.0) -> Tuple[pd.DataFrame, plt.Axes]:\n","  df = df.copy()\n","  if threshold is None or threshold < 0 or threshold > 1:\n","    raise ValueError(f\"threshold must be in the interval [0, 1] but was {threshold}\")\n","  if datetime_is_numeric:\n","    df = df_datetime_to_numeric(df)\n","  if variable_cols is None or len(variable_cols) == 0:\n","    variable_cols = df.select_dtypes(include=[\"number\", \"category\"]).columns.to_list()\n","\n","  df = df[variable_cols]\n","  ordered_cols, unordered_cols = df_separate_categoricals(df)\n","\n","  for ordinal_col in ordered_cols:\n","    df[ordinal_col] = df[ordinal_col].cat.codes\n","\n","  if len(unordered_cols) > 0:\n","    df = df.join(pd.get_dummies(df[unordered_cols], sparse=True)).drop(columns=unordered_cols)\n","  variable_cols = df.columns.to_list()\n","\n","  df_corr = pd.DataFrame(index=variable_cols, columns=[f\"{method} correlation\"], dtype=\"float\")\n","\n","  for col in variable_cols:\n","    if col == label_col:\n","      continue\n","    df_corr.loc[col][f\"{method} correlation\"] = df[col].dropna().astype(\"float\").corr(df[label_col])\n","\n","  df_corr.sort_values(by=[f\"{method} correlation\"], inplace=True)\n","  fig, ax = plt.subplots(figsize=(100,100))\n","  barplot = sns.barplot(data=df_corr, x=f\"{method} correlation\", y=df_corr.index, orient=\"h\", ax=ax).set_title(label_col)\n","\n","  return df_corr, barplot\n","\n","def df_visualize_strict_temporal_splitting(df_train: pd.DataFrame, df_test: pd.DataFrame, time_col: str = EVENTLOG_TIMESTAMP) -> plt.Axes:\n","  s_months_train = df_train[time_col].dt.to_period('M').value_counts()\n","  s_months_test_before_sep = df_test[df_test[df_find_labels(df_test)].isna().any(axis=1)][time_col].dt.to_period('M').value_counts()\n","  s_months_test_after_sep = df_test[~df_test[df_find_labels(df_test)].isna().any(axis=1)][time_col].dt.to_period('M').value_counts()\n","\n","  df = pd.concat([\n","    s_months_train.rename(\"Training Set Correct\"),\n","    s_months_test_before_sep.rename(\"Training Set Wrong\"),\n","    s_months_test_after_sep.rename(\"Test Set\")\n","  ], axis=1).sort_index().fillna(0)\n","  return df.plot(kind='bar', stacked=True, color=['green', 'red', 'grey'])"]},{"cell_type":"markdown","metadata":{"id":"tpOclu7boQBd"},"source":["### Model Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5HRdWiWHKkT"},"outputs":[],"source":["def evaluate_regression(y_true: np.ndarray, y_pred: np.ndarray, return_df: bool = False) -> Union[Dict[str, float], pd.DataFrame]:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","  if y_pred.ndim > 1:\n","      y_pred = np.argmax(y_pred, axis=1)\n","\n","  def logcosh_error(y_true, y_pred):\n","    error = np.subtract(y_pred, y_true)\n","    return np.mean(np.log(np.cosh(error)), axis=-1)\n","\n","  eval = {\n","    'mae': sl.metrics.mean_absolute_error(y_true, y_pred),\n","    'mse': sl.metrics.mean_squared_error(y_true, y_pred),\n","    'rmse': sl.metrics.root_mean_squared_error(y_true, y_pred),\n","    'mape': sl.metrics.mean_absolute_percentage_error(y_true, y_pred),\n","    'medae': sl.metrics.median_absolute_error(y_true, y_pred),\n","    'logcosh': logcosh_error(y_true, y_pred),\n","    'max_error': sl.metrics.max_error(y_true, y_pred),\n","  }\n","\n","  if y_true.min() >= 0 and y_pred.min() >= 0:\n","    eval.update({\n","      'msle': float(sl.metrics.mean_squared_log_error(y_true, y_pred)),\n","      'rmsle': float(sl.metrics.root_mean_squared_log_error(y_true, y_pred)),\n","    })\n","\n","  if return_df:\n","    return pd.DataFrame.from_dict(eval, orient='index', columns=[\"Value\"])\n","  else:\n","    return eval\n","\n","def evaluate_classification(y_true: np.ndarray, y_pred: np.ndarray, return_df: bool = False) -> Union[Dict[str, float], pd.DataFrame]:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","  if y_pred.ndim > 1:\n","      y_pred = np.argmax(y_pred, axis=1)\n","\n","  eval = {\n","    'accuracy': sl.metrics.accuracy_score(y_true, y_pred),\n","    'accuracy_balanced': sl.metrics.balanced_accuracy_score(y_true, y_pred),\n","    'accuracy_balanced_adjusted': sl.metrics.balanced_accuracy_score(y_true, y_pred, adjusted=True),\n","    'f1_micro': sl.metrics.f1_score(y_true, y_pred, average='micro', zero_division='warn'),\n","    'f1_macro': sl.metrics.f1_score(y_true, y_pred, average='macro', zero_division='warn'),\n","    'f1_weighted': sl.metrics.f1_score(y_true, y_pred, average='weighted', zero_division='warn'),\n","    'precision_micro': sl.metrics.precision_score(y_true, y_pred, average='micro', zero_division='warn'),\n","    'precision_macro': sl.metrics.precision_score(y_true, y_pred, average='macro', zero_division='warn'),\n","    'precision_weighted': sl.metrics.precision_score(y_true, y_pred, average='weighted', zero_division='warn'),\n","    'recall_micro': sl.metrics.recall_score(y_true, y_pred, average='micro'),\n","    'recall_macro': sl.metrics.recall_score(y_true, y_pred, average='macro'),\n","    'recall_weighted': sl.metrics.recall_score(y_true, y_pred, average='weighted'),\n","  }\n","  if return_df:\n","    return pd.DataFrame.from_dict(eval, orient='index', columns=[\"Value\"])\n","  else:\n","    return eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9s7JdNd_GK3-"},"outputs":[],"source":["def regression_paired_differences(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, error_type: Literal['absolute', 'squared'] = 'absolute') -> np.ndarray:\n","  if 'absolute' == error_type:\n","    model_1_errors = np.abs(y_true - y_pred_model_1)\n","    model_2_errors = np.abs(y_true - y_pred_model_2)\n","  elif 'squared' == error_type:\n","    model_1_errors = np.square(y_true - y_pred_model_1)\n","    model_2_errors = np.square(y_true - y_pred_model_2)\n","\n","  paired_differences = model_1_errors - model_2_errors # Positive value means model 2 is better\n","  return paired_differences\n","\n","def classification_paired_differences(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray) -> np.ndarray:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","\n","  y_true = y_true.astype(int)\n","\n","  model_1_correct = y_pred_model_1[np.arange(len(y_true)), y_true]\n","  model_2_correct = y_pred_model_2[np.arange(len(y_true)), y_true]\n","\n","  # Calculate paired differences in confidence for the true class\n","  paired_differences = model_2_correct - model_1_correct # Positive value means model 2 is better\n","\n","  return paired_differences\n","\n","def regression_bootstrap_twosided_confidence_intervals(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, metric: Callable[[np.ndarray, np.ndarray], np.ndarray] = sl.metrics.mean_absolute_error, n_iterations: int = 10000, alpha: float = 0.05) -> Tuple[float, float]:\n","  return bootstrap_twosided_confidence_intervals(y_true, y_pred_model_1, y_pred_model_2, metric, n_iterations, alpha)\n","\n","def classification_bootstrap_twosided_confidence_intervals(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, metric: Callable[[np.ndarray, np.ndarray], np.ndarray] = sl.metrics.accuracy_score, n_iterations: int = 10000, alpha: float = 0.05) -> Tuple[float, float]:\n","  return bootstrap_twosided_confidence_intervals(y_true, y_pred_model_1, y_pred_model_2, metric, n_iterations, alpha)\n","\n","def bootstrap_twosided_confidence_intervals(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, metric: Callable[[np.ndarray, np.ndarray], np.ndarray], n_iterations: int = 10000, alpha: float = 0.05) -> Tuple[float, float]:\n","  sample_size = len(y_true)\n","\n","  boot_diffs = []\n","  for _ in range(n_iterations):\n","    # Resample indices with replacement\n","    indices = np.random.choice(range(sample_size), size=sample_size, replace=True)\n","\n","    y_true_sample = y_true[indices]\n","    y_pred_1_sample = y_pred_model_1[indices]\n","    y_pred_2_sample = y_pred_model_2[indices]\n","\n","    metric_1 = metric(y_true_sample, y_pred_1_sample)\n","    metric_2 = metric(y_true_sample, y_pred_2_sample)\n","\n","    boot_diffs.append(metric_1 - metric_2)\n","\n","  lower = np.percentile(boot_diffs, 100 * alpha / 2)\n","  upper = np.percentile(boot_diffs, 100 * (1 - alpha / 2))\n","\n","  return lower, upper\n","\n","def bootstrap_paired_differences_twosided_confidence_interval(paired_differences: np.ndarray, n_iterations: int = 10000, alpha: float = 0.05) -> Tuple[float, float]:\n","  sample_size = len(paired_differences)\n","\n","  boot_means = []\n","  for _ in range(n_iterations):\n","    # Resample indices with replacement\n","    indices = np.random.choice(paired_differences, size=sample_size, replace=True)\n","\n","    paired_diff_sample = np.random.choice(paired_differences, size=len(paired_differences), replace=True)\n","    boot_means.append(np.mean(paired_diff_sample))\n","\n","  lower = np.percentile(boot_means, 100 * alpha / 2)\n","  upper = np.percentile(boot_means, 100 * (1 - alpha / 2))\n","\n","  return lower, upper\n","\n","def test_mcnemar(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, exact: bool = True, return_df: bool = False) -> Tuple[float, float]:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","  if y_pred_model_1.ndim > 1:\n","      y_pred_model_1 = np.argmax(y_pred_model_1, axis=1)\n","  if y_pred_model_2.ndim > 1:\n","      y_pred_model_2 = np.argmax(y_pred_model_2, axis=1)\n","\n","  # Build the contingency table\n","  a = np.sum((y_true == y_pred_model_1) & (y_true == y_pred_model_2)) # both correct\n","  b = np.sum((y_true == y_pred_model_1) & (y_true != y_pred_model_2)) # m1 correct, m2 incorrect\n","  c = np.sum((y_true != y_pred_model_1) & (y_true == y_pred_model_2)) # m1 incorrect, m2 correct\n","  d = np.sum((y_true != y_pred_model_1) & (y_true != y_pred_model_2)) # both incorrect\n","\n","  contingency_table = np.array([[a, b],\n","                                [c, d]], dtype=int)\n","\n","  test_result = sm.stats.contingency_tables.mcnemar(contingency_table, exact=exact)\n","\n","  # Effect sizes\n","  n = a + b + c + d\n","  odds_ratio = b / c if c != 0 else np.nan\n","  prop_diff = (b - c) / n\n","\n","  eval = {\n","    \"test_statistic\": test_result.statistic,\n","    \"p_value\": test_result.pvalue,\n","    \"contingency_table\": contingency_table.tolist(),\n","    \"proportion_difference\": prop_diff,\n","    \"odds_ratio\": odds_ratio,\n","  }\n","\n","  if return_df:\n","    return pd.DataFrame.from_dict(eval, orient='index', columns=[\"Value\"])\n","  else:\n","    return eval\n","\n","def test_wilcoxon_signed_rank(paired_differences: np.ndarray, return_df: bool = False) -> Tuple[float, float]:\n","  # Large sample size (n > 50) so use asymptotic approximation\n","  test_result = sp.stats.wilcoxon(paired_differences, alternative='two-sided', method='approx', correction=True)\n","\n","  # Calculate Rosenthal's r effect size\n","  n = len(paired_differences)\n","  r = abs(test_result.zstatistic) / np.sqrt(n)\n","\n","  eval = {\n","    \"test_statistic\": test_result.statistic,\n","    \"p_value\": test_result.pvalue,\n","    \"z_score\": test_result.zstatistic,\n","    \"rosenthals_r\": r,\n","  }\n","\n","  if return_df:\n","    return pd.DataFrame.from_dict(eval, orient='index', columns=[\"Value\"])\n","  else:\n","    return eval\n","\n","def interpret_effect_size(effect_size: float) -> Literal['negligible', 'small', 'medium', 'large']:\n","  if effect_size < 0.1:\n","    effect_interpretation = \"negligible\"\n","  elif effect_size < 0.3:\n","    effect_interpretation = \"small\"\n","  elif effect_size < 0.5:\n","    effect_interpretation = \"medium\"\n","  else:\n","    effect_interpretation = \"large\"\n","\n","  return effect_interpretation\n","\n","def visualize_paired_differences(\n","    paired_differences: np.ndarray,\n","    confidence_level: float = 0.95,\n","    ci_lower: Optional[float] = None,\n","    ci_upper: Optional[float] = None,\n","    name_model_1: str = \"Model 1\",\n","    name_model_2: str = \"Model 2\"\n","  ):\n","  mean_diff = np.mean(paired_differences)\n","  median_diff = np.median(paired_differences)\n","\n","  # Create visualization\n","  plt.figure(figsize=(12, 6))\n","\n","  # Main histogram\n","  plt.hist(paired_differences, bins='auto', density=True, alpha=0.7, color='skyblue', edgecolor='black')\n","\n","  # Add kernel density estimate\n","  kde = sp.stats.gaussian_kde(paired_differences)\n","  x_range = np.linspace(min(paired_differences), max(paired_differences), 200)\n","  plt.plot(x_range, kde(x_range), 'r-', lw=2, label='KDE')\n","\n","  # Reference lines\n","  plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5, label=\"No difference\")\n","  plt.axvline(x=mean_diff, color='green', linestyle='-', label=\"Mean\")\n","  plt.axvline(x=median_diff, color='blue', linestyle='-', label=\"Median\")\n","\n","  # Confidence interval\n","  if ci_lower is not None and ci_upper is not None:\n","    plt.axvspan(ci_lower, ci_upper, alpha=0.2, color='green', label=f'{confidence_level*100:.0f}% CI')\n","\n","  plt.xlabel(f\"Performance Difference ({name_model_1} - {name_model_2})\")\n","  plt.ylabel(\"Density\")\n","  plt.title(\"Distribution of Paired Differences\")\n","  plt.legend()\n","  plt.grid(True, alpha=0.3)\n"]},{"cell_type":"markdown","metadata":{"id":"r3AIDPxZlS-v"},"source":["## Data Import"]},{"cell_type":"markdown","metadata":{"id":"XAPgp73mjlFP"},"source":["### A: Import from Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4F4xUcGjkc0"},"outputs":[],"source":["drive.mount(\"/content/drive\")\n","\n","!cp -r \"$GDRIVE_INPUT_DIR\" \"$INPUT_DATA_DIR\"\n","\n","drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"FzhkKQsVj11R"},"source":["### B: Upload from Local Machine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHaQpf2gjxzL"},"outputs":[],"source":["#uploaded = files.upload()\n","\n","#for filename in uploaded.keys():\n","#  target = os.path.join(INPUT_DATA_DIR, filename)\n","#  !mv \"$filename\" \"$target\"\n","\n","# del uploaded"]},{"cell_type":"markdown","metadata":{"id":"aFJHEiyh9nqo"},"source":["# Dataset: Incident management process enriched event log"]},{"cell_type":"markdown","metadata":{"id":"9pqeJp32LH6d"},"source":["This event log is of an incident management process extracted from an instance of the ServiceNow platform used by an IT company. See also https://doi.org/10.24432/C57S4H and http://processmining.each.webhostusp.sti.usp.br/index.php/event-logs/.\n","\n","- **Control Attributes**:\n","    - *number*: incident identifier with the same number as total cases;\n","    \n","    - *incident state*: attribute with eight levels controlling incident management process transitions from opening until closing the case;\n","    \n","    - *active*: boolean attribute indicating if record is active or closed/canceled;\n","    \n","    - *reassignment_count*: number of times incident has changed group or support analysts;\n","    \n","    - *reopen_count*: number of times incident resolution was rejected by caller;\n","    \n","    - *sys_mod_count*: number of incident updates until that moment;\n","    \n","    - *made_sla*: boolean attribute to incident exceeded target SLA or not;\n","\n","- **Identification and Classification Attributes**:\n","    - *caller_id*: user identifier affected;\n","    \n","    - *opened_by*: user identifier that reported the incident;\n","    \n","    - *opened_at*: incident opening date and time;\n","    \n","    - *sys_created_by*: user identifier that registered the incident;\n","    \n","    - *sys_created_at*: incident creation date and time;\n","    \n","    - *sys_updated_by*: user identifier that made update and generated current log record;\n","    \n","    - *sys_updated_at*: log update date and time;\n","    \n","    - *contact_type*: categorical field with values indicating how incident was reported;\n","    \n","    - *location*: location identifier of place being affected;\n","    \n","    - *category*: description of the first level of service being affected;\n","    \n","    - *subcategory*: description of the second level of service being affected related to first level;\n","    \n","    - *u_symptom*: description about user perception of service availability;\n","    \n","    - *cmdb_ci*: (confirmation item) identifier (not mandatory) referencing homonyms relation and used to report item being affected;\n","    \n","    - *impact*: description of the impact caused by incident. Values are: 1-High; 2-Medium; 3-Low;\n","    \n","    - *urgency*: description to the urgency asked by user for incident resolution. Values are same as impact;\n","    \n","    - *priority*: priority calculated by system based on Impact and urgency;\n","\n","- **Support, Diagnosis and Other Attributes**:\n","    - *assignment_group*: identifier referencing the relation Group (database relational model in ServiceNowTM) describing support group in charge of incident;\n","    \n","    - *assigned_to*: user identifier in charge of incident;\n","    \n","    - *knowledge*: boolean attribute indicating whether a knowledge base document was used to resolve incident;\n","    \n","    - *u_priority_confirmation*: boolean attribute indicating whether priority field was double checked;\n","    \n","    - *notify*: categorical attribute indicating whether notifications was generated for this incident;\n","    \n","    - *problem_id*: identifier referencing homonyms relation describing problem identifier associated with this incident;\n","    \n","    - *rfc*: (change request) identifier referencing homonyms relation describing change request identifier associated with incident;\n","    \n","    - *vendor*: identifier referencing homonyms relation describing vendor in charge of incident;\n","    \n","    - *caused_by*: relation with RFC code responsible by the incident;\n","    \n","    - *close_code*: resolution code of the incident;\n","    \n","    - *resolved_by*: user identifier who resolved the incident;\n","    \n","    - *resolved_at*: incident resolution date and time;\n","    \n","    - *closed_at*: incident close date and time;"]},{"cell_type":"markdown","metadata":{"id":"bcr6aHNqzZhO"},"source":["## Read Incident management process enriched event log V1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GR798kUuwzYf"},"outputs":[],"source":["df_servicenow = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"incident_event_log_cleaned.feather\"))\n","df_servicenow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Gy_MLZLXr1x"},"outputs":[],"source":["df_servicenow_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"incident_event_log_train.feather\"))\n","df_servicenow_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJpalikfXwBi"},"outputs":[],"source":["df_servicenow_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"incident_event_log_test.feather\"))\n","df_servicenow_test"]},{"cell_type":"markdown","metadata":{"id":"VBBLNkR5pN3i"},"source":["## Result Evaluation for Incident management process enriched event log V1"]},{"cell_type":"markdown","metadata":{"id":"7ADrEmn9mvEl"},"source":["### Next Activity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMMo3uA1mulB"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Activity\", \"Servicenow\")\n","\n","y_true_servicenow_na = np.load(os.path.join(in_dir, \"servicenow_next_activity_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"5rG3AqJcznW7"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnkSRH6Rzmsa"},"outputs":[],"source":["df_naive_classification_metrics(df_servicenow_train, df_servicenow_test, EVENTLOG_LABEL_NEXT_ACT, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"02Aon_5WpYTu"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mx0sYXrpU6B"},"outputs":[],"source":["y_pred_servicenow_lstm_na = np.load(os.path.join(in_dir, \"lstm_servicenow_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_servicenow_na, y_pred_servicenow_lstm_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"y-ZqYn7CpcUH"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZbN1Mukpbx2"},"outputs":[],"source":["y_pred_servicenow_trans_na = np.load(os.path.join(in_dir, \"processtransformer_servicenow_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_servicenow_na, y_pred_servicenow_trans_na, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"adxiLsVcHJF9"}},{"cell_type":"code","source":["y_pred_servicenow_prophet_na = np.load(os.path.join(in_dir, \"prophet_servicenow_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_servicenow_na, y_pred_servicenow_prophet_na, return_df=True)"],"metadata":{"id":"7XeymrCGHIZ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XAXYHzLDpgIo"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irzHYzMBpfIu"},"outputs":[],"source":["y_pred_servicenow_ast_na = np.load(os.path.join(in_dir, \"ast_servicenow_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_servicenow_na, y_pred_servicenow_ast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"qA70x0f0piQN"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q4KIJ53pkQQ"},"outputs":[],"source":["y_pred_servicenow_tgnast_na = np.load(os.path.join(in_dir, \"tgnast_servicenow_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_servicenow_na, y_pred_servicenow_tgnast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"UlphOAEMzfh5"},"source":["#### Comparison: AST vs. TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llGtVCaN-rxJ"},"outputs":[],"source":["paired_diff = classification_paired_differences(y_true_servicenow_na, y_pred_servicenow_ast_na, y_pred_servicenow_tgnast_na)\n","test_wilcoxon_signed_rank(paired_diff, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"5TEWYEaen5gI"},"source":["### Next Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9MDEembwKH9"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Time\", \"Servicenow\")\n","\n","y_true_servicenow_nt = np.load(os.path.join(in_dir, \"servicenow_next_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"hxN87vI60e20"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5VJEovB0gph"},"outputs":[],"source":["df_naive_regression_metrics(df_servicenow_train, df_servicenow_test, EVENTLOG_LABEL_NEXT_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"3VMb21IvwGaV"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjbRRpvbwFgP"},"outputs":[],"source":["y_pred_servicenow_lstm_nt = np.load(os.path.join(in_dir, \"lstm_servicenow_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_nt, y_pred_servicenow_lstm_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"LxCQG0kK3L6b"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGrsXNaUn41f"},"outputs":[],"source":["y_pred_servicenow_trans_nt = np.load(os.path.join(in_dir, \"processtransformer_servicenow_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_nt, y_pred_servicenow_trans_nt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"T5v1rw_6HSef"}},{"cell_type":"code","source":["y_pred_servicenow_prophet_nt = np.load(os.path.join(in_dir, \"prophet_servicenow_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_nt, y_pred_servicenow_prophet_nt, return_df=True)"],"metadata":{"id":"Qf_Vd3SDHS0I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdMlCKZO3QHz"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9j90Grw3K1w"},"outputs":[],"source":["y_pred_servicenow_ast_nt = np.load(os.path.join(in_dir, \"ast_servicenow_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_nt, y_pred_servicenow_ast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"WNQm_L5t3UPH"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uosqK83X3UjS"},"outputs":[],"source":["y_pred_servicenow_tgnast_nt = np.load(os.path.join(in_dir, \"tgnast_servicenow_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_nt, y_pred_servicenow_tgnast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"qP2suXcSoYwD"},"source":["### Remaining Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"My31vyyboYMi"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Remaining Time\", \"Servicenow\")\n","\n","y_true_servicenow_rt = np.load(os.path.join(in_dir, \"servicenow_remaining_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"CnSb7f_MA_Te"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DmFqnQKA_K6"},"outputs":[],"source":["df_naive_regression_metrics(df_servicenow_train, df_servicenow_test, EVENTLOG_LABEL_REM_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"qaVPg0DjBJTA"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zcIrH_SBIhW"},"outputs":[],"source":["y_pred_servicenow_lstm_rt = np.load(os.path.join(in_dir, \"lstm_servicenow_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_rt, y_pred_servicenow_lstm_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"feMv1oK3BMSa"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_kmvvJEA-wj"},"outputs":[],"source":["y_pred_servicenow_trans_rt = np.load(os.path.join(in_dir, \"processtransformer_servicenow_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_rt, y_pred_servicenow_trans_rt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"7tg8FABsHbu_"}},{"cell_type":"code","source":["y_pred_servicenow_prophet_rt = np.load(os.path.join(in_dir, \"prophet_servicenow_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_rt, y_pred_servicenow_prophet_rt, return_df=True)"],"metadata":{"id":"7eiyV_6mHcCZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2OLgq1RmEDch"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68i4Z5huEE9c"},"outputs":[],"source":["y_pred_servicenow_ast_rt = np.load(os.path.join(in_dir, \"ast_servicenow_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_rt, y_pred_servicenow_ast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"7EGc42mWEFTy"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7BFxUmnEGiy"},"outputs":[],"source":["y_pred_servicenow_tgnast_rt = np.load(os.path.join(in_dir, \"tgnast_servicenow_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_servicenow_rt, y_pred_servicenow_tgnast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"oM2tjm7REHVX"},"source":["#### Comparison: PROPHET vs TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6ucyNgXWt46"},"outputs":[],"source":["paired_diff = regression_paired_differences(y_true_servicenow_rt, y_pred_servicenow_prophet_rt, y_pred_servicenow_tgnast_rt)\n","test_wilcoxon_signed_rank(paired_diff, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"kzxxUVrg93w6"},"source":["# Dataset: Dataset belonging to the help desk log of an Italian Company"]},{"cell_type":"markdown","metadata":{"id":"zxhz-B2iO2cQ"},"source":["The event log concerns the ticketing management process of the Help desk of an Italian software company. See also https://doi.org/10.4121/uuid:0c60edf1-6f83-4e75-9367-4c63b3e9d5bb.\n","\n","- *Case ID*: the case identifier\n","\n","- *Activity*: the activity name\n","\n","- *Resource*: the resource who performed the action\n","\n","- Complete Timestamp: the timestamp of the event. Format: YYYY/MM/DD hh:mm:ss.\n","\n","- *Variant*: case variant\n","\n","- Variant index: case variant in integer format\n","\n","- *seriousness*: a seriousness level for the ticket\n","\n","- *customer*: name of the customer\n","\n","- *product*: name of the product\n","\n","- *responsible_section*: name of the responsible section\n","\n","- *seriousness_2*: a sub-seriousness level\n","\n","- *service_level*: level of the service\n","\n","- *service_type*: type of the service\n","\n","- *support_section*: name of the support section\n","\n","- *workgroup*: name of the workgroup"]},{"cell_type":"markdown","metadata":{"id":"h1CXgfZ3-E3q"},"source":["## Read Dataset belonging to the help desk log of an Italian Company"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7YIoVLxYaN3"},"outputs":[],"source":["df_italy = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"finale_cleaned.feather\"))\n","df_italy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfF3DBXdYaN4"},"outputs":[],"source":["df_italy_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"finale_train.feather\"))\n","df_italy_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aATDXtbxYaN4"},"outputs":[],"source":["df_italy_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"finale_test.feather\"))\n","df_italy_test"]},{"cell_type":"markdown","metadata":{"id":"ibCX83SVEdOr"},"source":["## Result Evaluation for Dataset belonging to the help desk log of an Italian Company"]},{"cell_type":"markdown","metadata":{"id":"T_2TJsNQEdOt"},"source":["### Next Activity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbew_cs5EdOt"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Activity\", \"Italy\")\n","\n","y_true_italy_na = np.load(os.path.join(in_dir, \"italy_next_activity_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"nPIWWjFWEdOu"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SR1w6sAtEdOu"},"outputs":[],"source":["df_naive_classification_metrics(df_italy_train, df_italy_test, EVENTLOG_LABEL_NEXT_ACT, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"MUU8gfjMEdOw"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EH8yf04bEdOw"},"outputs":[],"source":["y_pred_italy_lstm_na = np.load(os.path.join(in_dir, \"lstm_italy_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_italy_na, y_pred_italy_lstm_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"QCAJHvLXEdOx"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTQ9aX0EEdOx"},"outputs":[],"source":["y_pred_italy_trans_na = np.load(os.path.join(in_dir, \"processtransformer_italy_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_italy_na, y_pred_italy_trans_na, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"HCbUobH6HpOd"}},{"cell_type":"code","source":["y_pred_italy_prophet_na = np.load(os.path.join(in_dir, \"prophet_italy_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_italy_na, y_pred_italy_prophet_na, return_df=True)"],"metadata":{"id":"UAGSkGw7Hq9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9sjE1LmEdOx"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bil7BInQEdOx"},"outputs":[],"source":["y_pred_italy_ast_na = np.load(os.path.join(in_dir, \"ast_italy_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_italy_na, y_pred_italy_ast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"G33MZHmPEdOy"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2zUd632EdOy"},"outputs":[],"source":["y_pred_italy_tgnast_na = np.load(os.path.join(in_dir, \"tgnast_italy_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_italy_na, y_pred_italy_tgnast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"xvEAu1zIEdOz"},"source":["### Next Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G89Iz6vVEdOz"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Time\", \"Italy\")\n","\n","y_true_italy_nt = np.load(os.path.join(in_dir, \"italy_next_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"3KRSYILsEdOz"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVFcP92dEdOz"},"outputs":[],"source":["df_naive_regression_metrics(df_italy_train, df_italy_test, EVENTLOG_LABEL_NEXT_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"-nq6v4tKEdO0"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_8MSUfCEdO0"},"outputs":[],"source":["y_pred_italy_lstm_nt = np.load(os.path.join(in_dir, \"lstm_italy_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_nt, y_pred_italy_lstm_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"GEHiAwM8EdO0"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulVQke9LEdO0"},"outputs":[],"source":["y_pred_italy_trans_nt = np.load(os.path.join(in_dir, \"processtransformer_italy_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_nt, y_pred_italy_trans_nt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"JdMVl7PhHvRi"}},{"cell_type":"code","source":["y_pred_italy_prophet_nt = np.load(os.path.join(in_dir, \"prophet_italy_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_nt, y_pred_italy_prophet_nt, return_df=True)"],"metadata":{"id":"g4Bf0AX7HvRj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5UUobWifEdO0"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugBsatm1EdO1"},"outputs":[],"source":["y_pred_italy_ast_nt = np.load(os.path.join(in_dir, \"ast_italy_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_nt, y_pred_italy_ast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"BwbTYleqEdO1"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JG1bR_AGEdO1"},"outputs":[],"source":["y_pred_italy_tgnast_nt = np.load(os.path.join(in_dir, \"tgnast_italy_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_nt, y_pred_italy_tgnast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"JGeJ8K3fEdO1"},"source":["### Remaining Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jf-QQ9SmEdO1"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Remaining Time\", \"Italy\")\n","\n","y_true_italy_rt = np.load(os.path.join(in_dir, \"italy_remaining_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"SW2WbMX9EdO2"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2CdtnwWEdO2"},"outputs":[],"source":["df_naive_regression_metrics(df_italy_train, df_italy_test, EVENTLOG_LABEL_REM_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"jbczp7elEdO2"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzPmrlAUEdO2"},"outputs":[],"source":["y_pred_italy_lstm_rt = np.load(os.path.join(in_dir, \"lstm_italy_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_rt, y_pred_italy_lstm_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"vsl1-huhEdO2"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ov9CB_rgEdO3"},"outputs":[],"source":["y_pred_italy_trans_rt = np.load(os.path.join(in_dir, \"processtransformer_italy_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_rt, y_pred_italy_trans_rt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"VM5RNZuFHzcE"}},{"cell_type":"code","source":["y_pred_italy_prophet_rt = np.load(os.path.join(in_dir, \"prophet_italy_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_rt, y_pred_italy_prophet_rt, return_df=True)"],"metadata":{"id":"zUKmVLdmHzcF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D020v-2YEdO3"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-NKgY6T0EdO4"},"outputs":[],"source":["y_pred_italy_ast_rt = np.load(os.path.join(in_dir, \"ast_italy_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_rt, y_pred_italy_ast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"ytdb6XRZEdO4"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-y1zj5DXEdO4"},"outputs":[],"source":["y_pred_italy_tgnast_rt = np.load(os.path.join(in_dir, \"tgnast_italy_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_italy_rt, y_pred_italy_tgnast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"KwaK3XFhJ9DB"},"source":["# Dataset: BPIC 2014"]},{"cell_type":"markdown","metadata":{"id":"z9hLpLZ9Pqrw"},"source":["This datset is provided by the Rabobank ICT and contains information about the employed ITIL processes. See https://www.win.tue.nl/bpi/2014/challenge.html and https://data.4tu.nl/collections/dff0e630-9c91-4b8e-806d-ec9a3a0f2206"]},{"cell_type":"markdown","metadata":{"id":"j6rlZoabKC-H"},"source":["## Read BPIC 2014"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTsiD0XaZAnq"},"outputs":[],"source":["df_bpic14 = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_cleaned.feather\"))\n","df_bpic14"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_ANdt_tZAnr"},"outputs":[],"source":["df_bpic14_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_train.feather\"))\n","df_bpic14_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JRTEGuzZAns"},"outputs":[],"source":["df_bpic14_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_test.feather\"))\n","df_bpic14_test"]},{"cell_type":"markdown","metadata":{"id":"3GljDMDOHRET"},"source":["## Result Evaluation for BPIC 2014"]},{"cell_type":"markdown","metadata":{"id":"bVZ9pxi4HREV"},"source":["### Next Activity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0jFj8hpHREV"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Activity\", \"BPIC 2014\")\n","\n","y_true_bpic14_na = np.load(os.path.join(in_dir, \"bpic14_next_activity_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"cBcEeoKzHREW"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7GSD35DHREW"},"outputs":[],"source":["df_naive_classification_metrics(df_bpic14_train, df_bpic14_test, EVENTLOG_LABEL_NEXT_ACT, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"Y7b5iGGuHREX"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DnyTT3oHREX"},"outputs":[],"source":["y_pred_bpic14_lstm_na = np.load(os.path.join(in_dir, \"lstm_bpic14_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic14_na, y_pred_bpic14_lstm_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"FunmxY82HREY"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQ9h9eKOHREY"},"outputs":[],"source":["y_pred_bpic14_trans_na = np.load(os.path.join(in_dir, \"processtransformer_bpic14_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic14_na, y_pred_bpic14_trans_na, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"DWJhikt5H4kD"}},{"cell_type":"code","source":["y_pred_bpic14_prophet_na = np.load(os.path.join(in_dir, \"prophet_bpic14_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic14_na, y_pred_bpic14_prophet_na, return_df=True)"],"metadata":{"id":"juMPrCVuH4kE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFmJqV6QHREY"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3Sh_rvdHREZ"},"outputs":[],"source":["y_pred_bpic14_ast_na = np.load(os.path.join(in_dir, \"ast_bpic14_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic14_na, y_pred_bpic14_ast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"vHwlz0wqHREZ"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KwSacfsHREa"},"outputs":[],"source":["y_pred_bpic14_tgnast_na = np.load(os.path.join(in_dir, \"tgnast_bpic14_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic14_na, y_pred_bpic14_tgnast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"dixuD9tbHREa"},"source":["#### Comparison: AST vs TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMfRVEGkHREa"},"outputs":[],"source":["paired_diff = classification_paired_differences(y_true_bpic14_na, y_pred_bpic14_ast_na, y_pred_bpic14_tgnast_na)\n","test_wilcoxon_signed_rank(paired_diff, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"mD6D8ZHlHREb"},"source":["### Next Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXQOsqcVHREb"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Time\", \"BPIC 2014\")\n","\n","y_true_bpic14_nt = np.load(os.path.join(in_dir, \"bpic14_next_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"QIP0vNGqHREb"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSRiE2M1HREb"},"outputs":[],"source":["df_naive_regression_metrics(df_bpic14_train, df_bpic14_test, EVENTLOG_LABEL_NEXT_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"lkGY9X61HREc"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkpB_OnKHREc"},"outputs":[],"source":["y_pred_bpic14_lstm_nt = np.load(os.path.join(in_dir, \"lstm_bpic14_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_nt, y_pred_bpic14_lstm_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"tpaijwiKHREc"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oaQFclqHREc"},"outputs":[],"source":["y_pred_bpic14_trans_nt = np.load(os.path.join(in_dir, \"processtransformer_bpic14_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_nt, y_pred_bpic14_trans_nt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"6Ll7mf0fH8p0"}},{"cell_type":"code","source":["y_pred_bpic14_prophet_nt = np.load(os.path.join(in_dir, \"prophet_bpic14_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_nt, y_pred_bpic14_prophet_nt, return_df=True)"],"metadata":{"id":"qdQ2asArH8p1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-mjNsJyHREd"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwctZsk5HREd"},"outputs":[],"source":["y_pred_bpic14_ast_nt = np.load(os.path.join(in_dir, \"ast_bpic14_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_nt, y_pred_bpic14_ast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"JetArhoBHREd"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPTLGNo9HREd"},"outputs":[],"source":["y_pred_bpic14_tgnast_nt = np.load(os.path.join(in_dir, \"tgnast_bpic14_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_nt, y_pred_bpic14_tgnast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"8CtRw06XHREd"},"source":["#### Comparison: AST vs. TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuCt8iW_ajJl"},"outputs":[],"source":["paired_diff = regression_paired_differences(y_true_bpic14_nt, y_pred_bpic14_ast_nt, y_pred_bpic14_tgnast_nt)\n","test_wilcoxon_signed_rank(paired_diff, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"5Io4eClPHREe"},"source":["### Remaining Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzy0Fh4iHREe"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Remaining Time\", \"BPIC 2014\")\n","\n","y_true_bpic14_rt = np.load(os.path.join(in_dir, \"bpic14_remaining_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"bspkD96WHREe"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E088Ue2cHREe"},"outputs":[],"source":["df_naive_regression_metrics(df_bpic14_train, df_bpic14_test, EVENTLOG_LABEL_REM_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"SVnmENn0HREf"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6115zG3IHREg"},"outputs":[],"source":["y_pred_bpic14_lstm_rt = np.load(os.path.join(in_dir, \"lstm_bpic14_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_rt, y_pred_bpic14_lstm_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"wUKAy6viHREg"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKw8gqYCHREg"},"outputs":[],"source":["y_pred_bpic14_trans_rt = np.load(os.path.join(in_dir, \"processtransformer_bpic14_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_rt, y_pred_bpic14_trans_rt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"qFmgnHAIIBVD"}},{"cell_type":"code","source":["y_pred_bpic14_prophet_rt = np.load(os.path.join(in_dir, \"prophet_bpic14_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_rt, y_pred_bpic14_prophet_rt, return_df=True)"],"metadata":{"id":"EouTttMdIBVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wt6MHkr1HREg"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tg3xFp3LHREh"},"outputs":[],"source":["y_pred_bpic14_ast_rt = np.load(os.path.join(in_dir, \"ast_bpic14_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_rt, y_pred_bpic14_ast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"g6dY-lx7HREh"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaXD2CZsHREh"},"outputs":[],"source":["y_pred_bpic14_tgnast_rt = np.load(os.path.join(in_dir, \"tgnast_bpic14_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic14_rt, y_pred_bpic14_tgnast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"tYcrP6TFjucY"},"source":["# Dataset: Helpdesk"]},{"cell_type":"markdown","metadata":{"id":"ZfFzt-NirMwx"},"source":["## Read Helpdesk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUNIfsA8alzt"},"outputs":[],"source":["df_helpdesk = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"helpdesk_cleaned.feather\"))\n","df_helpdesk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlo8q_blalzu"},"outputs":[],"source":["df_helpdesk_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"helpdesk_train.feather\"))\n","df_helpdesk_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRssS9m7alzu"},"outputs":[],"source":["df_helpdesk_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"helpdesk_test.feather\"))\n","df_helpdesk_test"]},{"cell_type":"markdown","metadata":{"id":"G1su51jFJoJs"},"source":["## Result Evaluation for Helpdesk"]},{"cell_type":"markdown","metadata":{"id":"DU9EcjgdJoJv"},"source":["### Next Activity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Z4KH7SsJoJw"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Activity\", \"Helpdesk\")\n","\n","y_true_helpdesk_na = np.load(os.path.join(in_dir, \"helpdesk_next_activity_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"1pGLLbobJoJw"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aKVrMUTJoJx"},"outputs":[],"source":["df_naive_classification_metrics(df_helpdesk_train, df_helpdesk_test, EVENTLOG_LABEL_NEXT_ACT, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"2a4WUPBmJoJx"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7WM93hhJoJx"},"outputs":[],"source":["y_pred_helpdesk_lstm_na = np.load(os.path.join(in_dir, \"lstm_helpdesk_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_helpdesk_na, y_pred_helpdesk_lstm_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"fAbTbkN2JoJy"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gA4Lj3QJoJy"},"outputs":[],"source":["y_pred_helpdesk_trans_na = np.load(os.path.join(in_dir, \"processtransformer_helpdesk_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_helpdesk_na, y_pred_helpdesk_trans_na, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"JyVIDCWeIF7i"}},{"cell_type":"code","source":["y_pred_helpdesk_prophet_na = np.load(os.path.join(in_dir, \"prophet_helpdesk_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_helpdesk_na, y_pred_helpdesk_prophet_na, return_df=True)"],"metadata":{"id":"GsnbnI6lIF7i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjJzivDuJoJz"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUPMRcBuJoJz"},"outputs":[],"source":["y_pred_helpdesk_ast_na = np.load(os.path.join(in_dir, \"ast_helpdesk_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_helpdesk_na, y_pred_helpdesk_ast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"JuADF3emJoJ0"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtOv1FoNJoJ0"},"outputs":[],"source":["y_pred_helpdesk_tgnast_na = np.load(os.path.join(in_dir, \"tgnast_helpdesk_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_helpdesk_na, y_pred_helpdesk_tgnast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"7dw583MOJoJ1"},"source":["### Next Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqZ5cX5qJoJ1"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Time\", \"Helpdesk\")\n","\n","y_true_helpdesk_nt = np.load(os.path.join(in_dir, \"helpdesk_next_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"OuJgZBMzJoJ1"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfq1b4AQJoJ2"},"outputs":[],"source":["df_naive_regression_metrics(df_helpdesk_train, df_helpdesk_test, EVENTLOG_LABEL_NEXT_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"iZ8FlK9FJoJ2"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1X2yCXKrJoJ2"},"outputs":[],"source":["y_pred_helpdesk_lstm_nt = np.load(os.path.join(in_dir, \"lstm_helpdesk_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_nt, y_pred_helpdesk_lstm_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"E5j4pvMgJoJ2"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbrZCXdPJoJ2"},"outputs":[],"source":["y_pred_helpdesk_trans_nt = np.load(os.path.join(in_dir, \"processtransformer_helpdesk_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_nt, y_pred_helpdesk_trans_nt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"E_zkX5BGIPr4"}},{"cell_type":"code","source":["y_pred_helpdesk_prophet_nt = np.load(os.path.join(in_dir, \"prophet_helpdesk_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_nt, y_pred_helpdesk_prophet_nt, return_df=True)"],"metadata":{"id":"1hYj9wJJIPr6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WjTfL3i4JoJ3"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYjxH8hUJoJ3"},"outputs":[],"source":["y_pred_helpdesk_ast_nt = np.load(os.path.join(in_dir, \"ast_helpdesk_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_nt, y_pred_helpdesk_ast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"eKEmhiQjJoJ4"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIh9ycYsJoJ4"},"outputs":[],"source":["y_pred_helpdesk_tgnast_nt = np.load(os.path.join(in_dir, \"tgnast_helpdesk_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_nt, y_pred_helpdesk_tgnast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"OYQ5SBmVJoJ4"},"source":["#### Comparison: AST vs TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pys4-wcAXtnP"},"outputs":[],"source":["paired_diff = regression_paired_differences(y_true_helpdesk_nt, y_pred_helpdesk_ast_nt, y_pred_helpdesk_tgnast_nt)\n","test_wilcoxon_signed_rank(paired_diff, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"-TvQAYKjJoJ4"},"source":["### Remaining Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8oG1EqSJoJ4"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Remaining Time\", \"Helpdesk\")\n","\n","y_true_helpdesk_rt = np.load(os.path.join(in_dir, \"helpdesk_remaining_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"DdRjZCb-JoJ4"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQX5zetEJoJ5"},"outputs":[],"source":["df_naive_regression_metrics(df_helpdesk_train, df_helpdesk_test, EVENTLOG_LABEL_REM_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"hl0AMkwMJoJ5"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lv1ZIC88JoJ5"},"outputs":[],"source":["y_pred_helpdesk_lstm_rt = np.load(os.path.join(in_dir, \"lstm_helpdesk_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_rt, y_pred_helpdesk_lstm_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"eocT9hCbJoJ5"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hVeg-ykZJoJ5"},"outputs":[],"source":["y_pred_helpdesk_trans_rt = np.load(os.path.join(in_dir, \"processtransformer_helpdesk_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_rt, y_pred_helpdesk_trans_rt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"Z26byvWhIS0N"}},{"cell_type":"code","source":["y_pred_helpdesk_prophet_rt = np.load(os.path.join(in_dir, \"prophet_helpdesk_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_rt, y_pred_helpdesk_prophet_rt, return_df=True)"],"metadata":{"id":"ECCAX3HCIS0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UeZPfYAJoJ5"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LpyASxXVJoJ5"},"outputs":[],"source":["y_pred_helpdesk_ast_rt = np.load(os.path.join(in_dir, \"ast_helpdesk_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_rt, y_pred_helpdesk_ast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"3G9Qt1kwJoJ5"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZQYsCpRJoJ5"},"outputs":[],"source":["y_pred_helpdesk_tgnast_rt = np.load(os.path.join(in_dir, \"tgnast_helpdesk_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_helpdesk_rt, y_pred_helpdesk_tgnast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"FjBMR7v0jlM8"},"source":["# Dataset: BPIC 2013"]},{"cell_type":"markdown","metadata":{"id":"Zgkk-dJ-rFhe"},"source":["## Read BPIC 2013"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M44bsTMdbML6"},"outputs":[],"source":["df_bpic13 = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_cleaned.feather\"))\n","df_bpic13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9CPWOeVbML6"},"outputs":[],"source":["df_bpic13_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_train.feather\"))\n","df_bpic13_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AsuMv6vMbML7"},"outputs":[],"source":["df_bpic13_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_test.feather\"))\n","df_bpic13_test"]},{"cell_type":"markdown","metadata":{"id":"4aStnOFrQ4DU"},"source":["## Result Evaluation for BPIC 2013"]},{"cell_type":"markdown","metadata":{"id":"iA7A8iykQ4DW"},"source":["### Next Activity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeHj4FW6Q4DW"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Activity\", \"BPIC 2013\")\n","\n","y_true_bpic13_na = np.load(os.path.join(in_dir, \"bpic13_next_activity_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"pzMMCVRYQ4DX"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jo4pc4C1Q4DY"},"outputs":[],"source":["df_naive_classification_metrics(df_bpic13_train, df_bpic13_test, EVENTLOG_LABEL_NEXT_ACT, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"cpfA7-QfQ4DY"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blZ1B6l0Q4DZ"},"outputs":[],"source":["y_pred_bpic13_lstm_na = np.load(os.path.join(in_dir, \"lstm_bpic13_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic13_na, y_pred_bpic13_lstm_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"4ayibxsmQ4DZ"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cW8WZrPrQ4DZ"},"outputs":[],"source":["y_pred_bpic13_trans_na = np.load(os.path.join(in_dir, \"processtransformer_bpic13_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic13_na, y_pred_bpic13_trans_na, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"YGwh7DYOIhSM"}},{"cell_type":"code","source":["y_pred_bpic13_prophet_na = np.load(os.path.join(in_dir, \"prophet_bpic13_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic13_na, y_pred_bpic13_prophet_na, return_df=True)"],"metadata":{"id":"ZSshrQTVIhSN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPS5Y6n4Q4Da"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ND3qvtxxQ4Da"},"outputs":[],"source":["y_pred_bpic13_ast_na = np.load(os.path.join(in_dir, \"ast_bpic13_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic13_na, y_pred_bpic13_ast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"nKMERv1mQ4Db"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svVzj-O4Q4Db"},"outputs":[],"source":["y_pred_bpic13_tgnast_na = np.load(os.path.join(in_dir, \"tgnast_bpic13_multi_next_activity_predictions.npy\"))\n","\n","evaluate_classification(y_true_bpic13_na, y_pred_bpic13_tgnast_na, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"NaY-Fcf7Q4Db"},"source":["#### Comparison: PROPHET vs. TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVC_KuweQ4Db"},"outputs":[],"source":["paired_diff = classification_paired_differences(y_true_bpic13_na, y_pred_bpic13_prophet_na, y_pred_bpic13_tgnast_na)\n","test_wilcoxon_signed_rank(paired_diff, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"QJTfjVxDQ4Dc"},"source":["### Next Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmGuLUB9Q4Dc"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Next Time\", \"BPIC 2013\")\n","\n","y_true_bpic13_nt = np.load(os.path.join(in_dir, \"bpic13_next_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"RUi-9RHCQ4Dd"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABqRN3azQ4Dd"},"outputs":[],"source":["df_naive_regression_metrics(df_bpic13_train, df_bpic13_test, EVENTLOG_LABEL_NEXT_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"6NtnZy3tQ4Dd"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5Ue9meMQ4De"},"outputs":[],"source":["y_pred_bpic13_lstm_nt = np.load(os.path.join(in_dir, \"lstm_bpic13_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_nt, y_pred_bpic13_lstm_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"DmPinuzNQ4Df"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTveeKD0Q4Df"},"outputs":[],"source":["y_pred_bpic13_trans_nt = np.load(os.path.join(in_dir, \"processtransformer_bpic13_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_nt, y_pred_bpic13_trans_nt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"y_7mO0SyIkb7"}},{"cell_type":"code","source":["y_pred_bpic13_prophet_nt = np.load(os.path.join(in_dir, \"prophet_bpic13_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_nt, y_pred_bpic13_prophet_nt, return_df=True)"],"metadata":{"id":"UOJUJ9GwIkb8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0mnX0EyQ4Df"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKw4v-9pQ4Dg"},"outputs":[],"source":["y_pred_bpic13_ast_nt = np.load(os.path.join(in_dir, \"ast_bpic13_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_nt, y_pred_bpic13_ast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"3GFlQ0NzQ4Dg"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lke_ke2eQ4Dg"},"outputs":[],"source":["y_pred_bpic13_tgnast_nt = np.load(os.path.join(in_dir, \"tgnast_bpic13_multi_next_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_nt, y_pred_bpic13_tgnast_nt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"ht_dpf0kQ4Dg"},"source":["### Remaining Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-ZQ4x0OQ4Dh"},"outputs":[],"source":["in_dir = os.path.join(INPUT_DATA_RESULT_DIR, \"Remaining Time\", \"BPIC 2013\")\n","\n","y_true_bpic13_rt = np.load(os.path.join(in_dir, \"bpic13_remaining_time_groundtruth.npy\"))"]},{"cell_type":"markdown","metadata":{"id":"ZMiY4iN4Q4Dh"},"source":["#### Naive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVikiYsBQ4Dh"},"outputs":[],"source":["df_naive_regression_metrics(df_bpic13_train, df_bpic13_test, EVENTLOG_LABEL_REM_TIME, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"y-BQ3B73Q4Dh"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FF9Q2gtnQ4Di"},"outputs":[],"source":["y_pred_bpic13_lstm_rt = np.load(os.path.join(in_dir, \"lstm_bpic13_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_rt, y_pred_bpic13_lstm_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"_e40dfWyQ4Di"},"source":["#### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0i1-nXsZQ4Di"},"outputs":[],"source":["y_pred_bpic13_trans_rt = np.load(os.path.join(in_dir, \"processtransformer_bpic13_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_rt, y_pred_bpic13_trans_rt, return_df=True)"]},{"cell_type":"markdown","source":["#### PROPHET"],"metadata":{"id":"OsuR-CC6IsSC"}},{"cell_type":"code","source":["y_pred_bpic13_prophet_rt = np.load(os.path.join(in_dir, \"prophet_bpic13_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_rt, y_pred_bpic13_prophet_rt, return_df=True)"],"metadata":{"id":"buSr59IGIsSE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HdfEqQlQQ4Di"},"source":["#### AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwOmVzzeQ4Dj"},"outputs":[],"source":["y_pred_bpic13_ast_rt = np.load(os.path.join(in_dir, \"ast_bpic13_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_rt, y_pred_bpic13_ast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"5-vY3bt0Q4Dj"},"source":["#### TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2F5axpVQ4Dj"},"outputs":[],"source":["y_pred_bpic13_tgnast_rt = np.load(os.path.join(in_dir, \"tgnast_bpic13_multi_remaining_time_predictions.npy\"))\n","\n","evaluate_regression(y_true_bpic13_rt, y_pred_bpic13_tgnast_rt, return_df=True)"]},{"cell_type":"markdown","metadata":{"id":"tEnsomiJlYF_"},"source":["# Data Export"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbnZaIXvlpIp"},"outputs":[],"source":["output_file = f\"results_{datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S%z')}.zip\"\n","\n","!zip -r \"$output_file\" \"$DATA_DIR\" \"$GRAPHIC_DIR\" \"$MODEL_DIR\""]},{"cell_type":"markdown","metadata":{"id":"jT5qK1yAleCQ"},"source":["## A: Export to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeUyaAkGmCts"},"outputs":[],"source":["drive.mount(\"/content/drive\")\n","\n","Path(GDRIVE_OUTPUT_DIR).mkdir(exist_ok=True)\n","\n","!cp \"$output_file\" \"$GDRIVE_OUTPUT_DIR\"\n","\n","drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"A6l3vuhklh7h"},"source":["## B: Download to Local Machine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBY3BvZTl3_0"},"outputs":[],"source":["files.download(output_file)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}