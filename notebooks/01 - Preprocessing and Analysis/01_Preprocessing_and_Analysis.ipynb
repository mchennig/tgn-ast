{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MocEEl6evMpO"
   },
   "source": [
    "# Preprocessing & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hq6JnUhcvUFi"
   },
   "source": [
    "This file contains all analysis and preprocessings performed on the ITSM datasets and was developed by Marc C. Hennig (mhennig@hm.edu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQbWuHdWhhbN"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKQT5Dhuhni0"
   },
   "source": [
    "## Dependency installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwEddQ2GrlRy"
   },
   "source": [
    "### A. PIP Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0luyK5STpuf"
   },
   "outputs": [],
   "source": [
    "!pip install ipdb pm4py category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcsrbOX9Bq_h"
   },
   "outputs": [],
   "source": [
    "# Workaround for installing torch dependencies on Colab without long waiting\n",
    "import torch\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html git+https://github.com/pyg-team/pytorch_geometric.git git+https://github.com/benedekrozemberczki/pytorch_geometric_temporal\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jkSkz2Dg4hi"
   },
   "source": [
    "### B. Git Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUi-s7Obg3xC"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/MaxVidgof/process-complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBG__Y3whra8"
   },
   "source": [
    "## Dependency Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h6Sq8RoTiLC"
   },
   "outputs": [],
   "source": [
    "# Python dependencies\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "import operator\n",
    "import statistics\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "from typing import List, Tuple, Union, Optional, Literal, Callable\n",
    "\n",
    "import time\n",
    "import calendar\n",
    "import dateutil\n",
    "from dateutil import rrule\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "# Debugging\n",
    "import ipdb\n",
    "\n",
    "# Colab dependencies\n",
    "from google.colab import files, drive, output\n",
    "\n",
    "# Basic dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Plotting dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#import pyvis\n",
    "\n",
    "# Process mining dependencies\n",
    "import pm4py\n",
    "\n",
    "# Graph dependencies\n",
    "#import raphtory as rp\n",
    "import networkx as nx\n",
    "\n",
    "# Machine learning depenencies\n",
    "import sklearn as sl\n",
    "import sklearn.metrics\n",
    "\n",
    "import category_encoders as ce\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import torch\n",
    "import torch_geometric as pyg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSaBMrqDhuKF"
   },
   "source": [
    "## Variables & Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7Tg7Nrrh1zG"
   },
   "outputs": [],
   "source": [
    "# Assign a random seed for reproduceability\n",
    "RANDOM_STATE = 1337\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Show all Pandas columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Set Matplotlib and Seaborn color scheme\n",
    "plt.rcParams[\"image.cmap\"] = \"Blues\"\n",
    "sns.set_palette(\"Blues\")\n",
    "\n",
    "# Set pytorch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Colab settings\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh37EJKuirTG"
   },
   "outputs": [],
   "source": [
    "# Google Drive folders\n",
    "GDRIVE_INPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/TGN-AST/Eventlogs\"\n",
    "GDRIVE_OUTPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/TGN-AST/Results\"\n",
    "\n",
    "# Local Colab folders\n",
    "UTIL_DIR = os.path.join(\".\", \"Util\")\n",
    "DATA_DIR = os.path.join(\".\", \"Data\")\n",
    "INPUT_DATA_DIR = os.path.join(DATA_DIR, \"Input\")\n",
    "INPUT_DATA_BPIC2013_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2013\")\n",
    "INPUT_DATA_BPIC2014_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2014\")\n",
    "INPUT_DATA_BPIC2015_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2015\")\n",
    "INTERIM_DATA_DIR = os.path.join(DATA_DIR, \"Interim\")\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"Output\")\n",
    "\n",
    "GRAPHIC_DIR = os.path.join(\".\", \"Graphics\")\n",
    "MODEL_DIR = os.path.join(\".\", \"Models\")\n",
    "\n",
    "Path(DATA_DIR).mkdir(exist_ok=True)\n",
    "Path(INTERIM_DATA_DIR).mkdir(exist_ok=True)\n",
    "Path(OUTPUT_DATA_DIR).mkdir(exist_ok=True)\n",
    "Path(GRAPHIC_DIR).mkdir(exist_ok=True)\n",
    "Path(MODEL_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8ttws9ksEcm"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kDkI1AkTKNY"
   },
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w25bjIuz_YN"
   },
   "source": [
    "### Cleaning & Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWFMaStGWnnc"
   },
   "outputs": [],
   "source": [
    "EVENTLOG_CASE = \"case:concept:name\"\n",
    "EVENTLOG_ACTIVITY = \"concept:name\"\n",
    "EVENTLOG_TIMESTAMP = \"time:timestamp\"\n",
    "EVENTLOG_GROUP = \"org:group\"\n",
    "EVENTLOG_RESOURCE = \"org:resource\"\n",
    "EVENTLOG_ROLE = \"org:role\"\n",
    "EVENTLOG_CASE_PREFIX = \"case:\"\n",
    "EVENTLOG_LABEL_PREFIX = \"label:\"\n",
    "\n",
    "EVENTLOG_LABEL_REM_TIME = f\"{EVENTLOG_LABEL_PREFIX}time:timestamp:last\"\n",
    "EVENTLOG_LABEL_NEXT_ACT = f\"{EVENTLOG_LABEL_PREFIX}concept:name:next\"\n",
    "EVENTLOG_LABEL_NEXT_TIME = f\"{EVENTLOG_LABEL_PREFIX}time:timestamp:next\"\n",
    "\n",
    "EVENTLOG_FEAT_TIME_OF_YEAR_SUFFIX = \":timeofyear\"\n",
    "EVENTLOG_FEAT_TIME_OF_MONTH_SUFFIX = \":timeofmonth\"\n",
    "EVENTLOG_FEAT_TIME_OF_WEEK_SUFFIX = \":timeofweek\"\n",
    "EVENTLOG_FEAT_TIME_OF_DAY_SUFFIX = \":timeofday\"\n",
    "EVENTLOG_FEAT_TIME_ELAPSED_CYCLE_SUFFIX = \":elapsedcycle\"\n",
    "EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX = \":elapsedprev\"\n",
    "\n",
    "TOKEN_PADDING = \"[PAD]\"\n",
    "TOKEN_PADDING_NUM = 0\n",
    "TOKEN_NA = \"[NA]\"\n",
    "TOKEN_EOC = \"[EOC]\"\n",
    "\n",
    "def df_find_case_attributes(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, label_prefix: str = EVENTLOG_LABEL_PREFIX, exclude_labels: bool = False) -> List[str]:\n",
    "  \"\"\"\n",
    "  Identifies and returns a list of attributes from the DataFrame that have a constant value within each case. Attributes are considered 'case attributes' if they have the same single value for all rows associated with a given case.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): A pandas DataFrame containing the event log data.\n",
    "    case_col (str, optional): The name of the column in df that represents the case identifier. Defaults to `EVENTLOG_CASE`.\n",
    "    label_prefix (str, optional): The prefix used to identify label columns within df. Defaults to `EVENTLOG_LABEL_PREFIX`.\n",
    "    exclude_labels (bool, optional): If True, attributes that are considered labels (i.e., start with label_prefix) will be excluded from the result.\n",
    "\n",
    "  Returns:\n",
    "    List[str]: A list of case attributes that are constant within each case. If `exclude_labels` is set to True, attributes considered as labels will not be included in the list.\n",
    "  \"\"\"\n",
    "  attrs = df.groupby(case_col).agg('nunique', dropna=False).agg('max', axis='rows')\n",
    "  attrs = attrs.where(attrs == 1).dropna().index.to_list()\n",
    "  if exclude_labels:\n",
    "    attrs = [attr for attr in attrs if attr not in df_find_labels(df, label_prefix)]\n",
    "  return attrs\n",
    "\n",
    "def df_find_event_attributes(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, label_prefix: str = EVENTLOG_LABEL_PREFIX, exclude_labels: bool = False) -> List[str]:\n",
    "  \"\"\"\n",
    "  Identifies and returns a list of attributes from the DataFrame that have varying values across events within the same case. Attributes are considered 'event attributes' if they do not have the same single value for all rows associated with a given case  (i.e., their value varies across events within the same case).\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): A pandas DataFrame containing the event log data.\n",
    "    case_col (str, optional): The name of the column in df that represents the case identifier. Defaults to `EVENTLOG_CASE`.\n",
    "    label_prefix (str, optional): The prefix used to identify label columns within df. Defaults to `EVENTLOG_LABEL_PREFIX`.\n",
    "    exclude_labels (bool, optional): If True, attributes that are considered labels (i.e., start with label_prefix) will be excluded from the result.\n",
    "\n",
    "  Returns:\n",
    "    List[str]: A list of event attributes that have more than one unique value within each case. If `exclude_labels` is set to True, attributes considered as labels will not be included in the list.\n",
    "  \"\"\"\n",
    "  attrs = df.groupby(case_col).agg('nunique', dropna=False).agg('max', axis='rows')\n",
    "  attrs = attrs.where(attrs > 1).dropna().index.to_list()\n",
    "  if exclude_labels:\n",
    "    attrs = [attr for attr in attrs if attr not in df_find_labels(df, label_prefix)]\n",
    "  return attrs\n",
    "\n",
    "\n",
    "\n",
    "def df_find_labels(df: pd.DataFrame, label_prefix: str = EVENTLOG_LABEL_PREFIX) -> List[str]:\n",
    "  \"\"\"\n",
    "  Identifies and returns a list of column names that are considered labels in the DataFrame based on a given prefix. Label columns are those that start with the specified `label_prefix`.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): A pandas DataFrame containing the event log data or similar structured data.\n",
    "    label_prefix (str, optional): The prefix used to identify label columns within df. Defaults to `EVENTLOG_LABEL_PREFIX`.\n",
    "\n",
    "  Returns:\n",
    "    List[str]: A list of column names that are identified as labels based on the prefix.\n",
    "  \"\"\"\n",
    "  return [col for col in df.columns if col.startswith(label_prefix)]\n",
    "\n",
    "def df_separate_categoricals(df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "  ordered_cols = []\n",
    "  unordered_cols = []\n",
    "  for col in df.select_dtypes(include='category').columns:\n",
    "    if df[col].cat.ordered:\n",
    "      ordered_cols.append(col)\n",
    "    else:\n",
    "      unordered_cols.append(col)\n",
    "  return ordered_cols, unordered_cols\n",
    "\n",
    "def df_convert_datetimes(df: pd.DataFrame, cols: List[str] = [], dayfirst: bool = False, yearfirst: bool = False, tz: Optional[Union[str, datetime.tzinfo]] = None) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Converts specified columns of a DataFrame to datetime format and localizes the datetime objects to the specified timezone if provided. Attempts to parse the columns as date times, optionally interpreting the day first or year first. If the initial parsing fails, it retries with the assumption that the datetime is in UTC. After conversion, the datetime objects may be localized to a specific timezone if `tz` is not None.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing columns to be converted to datetime.\n",
    "    cols (List[str], optional): The list of column names to convert to datetime. Defaults to an empty list.\n",
    "    dayfirst (bool, optional): Boolean indicating if the day is the first number in the date string. Defaults to False.\n",
    "    yearfirst (bool, optional): Boolean indicating if the year is the first number in the date string. Defaults to False.\n",
    "    tz (Optional[Union[str, datetime.tzinfo]], optional): Optional timezone information to which the datetimes will be localized. Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: The DataFrame with the specified columns converted to datetime format.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If parsing the dates fails even after assuming UTC.\n",
    "  \"\"\"\n",
    "  for col in cols:\n",
    "    try:\n",
    "      df[col] = pd.to_datetime(df[col], dayfirst=dayfirst, yearfirst=yearfirst)\n",
    "    except ValueError:\n",
    "      df[col] = pd.to_datetime(df[col], dayfirst=dayfirst, yearfirst=yearfirst, utc=True)\n",
    "\n",
    "    df[col] = df[col].dt.tz_localize(tz=tz)\n",
    "  return df\n",
    "\n",
    "def df_convert_timedeltas(df: pd.DataFrame, cols: List[str] = [], unit: str = 'nanoseconds') -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Converts specified columns of a DataFrame to timedelta format using the given time unit. Each value in the specified columns will be converted into a timedelta object, interpreting the value according to the specified unit.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing columns to be converted to timedelta.\n",
    "    cols (List[str], optional): The list of column names to convert to timedelta. Defaults to an empty list.\n",
    "    unit (str, optional): The time unit to interpret the values in `cols` when converting. Defaults to 'nanoseconds'.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: The DataFrame with the specified columns converted to timedelta format.\n",
    "  \"\"\"\n",
    "  for col in cols:\n",
    "    df[col] = pd.to_timedelta(df[col], unit=unit)\n",
    "  return df\n",
    "\n",
    "def df_convert_bools(df: pd.DataFrame, cols: list[str] = [], true_vals: Union[str, List[str]] = [], false_vals: Union[str, List[str]] = []) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Converts specified columns of a DataFrame to boolean format based on provided true and false values. String values from `true_vals` are mapped to `True`, while values from `false_vals` are mapped to `False`. All other values not included in `true_vals` or `false_vals` will be converted based on the presence of either list; if only `true_vals` is provided, all other values are considered `False`, and vice-versa.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing columns to be converted to boolean.\n",
    "    cols (List[str], optional): The list of column names to convert to boolean. Defaults to an empty list.\n",
    "    true_vals (Union[str, List[str]], optional): Values to be mapped to `True`. Can be a single string or a list of strings. Defaults to an empty list.\n",
    "    false_vals (Union[str, List[str]], optional): Values to be mapped to `False`. Can be a single string or a list of strings. Defaults to an empty list.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: The DataFrame with the specified columns converted to boolean format.\n",
    "\n",
    "  \"\"\"\n",
    "  if isinstance(true_vals, str):\n",
    "    true_vals = [true_vals]\n",
    "  if isinstance(false_vals, str):\n",
    "    false_vals = [false_vals]\n",
    "\n",
    "  map = {true_val: True for true_val in true_vals} | {false_val: False for false_val in false_vals}\n",
    "  if len(true_vals) == 0 and len(false_vals) > 0:\n",
    "    # Replace unknown values with True\n",
    "    map['__missing__'] = True\n",
    "  elif len(false_vals) == 0 and len(true_vals) > 0:\n",
    "    # Replace unknown values with False\n",
    "    map['__missing__'] = False\n",
    "  else:\n",
    "    map[\"__missing__\"] = pd.NA\n",
    "\n",
    "  for col in cols:\n",
    "    df[col] = df[col].map(map).astype('boolean')\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_convert_bool_to_int(df: pd.DataFrame, cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if cols is None:\n",
    "    cols = df.select_dtypes('boolean').columns\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  for col in cols:\n",
    "    df[col] = df[col].astype('Int8')\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_convert_ordered_cat_to_int(df: pd.DataFrame, cols: Optional[Union[str, List[str]]] = None, relative: bool = False) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if cols is None:\n",
    "    cols, _ = df_separate_categoricals(df)\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  for col in cols:\n",
    "    max_code = df[col].cat.codes.astype('Int16').max()\n",
    "    na_rows = df[df[col].isna()].index\n",
    "\n",
    "    df[col] = df[col].cat.codes.astype('Int16')\n",
    "    df.loc[na_rows, col] = pd.NA\n",
    "    if relative:\n",
    "      df[col] = df[col] / max_code\n",
    "\n",
    "\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_fillna_str(df: pd.DataFrame, val: str, cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if cols is None:\n",
    "    cols = df.select_dtypes('object').columns\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  for col in cols:\n",
    "    df[col] = df[col].astype('string').fillna(val)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_fillna_num(df: pd.DataFrame, val: Union[int, float], cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if cols is None:\n",
    "    cols = df.select_dtypes('number').columns\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  for col in cols:\n",
    "    df[col] = df[col].fillna(val)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_fillna_cat(df: pd.DataFrame, val: str, cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if cols is None:\n",
    "    cols = df.select_dtypes('category').columns\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  for col in cols:\n",
    "    if val not in df[col].cat.categories.array:\n",
    "      df[col] = df[col].cat.add_categories(val)\n",
    "    df[col] = df[col].fillna(val)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_rename_cat_values(df: pd.DataFrame, cols: Union[str, List[str]], from_cats: Union[str, List[str]], to_cat: str) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Changes one or more categorical values to another specified value within the provided columns, and removes any categories that are no longer used.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing categorical columns where values will be renamed.\n",
    "    cols (Union[str, List[str]]): A column name or list of column names to be modified.\n",
    "    from_cats (Union[str, List[str]]): The category or list of categories to be changed.\n",
    "    to_cat (str): The new category value that replaces the `from_cats`.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: The DataFrame with renamed categorical values and cleaned categories.\n",
    "\n",
    "  Raises:\n",
    "    TypeError: If the columns specified are not categorical dtype.\n",
    "  \"\"\"\n",
    "  if isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "  for col in cols:\n",
    "    df[col] = df[col].cat.remove_categories(from_cats).fillna(to_cat)\n",
    "    df[col] = df[col].cat.remove_unused_categories()\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_drop_duplicate_rows(df: pd.DataFrame, inplace: bool = True, ignore_index: bool = True, keep: str = 'first') -> Union[pd.DataFrame, None]:\n",
    "  \"\"\"\n",
    "  Removes duplicate rows from the DataFrame, optionally updating the DataFrame in place and resetting the index.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which duplicate rows will be removed.\n",
    "    inplace (bool, optional): If True, the DataFrame will be updated in place and None will be returned. Otherwise, a new DataFrame is returned. Defaults to True.\n",
    "    ignore_index (bool, optional): If True, the index will be reset to the default integer index after dropping duplicates. Otherwise, the original index will be preserved. This parameter is ignored when inplace is True. Defaults to True.\n",
    "    keep (str, optional): Determines which duplicates (if any) to keep.\n",
    "        - 'first': Drop duplicates except for the first occurrence.\n",
    "        - 'last': Drop duplicates except for the last occurrence.\n",
    "        - False: Drop all duplicates.\n",
    "        Defaults to 'first'.\n",
    "\n",
    "  Returns:\n",
    "    Union[pd.DataFrame, None]: The DataFrame with duplicate rows removed if inplace is set to False, otherwise None.\n",
    "  \"\"\"\n",
    "  df = df.drop_duplicates(keep=keep, ignore_index=ignore_index, inplace=inplace)\n",
    "  return df\n",
    "\n",
    "def df_drop_duplicate_cols(df: pd.DataFrame, keep: str = 'first') -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Removes duplicate columns from the DataFrame while keeping the first occurrence by default. It also ensures that the data types of the remaining columns are preserved.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which duplicate columns will be removed.\n",
    "    keep (str, optional): Determines which duplicates (if any) to keep.\n",
    "        - 'first': Drop duplicates except for the first occurrence.\n",
    "        - 'last': Drop duplicates except for the last occurrence.\n",
    "        - False: Drop all duplicates.\n",
    "        Defaults to 'first'.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A new DataFrame with duplicate columns removed and original data types intact.\n",
    "  \"\"\"\n",
    "  dtypes = df.dtypes\n",
    "  df = df.T.drop_duplicates(keep=keep).T\n",
    "  dtypes.drop(dtypes.index[~dtypes.index.isin(df.columns)], inplace=True)\n",
    "  return df.astype(dtypes)\n",
    "\n",
    "# Remove rows and columns that are completely empty\n",
    "def df_drop_na_rows_and_cols(df: pd.DataFrame, inplace: bool = True) -> Union[pd.DataFrame, None]:\n",
    "  \"\"\"\n",
    "  Removes rows and columns from the DataFrame that are completely empty (all values are NaN).\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which completely empty rows and columns will be removed.\n",
    "    inplace (bool, optional): If True, the DataFrame will be updated in place, which modifies the original DataFrame and returns None.\n",
    "                              If False, a new DataFrame is returned with the empty rows and columns removed.\n",
    "                              Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "    Union[pd.DataFrame, None]: None if inplace is True; otherwise, a new DataFrame with empty rows and columns removed.\n",
    "  \"\"\"\n",
    "  if inplace:\n",
    "    df.dropna(how=\"all\", axis='index', inplace=inplace)\n",
    "    df = df.dropna(how=\"all\", axis='columns', inplace=inplace)\n",
    "  else:\n",
    "    df = df.dropna(how=\"all\", axis='index', inplace=inplace).dropna(how=\"all\", axis='columns', inplace=inplace)\n",
    "  return df\n",
    "\n",
    "def df_drop_single_val_cols(df: pd.DataFrame, inplace: bool = True) -> Union[pd.DataFrame, None]:\n",
    "  \"\"\"\n",
    "  Removes columns from the DataFrame that only contain a single unique value.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which columns with a single unique value will be removed.\n",
    "    inplace (bool, optional): If True, the operation will be performed inplace and the function will return None.\n",
    "                              If False, a new DataFrame with the specified columns removed will be returned.\n",
    "                              Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "    Union[pd.DataFrame, None]: None if inplace is True; otherwise, a new DataFrame with columns that have a single unique value removed.\n",
    "  \"\"\"\n",
    "  df = df.drop(columns=df.columns[df.nunique(dropna=True) == 1], inplace=inplace)\n",
    "  return df\n",
    "\n",
    "def df_drop_threshold_cols(df: pd.DataFrame, gte: float = sys.float_info.min, lt: float = sys.float_info.max, cols: List[str] = [], absolute: bool = False) -> Union[pd.DataFrame, None]:\n",
    "  \"\"\"\n",
    "  Removes columns from the DataFrame where all values meet a threshold condition. Greater than or equal to `gte` and less than `lt` thresholds can be set, and optionally, absolute value conditions can be considered.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which columns will be dropped based on threshold conditions.\n",
    "    gte (float, optional): The 'greater than or equal to' threshold condition. Defaults to the smallest representable float.\n",
    "    lt (float, optional): The 'less than' threshold condition. Defaults to the largest representable float.\n",
    "    cols (List[str], optional): The list of columns to check for the threshold conditions. If empty, all numeric columns will be checked. Defaults to an empty list.\n",
    "    absolute (bool, optional): If True, the absolute values of the column data will be considered for the thresholds. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "    Union[pd.DataFrame, None]: The modified DataFrame with thresholded columns removed. As per the modification of the function, this will always return a new DataFrame and never None.\n",
    "  \"\"\"\n",
    "  df = df.copy()\n",
    "  if cols is None or len(cols) == 0:\n",
    "    cols = df.select_dtypes('number').columns.to_list()\n",
    "\n",
    "  if absolute:\n",
    "    if gte != sys.float_info.min:\n",
    "      drop_cols = df[cols].mask(df[cols].abs() >= abs(gte)).dropna(axis='columns', how='all').columns.to_list()\n",
    "    elif lt != sys.float_info.max:\n",
    "      drop_cols = df[cols].mask(df[cols].abs() < abs(lt)).dropna(axis='columns', how='all').columns.to_list()\n",
    "  else:\n",
    "    drop_cols = df[cols].mask((df[cols] >= gte) & (df[cols] < lt)).dropna(axis='columns', how='all').columns.to_list()\n",
    "\n",
    "  return df.drop(columns=drop_cols)\n",
    "\n",
    "def df_drop_threshold_na_cols(df: pd.DataFrame, threshold: Union[float, int], inplace: bool = True) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Removes columns from the DataFrame that have NaN values equal to or exceeding the specified threshold.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which columns with excessive NaN values will be removed.\n",
    "    threshold (Union[float, int]): The threshold for NaN values (absolute number or percentage). If provided as a float,\n",
    "                                   it is interpreted as a percentage of the total number of rows.\n",
    "    inplace (bool, optional): If True, the DataFrame will be updated in place, and None will be returned.\n",
    "                              If False, a new DataFrame with the specified columns removed will be returned.\n",
    "                              Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: The DataFrame with columns removed if `inplace` is False. If `inplace` is True, the original DataFrame is modified and the function will return None.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the threshold is greater than the size of the DataFrame.\n",
    "  \"\"\"\n",
    "  if isinstance(threshold, float):\n",
    "    threshold = threshold * len(df)\n",
    "\n",
    "  if threshold > len(df):\n",
    "    raise ValueError(f\"Threshold {threshold} must be lower than or equal to the number of rows in the DataFrame {len(df)}\")\n",
    "\n",
    "  df_na = df.isna().sum()\n",
    "  df_na = df_na[df_na >= threshold]\n",
    "\n",
    "  return df.drop(columns=df_na.index.array, inplace=inplace)\n",
    "\n",
    "def df_drop_threshold_rows(df: pd.DataFrame, gte: float = sys.float_info.min, lt: float = sys.float_info.max, cols: List[str] = [], absolute: bool = False) -> Union[pd.DataFrame, None]:\n",
    "  \"\"\"\n",
    "  Drops rows from the DataFrame where all numeric values in specified columns meet threshold conditions of either 'greater than or equal to' (`gte`) or 'less than' (`lt`). Optionally, absolute values can be considered for the thresholds.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which rows will be dropped.\n",
    "    gte (float, optional): The 'greater than or equal to' threshold condition. Defaults to the smallest representable float.\n",
    "    lt (float, optional): The 'less than' threshold condition. Defaults to the largest representable float.\n",
    "    cols (List[str], optional): The list of column names to check against the threshold conditions. If empty, all numeric columns will be checked. Defaults to an empty list.\n",
    "    absolute (bool, optional): If True, the absolute values of the data in the columns will be considered when comparing against the thresholds. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame with the specified rows dropped.\n",
    "  \"\"\"\n",
    "  df = df.copy()\n",
    "  if cols is None or len(cols) == 0:\n",
    "    cols = df.select_dtypes('number').columns.to_list()\n",
    "\n",
    "  if absolute:\n",
    "    if gte != sys.float_info.min:\n",
    "      drop_rows = df[cols].mask(df[cols].abs() >= abs(gte)).dropna(axis='index', how='all').index.to_list()\n",
    "    elif lt != sys.float_info.max:\n",
    "      drop_rows = df[cols].mask(df[cols].abs() < abs(lt)).dropna(axis='index', how='all').index.to_list()\n",
    "  else:\n",
    "    drop_rows = df[cols].mask((df[cols] >= gte) & (df[cols] < lt)).dropna(axis='index', how='all').index.to_list()\n",
    "\n",
    "  return df.drop(index=drop_rows)\n",
    "\n",
    "def df_format_as_eventlog(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, activity_col: str = EVENTLOG_ACTIVITY, time_col: str = EVENTLOG_TIMESTAMP, resource_col: Optional[str] = None, group_col: Optional[str] = None, role_col: Optional[str] = None, inplace: Optional[bool] = True, sort: Union[bool, str] = True):\n",
    "  col_map = {\n",
    "    case_col: EVENTLOG_CASE,\n",
    "    activity_col: EVENTLOG_ACTIVITY,\n",
    "    time_col: EVENTLOG_TIMESTAMP\n",
    "  }\n",
    "  if resource_col is not None:\n",
    "    col_map[resource_col] = EVENTLOG_RESOURCE\n",
    "  if group_col is not None:\n",
    "    col_map[group_col] = EVENTLOG_GROUP\n",
    "  if role_col is not None:\n",
    "    col_map[role_col] = EVENTLOG_ROLE\n",
    "\n",
    "  if sort and isinstance(sort, str):\n",
    "    sort_cols = [case_col, sort, time_col, activity_col]\n",
    "  else:\n",
    "    sort_cols = [case_col, time_col, activity_col]\n",
    "\n",
    "  case_attrs = df_find_case_attributes(df, case_col)\n",
    "  col_map = col_map | {attr: f\"{EVENTLOG_CASE_PREFIX}{attr}\" for attr in case_attrs if not attr.startswith(EVENTLOG_CASE_PREFIX)}\n",
    "\n",
    "  event_attrs = df_find_event_attributes(df, case_col)\n",
    "  col_map = col_map | {attr: f\"{attr.replace(EVENTLOG_CASE_PREFIX, '', 1)}\" for attr in event_attrs if attr.startswith(EVENTLOG_CASE_PREFIX)}\n",
    "\n",
    "  if sort and inplace:\n",
    "    df.sort_values(by=sort_cols, inplace=inplace, ignore_index=True)\n",
    "  elif sort and not inplace:\n",
    "    df = df.sort_values(by=sort_cols, inplace=inplace, ignore_index=True)\n",
    "\n",
    "  return df.rename(columns=col_map, inplace=inplace)\n",
    "\n",
    "def df_write_files(df: pd.DataFrame, filename: str, index: bool = False, skip_xes: bool = True) -> None:\n",
    "  df.to_csv(f\"{filename}.csv\", index=index)\n",
    "  df.to_pickle(f\"{filename}.pkl.gz\")\n",
    "  try:\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "      df = df.copy()\n",
    "      df.columns = df.columns.to_flat_index()\n",
    "    df.reset_index().to_feather(f\"{filename}.feather\")\n",
    "  except Exception as e:\n",
    "    print(f\"Skipping feather: {e}\")\n",
    "  if not skip_xes:\n",
    "    pm4py.write_xes(df, f\"{filename}.xes\")\n",
    "\n",
    "def df_datetime_to_numeric(df: pd.DataFrame, cols: Optional[Union[List[str], str]] = None, convert_datetime: Optional[bool] = True, convert_timedelta: Optional[bool] = True) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Converts datetime and timedelta columns in a DataFrame to a numeric representation. Datetime columns are converted to UNIX timestamps, and timedelta columns are converted to total seconds.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame with columns to be converted.\n",
    "    cols (Optional[Union[List[str], str]], optional): Columns to be converted. If None, all columns are considered. It can be a single column name or a list of column names. Defaults to None.\n",
    "    convert_datetime (Optional[bool], optional): Flag indicating whether to convert datetime columns. Defaults to True.\n",
    "    convert_timedelta (Optional[bool], optional): Flag indicating whether to convert timedelta columns. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame with the specified datetime and timedelta columns converted to numeric values.\n",
    "  \"\"\"\n",
    "  df = df.copy()\n",
    "  if cols is None:\n",
    "    cols = df.columns.to_list()\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  if convert_datetime:\n",
    "    for col in df.select_dtypes(include='datetime').columns.to_list():\n",
    "      if col in cols:\n",
    "        df[col] = df[col].map(pd.Timestamp.timestamp, na_action='ignore')\n",
    "\n",
    "  if convert_timedelta:\n",
    "    for col in df.select_dtypes(include='timedelta').columns.to_list():\n",
    "      if col in cols:\n",
    "        df[col] = df[col].dt.total_seconds()\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_timedelta_to_unit(df: pd.DataFrame, timedelta_col: str, unit: Optional[Literal['days', \"day\", \"d\", \"hours\", \"hour\", \"hr\", \"h\", \"m\", \"minute\", \"min\", \"minutes\", \"t\", \"s\", \"seconds\", \"sec\", \"second\"]], floor: bool = False, na_token: Optional[pd.Timedelta] = pd.NA) -> pd.DataFrame:\n",
    "  if not pd.isna(na_token):\n",
    "    na_token = pd.Timedelta(na_token)\n",
    "    df[timedelta_col].fillna(na_token, inplace=True)\n",
    "\n",
    "  if unit in [\"days\", \"day\", \"d\"]:\n",
    "    df[timedelta_col] = df[timedelta_col].dt.total_seconds() / 60 / 60 / 24\n",
    "  elif unit in [\"hours\", \"hour\", \"hr\", \"h\"]:\n",
    "    df[timedelta_col] = df[timedelta_col].dt.total_seconds() / 60 / 60\n",
    "  elif unit in [\"m\", \"minute\", \"min\", \"minutes\", \"t\"]:\n",
    "    df[timedelta_col] = df[timedelta_col].dt.total_seconds() / 60\n",
    "  elif unit in [\"s\", \"seconds\", \"sec\", \"second\"]:\n",
    "    df[timedelta_col] = df[timedelta_col].dt.total_seconds()\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid timedelta unit {unit}\")\n",
    "\n",
    "  if floor:\n",
    "    df[timedelta_col] = df[timedelta_col].astype('Int64')\n",
    "  return df\n",
    "\n",
    "def df_to_multiindex(df: pd.DataFrame, case_col: str = EVENTLOG_CASE) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df = df.groupby(case_col).apply(lambda x: x.reset_index(drop=True))\n",
    "  df.columns = pd.MultiIndex.from_arrays([df.columns, np.zeros(len(df.columns), dtype=int)])\n",
    "  return df\n",
    "\n",
    "def df_to_flatindex(df: pd.DataFrame):\n",
    "  df = df.copy()\n",
    "  if df.columns.nlevels > 1:\n",
    "    df.columns = [\"_\".join(map(str, col)) for col in df.columns.to_flat_index()]\n",
    "  return df.reset_index()\n",
    "\n",
    "def np_filter_na(arr: np.ndarray, unsqueeze: bool = False) -> np.ndarray:\n",
    "  if unsqueeze:\n",
    "    return np.array([np.expand_dims(el[~pd.isna(el)], -1) for el in arr], dtype='object')\n",
    "  else:\n",
    "    return np.array([el[~pd.isna(el)] for el in arr], dtype='object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9IL8jcMz3lw"
   },
   "source": [
    "### Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDOzLBZVgph6"
   },
   "outputs": [],
   "source": [
    "def df_label_next(df: pd.DataFrame, case_col: str, next_col: str, label_col: str, eoc_token: Optional[str]) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[label_col] = df.groupby(case_col)[next_col].transform(lambda x: x.shift(-1))\n",
    "  if eoc_token is not None:\n",
    "    df[label_col] = df[label_col].astype(\"string\").fillna(eoc_token)\n",
    "  return df\n",
    "\n",
    "def df_label_next_activity(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, activity_col: str = EVENTLOG_ACTIVITY, label_col: str = f\"{EVENTLOG_LABEL_PREFIX}concept:name:next\", eoc_token: Optional[str] = None) -> pd.DataFrame:\n",
    "  return df_label_next(df, case_col, activity_col, label_col, eoc_token)\n",
    "\n",
    "def df_label_next_resource(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, resource_col: str = EVENTLOG_RESOURCE, label_col: str = f\"{EVENTLOG_LABEL_PREFIX}org:resource:next\", eoc_token: Optional[str] = None) -> pd.DataFrame:\n",
    "  return df_label_next(df, case_col, resource_col, label_col, eoc_token)\n",
    "\n",
    "def df_label_next_group(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, group_col: str = EVENTLOG_GROUP, label_col: str = f\"{EVENTLOG_LABEL_PREFIX}org:group:next\", eoc_token: Optional[str] = None) -> pd.DataFrame:\n",
    "  return df_label_next(df, case_col, group_col, label_col, eoc_token)\n",
    "\n",
    "def df_label_activity_duration(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, time_col: str = EVENTLOG_TIMESTAMP, label_col: str = f\"{EVENTLOG_LABEL_PREFIX}time:timestamp:next\", unit: Optional[Literal[\"days\", \"day\", \"d\", \"hours\", \"hour\", \"hr\", \"h\", \"m\", \"minute\", \"min\", \"minutes\", \"t\", \"s\", \"seconds\", \"sec\", \"second\"]] = None, eoc_token: Optional[pd.Timedelta] = None) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[label_col] = df.groupby(case_col)[time_col].transform(lambda x: x.shift(-1)) - df[time_col]\n",
    "\n",
    "  if eoc_token is not None:\n",
    "    df[label_col] = df[label_col].fillna(eoc_token)\n",
    "\n",
    "  if unit is not None:\n",
    "    df = df_timedelta_to_unit(df, label_col, unit)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_label_remaining_cycle_time(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, time_col: str = EVENTLOG_TIMESTAMP, label_col: str = f\"{EVENTLOG_LABEL_PREFIX}time:timestamp:last\", unit: Optional[Literal[\"days\", \"day\", \"d\", \"hours\", \"hour\", \"hr\", \"h\", \"m\", \"minute\", \"min\", \"minutes\", \"t\", \"s\", \"seconds\", \"sec\", \"second\"]] = None) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if isinstance(unit, str):\n",
    "      unit = unit.lower()\n",
    "\n",
    "  df[label_col] = df.groupby(case_col)[time_col].transform('last') - df[time_col]\n",
    "  if unit is not None:\n",
    "    df = df_timedelta_to_unit(df, label_col, unit)\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBpoJDs8Q8l6"
   },
   "source": [
    "### Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj8OKYf6Q8F7"
   },
   "outputs": [],
   "source": [
    "def df_extract_activity_duration(df: pd.DataFrame, feat_col: str, previous_event_count: int = 1, unit: str = None, absolute: bool = True, time_col: str = EVENTLOG_TIMESTAMP, case_col: str = EVENTLOG_CASE, na_token: Optional[Union[pd.Timedelta, int]] = pd.NA) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "\n",
    "  df[feat_col] = df[time_col] - df.groupby(case_col)[time_col].transform(lambda x: x.shift(previous_event_count))\n",
    "\n",
    "  if not pd.isna(na_token):\n",
    "    na_token = pd.Timedelta(na_token)\n",
    "    df[feat_col].fillna(na_token, inplace=True)\n",
    "\n",
    "  if absolute:\n",
    "    df[feat_col] = df[feat_col].abs()\n",
    "\n",
    "  if unit is not None:\n",
    "    df = df_timedelta_to_unit(df, feat_col, unit, na_token=na_token)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_elapsed_cycle_time(df: pd.DataFrame, feat_col: str, unit: str = None, time_col: str = EVENTLOG_TIMESTAMP, case_col: str = EVENTLOG_CASE) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col] - df.groupby(case_col)[time_col].transform('first')\n",
    "\n",
    "  if unit is not None:\n",
    "    df = df_timedelta_to_unit(df, feat_col, unit)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_time_feats_tax(df: pd.DataFrame, time_col: str = EVENTLOG_TIMESTAMP, case_col: str = EVENTLOG_CASE, drop: bool = True) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df = df_extract_elapsed_cycle_time(df, feat_col=f\"{time_col}{EVENTLOG_FEAT_TIME_ELAPSED_CYCLE_SUFFIX}\", time_col=time_col, case_col=case_col, unit='s')\n",
    "  df = df_extract_activity_duration(df, feat_col=f\"{time_col}{EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX}\", time_col=time_col, unit='s')\n",
    "  df[f\"{time_col}{EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX}\"] = df[f\"{time_col}{EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX}\"].fillna(0)\n",
    "\n",
    "  df = df_extract_time_of_day(df, feat_col=f\"{time_col}{EVENTLOG_FEAT_TIME_OF_DAY_SUFFIX}\", time_col=time_col, unit='s')\n",
    "  df = df_extract_day_of_week(df, feat_col=f\"{time_col}{EVENTLOG_FEAT_TIME_OF_WEEK_SUFFIX}\", time_col=time_col)\n",
    "\n",
    "  df[f\"{time_col}{EVENTLOG_FEAT_TIME_OF_DAY_SUFFIX}\"] = df[f\"{time_col}{EVENTLOG_FEAT_TIME_OF_DAY_SUFFIX}\"] / (24 * 60 * 60)\n",
    "  df[f\"{time_col}{EVENTLOG_FEAT_TIME_OF_WEEK_SUFFIX}\"] = df[f\"{time_col}{EVENTLOG_FEAT_TIME_OF_WEEK_SUFFIX}\"] / 7\n",
    "\n",
    "  if drop:\n",
    "    df.drop(columns=time_col, inplace=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_time_feats(df: pd.DataFrame, time_col: str = EVENTLOG_TIMESTAMP, drop: bool = True):\n",
    "  df = df.copy()\n",
    "  df = df_extract_hour_of_day(df, feat_col=f\"{time_col}{EVENTLOG_FEAT_TIME_OF_DAY_SUFFIX}\", time_col=time_col, relative=True)\n",
    "  df = df_extract_day_of_week(df, feat_col=f\"{time_col}{EVENTLOG_FEAT_TIME_OF_WEEK_SUFFIX}\", time_col=time_col, relative=True)\n",
    "  df = df_extract_day_of_month(df, feat_col=f\"{time_col}{EVENTLOG_FEAT_TIME_OF_MONTH_SUFFIX}\", time_col=time_col, relative=True)\n",
    "  df = df_extract_month_of_year(df, feat_col=f\"{time_col}{EVENTLOG_FEAT_TIME_OF_YEAR_SUFFIX}\", time_col=time_col, relative=True)\n",
    "\n",
    "  if drop:\n",
    "    df.drop(columns=time_col, inplace=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_time_of_day(df: pd.DataFrame, feat_col: str, unit: str = None, time_col: str = EVENTLOG_TIMESTAMP) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col] - df[time_col].dt.normalize()\n",
    "\n",
    "  if unit is not None:\n",
    "    df = df_timedelta_to_unit(df, feat_col, unit)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_time_of_week(df: pd.DataFrame, feat_col: str, unit: str = None, time_col: str = EVENTLOG_TIMESTAMP) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col] - (df[time_col].dt.weekday * np.timedelta64(1, 'D'))\n",
    "\n",
    "  if unit is not None:\n",
    "    df = df_timedelta_to_unit(df, feat_col, unit)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_time_of_month(df: pd.DataFrame, feat_col: str, unit: str = None, time_col: str = EVENTLOG_TIMESTAMP) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col] - (df[time_col].dt.day * np.timedelta64(1, 'D'))\n",
    "\n",
    "  if unit is not None:\n",
    "    df = df_timedelta_to_unit(df, feat_col, unit)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_time_of_year(df: pd.DataFrame, feat_col: str, unit: str = None, time_col: str = EVENTLOG_TIMESTAMP) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col] - (df[time_col].dt.dayofyear * np.timedelta64(1, 'D'))\n",
    "\n",
    "  if unit is not None:\n",
    "    df = df_timedelta_to_unit(df, feat_col, unit)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_hour_of_day(df: pd.DataFrame, feat_col: str, time_col: str = EVENTLOG_TIMESTAMP, relative: bool = False) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col].dt.hour + 1\n",
    "\n",
    "  if relative:\n",
    "    df[feat_col] = df[feat_col] / 24\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_day_of_week(df: pd.DataFrame, feat_col: str, time_col: str = EVENTLOG_TIMESTAMP, relative: bool = False) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col].dt.day_of_week + 1\n",
    "\n",
    "  if relative:\n",
    "    df[feat_col] = df[feat_col] / 7\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_day_of_month(df: pd.DataFrame, feat_col: str, time_col: str = EVENTLOG_TIMESTAMP, relative: bool = False) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col].dt.day\n",
    "\n",
    "  if relative:\n",
    "    df[feat_col] = df[feat_col] / df[time_col].dt.days_in_month\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_day_of_year(df: pd.DataFrame, feat_col: str, time_col: str = EVENTLOG_TIMESTAMP, relative: bool = False) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col].dt.day_of_year\n",
    "\n",
    "  if relative:\n",
    "    df.loc[df[time_col].dt.is_leap_year, feat_col] = df[df[time_col].dt.is_leap_year][feat_col] / 366\n",
    "    df.loc[~df[time_col].dt.is_leap_year, feat_col] = df[~df[time_col].dt.is_leap_year][feat_col] / 365\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_month_of_year(df: pd.DataFrame, feat_col: str, time_col: str = EVENTLOG_TIMESTAMP, relative: bool = False) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  df[feat_col] = df[time_col].dt.month\n",
    "\n",
    "  if relative:\n",
    "    df[feat_col] = df[feat_col] / 12\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_extract_has_value(df: pd.DataFrame, feat_col: str, replace: bool = True) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if replace:\n",
    "    df[feat_col] = (~df[feat_col].isna()).astype('boolean')\n",
    "  else:\n",
    "    df[f\"has_{feat_col}\"] = (~df[feat_col].isna()).astype('boolean')\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJxW9HIDURJb"
   },
   "outputs": [],
   "source": [
    "class MeanScaler(sl.base.BaseEstimator, sl.base.TransformerMixin):\n",
    "  def __init__(self):\n",
    "    self.is_fitted_ = False\n",
    "\n",
    "  def reset(self):\n",
    "    if hasattr(self, \"mean_\"):\n",
    "      del self.mean_\n",
    "      del self.seen_\n",
    "      self.is_fitted_ = False\n",
    "\n",
    "    return self\n",
    "\n",
    "  def fit(self, X):\n",
    "    self.reset()\n",
    "    self.partial_fit(X)\n",
    "    return self\n",
    "\n",
    "  def partial_fit(self, X):\n",
    "    sample_mean = np.nanmean(X)\n",
    "    sample_seen = np.prod(np.shape(X))\n",
    "\n",
    "    if hasattr(self, \"mean_\"):\n",
    "      self.mean_ = ((self.mean_ * self.seen_) + (sample_mean * sample_seen)) / (self.seen_ + sample_seen)\n",
    "      self.seen_ += sample_seen\n",
    "    else:\n",
    "      self.mean_ = sample_mean\n",
    "      self.seen_ = sample_seen\n",
    "\n",
    "    self.is_fitted_ = True\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    if not self.is_fitted_:\n",
    "      raise ValueError(\"Transformer must be fitted.\")\n",
    "\n",
    "    return np.divide(X, self.mean_)\n",
    "\n",
    "  def fit_transform(self, X):\n",
    "    self.fit(X)\n",
    "    return self.transform(X)\n",
    "\n",
    "  def inverse_transform(self, X):\n",
    "    if not self.is_fitted_:\n",
    "      raise ValueError(\"Transformer must be fitted.\")\n",
    "\n",
    "    return np.multiply(X, self.mean_)\n",
    "\n",
    "def df_transform_log(df: pd.DataFrame, cols: Optional[Union[str, List[str]]] = None, base: Union[Literal['e', '2', '10', '1p'], int] = 'e', inf_token: Optional[float] = None, na_token: float = pd.NA) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if cols is None:\n",
    "    cols = df.columns.to_list()\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  if 'e' == base:\n",
    "    func = np.log\n",
    "  elif '2' == base:\n",
    "    func = np.log2\n",
    "  elif '10' == base:\n",
    "    func = np.log10\n",
    "  elif '1p' == base:\n",
    "    func = np.log1p\n",
    "  elif isinstance(base, int) and base > 0:\n",
    "    func = lambda s: np.log(s) / np.log(base)\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid log base {base}\")\n",
    "\n",
    "  for col in cols:\n",
    "    df[col] = df[col].transform(func).fillna(na_token)\n",
    "    if inf_token is not None:\n",
    "      df[col].replace([np.inf, -np.inf], inf_token, inplace=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_transform_minmax(df_train: pd.DataFrame, df_test: Optional[pd.DataFrame] = None, cols: Optional[Union[str, List[str]]] = None, feature_range: Tuple[float, float] = (0,1), fix_feature_range: bool = False, na_token: float = pd.NA) -> Union[pd.DataFrame, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "  if cols is None:\n",
    "    cols = df_train.columns.to_list()\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  scaler = sl.preprocessing.MinMaxScaler(\n",
    "      feature_range=feature_range,\n",
    "      clip=fix_feature_range\n",
    "  )\n",
    "\n",
    "  df_train[cols] = scaler.fit_transform(df_train[cols])\n",
    "  df_train[cols].fillna(na_token, inplace=True)\n",
    "  if df_test is not None:\n",
    "    df_test[cols] = scaler.transform(df_test[cols])\n",
    "    df_test[cols].fillna(na_token, inplace=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "  return df_train\n",
    "\n",
    "def df_encode_label(df_train: pd.DataFrame, df_test: Optional[pd.DataFrame] = None, cols: Optional[Union[str, List[str]]] = None) -> Union[pd.DataFrame, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "  if cols is None:\n",
    "    cols = df_train.columns.to_list()\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  encoder = sl.preprocessing.LabelEncoder()\n",
    "\n",
    "  for col in cols:\n",
    "    df_train[col] = encoder.fit_transform(df_train[col])\n",
    "    if df_test is not None:\n",
    "      df_test[col] = encoder.transform(df_test[col])\n",
    "\n",
    "  if df_test is not None:\n",
    "    return df_train, df_test\n",
    "\n",
    "  return df_train\n",
    "\n",
    "def df_encode_onehot(df: pd.DataFrame, cols: Optional[Union[str, List[str]]] = None, drop_first: bool = False, dummy_na: bool = True, prefix_sep: str = \":\", remove_unused_cats: bool = True) -> pd.DataFrame:\n",
    "  if cols is None:\n",
    "    _, cols = df_separate_categoricals(df)\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  if remove_unused_cats:\n",
    "    for col in cols:\n",
    "      if not isinstance(df[col], pd.DataFrame) and isinstance(df[col].dtype, pd.CategoricalDtype):\n",
    "        df[col] = df[col].cat.remove_unused_categories()\n",
    "\n",
    "  df = pd.get_dummies(\n",
    "      df,\n",
    "      columns=cols,\n",
    "      prefix_sep=prefix_sep,\n",
    "      dummy_na=dummy_na,\n",
    "      drop_first=drop_first,\n",
    "      sparse=True\n",
    "  )\n",
    "  return df\n",
    "\n",
    "def df_encode_ordinal(df: pd.DataFrame, cols: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  if cols is None:\n",
    "    cols, _ = df_separate_categoricals(df)\n",
    "  elif isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "  for col in cols:\n",
    "    df[col] = df[col].cat.codes.astype('Int8')\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paz01hy80HHR"
   },
   "source": [
    "### Statistic & Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaIHRDggTNqM"
   },
   "outputs": [],
   "source": [
    "def df_naive_regression_metrics(df_train: pd.DataFrame, df_test: pd.DataFrame, label_col: str, method: Literal['median', 'mean', 'mode'] = 'median'):\n",
    "  if 'median' == method:\n",
    "    y_pred = df_train[label_col].median()\n",
    "  elif 'mean' == method:\n",
    "    y_pred = df_train[label_col].mean()\n",
    "  elif 'mode' == method:\n",
    "    y_pred = df_train[label_col].mode()\n",
    "  else:\n",
    "    raise ValueError(f\"Unknown method {method}\")\n",
    "\n",
    "  y_pred = np.full(df_test[label_col].size, y_pred)\n",
    "  y_true = df_test[label_col].to_numpy()\n",
    "\n",
    "  return {\n",
    "      'mae': sl.metrics.mean_absolute_error(y_true, y_pred),\n",
    "      'mse': sl.metrics.mean_squared_error(y_true, y_pred),\n",
    "      'rmse': sl.metrics.root_mean_squared_error(y_true, y_pred),\n",
    "      'msle': sl.metrics.mean_squared_log_error(y_true, y_pred),\n",
    "      'logcosh': keras.losses.log_cosh(y_true, y_pred).numpy(),\n",
    "  }\n",
    "\n",
    "def df_naive_classification_metrics(df_train: pd.DataFrame, df_test: pd.DataFrame, label_col: str):\n",
    "  y_pred = df_train[EVENTLOG_LABEL_NEXT_ACT].mode().iloc[0]\n",
    "\n",
    "  label_enc = sl.preprocessing.LabelEncoder()\n",
    "  label_enc.fit(np.concatenate((df_train[label_col], df_test[label_col]), axis=None))\n",
    "\n",
    "  y_pred = label_enc.transform(np.full(df_test[label_col].size, y_pred))\n",
    "  y_true = label_enc.transform(df_test[label_col])\n",
    "\n",
    "  return {\n",
    "      'accuracy': sl.metrics.accuracy_score(y_true, y_pred),\n",
    "      'f1_macro': sl.metrics.f1_score(y_true, y_pred, average='macro'),\n",
    "      'precision_macro': sl.metrics.precision_score(y_true, y_pred, average='macro'),\n",
    "      'recall_macro': sl.metrics.recall_score(y_true, y_pred, average='macro'),\n",
    "      'f1_micro': sl.metrics.f1_score(y_true, y_pred, average='micro'),\n",
    "      'precision_micro': sl.metrics.precision_score(y_true, y_pred, average='micro'),\n",
    "      'recall_micro': sl.metrics.recall_score(y_true, y_pred, average='micro'),\n",
    "  }\n",
    "\n",
    "\n",
    "def df_predictive_power_scores(df: pd.DataFrame, label_col: str, variable_cols: Optional[Union[str, List[str]]] = None, datetime_is_numeric: Optional [bool] = False, exclude_labels: Optional[bool] = True, threshold: float = 0.0) -> Tuple[pd.DataFrame, plt.Axes]:\n",
    "  df = df.copy()\n",
    "  if variable_cols is None:\n",
    "    if exclude_labels:\n",
    "      variable_cols = df.columns[~df.columns.str.startswith(EVENTLOG_LABEL_PREFIX) | (df.columns == label_col)].to_list()\n",
    "    else:\n",
    "      variable_cols = df.columns.to_list()\n",
    "  elif isinstance(variable_cols, str):\n",
    "    variable_cols = [variable_cols, label_col]\n",
    "\n",
    "  df = df[variable_cols]\n",
    "\n",
    "  if threshold is None or threshold < 0 or threshold > 1:\n",
    "    raise ValueError(f\"threshold must be in the interval [0, 1] but was {threshold}\")\n",
    "  if datetime_is_numeric:\n",
    "    df = df_datetime_to_numeric(df)\n",
    "\n",
    "  df_predictors = pps.predictors(df, label_col, output=\"df\", sorted=True, catch_errors=False, random_seed=RANDOM_STATE, invalid_score=np.NaN, cross_validation=2)\n",
    "  fig, ax = plt.subplots(figsize=(10,10))\n",
    "  barplot = sns.barplot(data=df_predictors[df_predictors[\"is_valid_score\"] & df_predictors[\"ppscore\"] >= threshold], x=\"ppscore\", y=\"x\", orient=\"h\", ax=ax).set_title(f\"Predictive power score {label_col}\")\n",
    "  return df_predictors, barplot\n",
    "\n",
    "def df_predictive_power_matrix(df: pd.DataFrame, datetime_is_numeric: Optional[bool] = False, threshold: float = 0.0) -> Tuple[pd.DataFrame, plt.Axes]:\n",
    "  df = df.copy()\n",
    "  if threshold is None or threshold < 0 or threshold > 1:\n",
    "    raise ValueError(f\"threshold must be in the interval [0, 1] but was {threshold}\")\n",
    "  if datetime_is_numeric:\n",
    "    df = df_datetime_to_numeric(df)\n",
    "\n",
    "  df_matrix = pps.matrix(df, output=\"df\", sorted=True, catch_errors=False, random_seed=RANDOM_STATE, invalid_score=np.NaN, cross_validation=2)\n",
    "  df_heatmap = df_matrix[df_matrix[\"is_valid_score\"] & df_matrix[\"ppscore\"] >= threshold][['x', 'y', 'ppscore']].pivot(columns='y', index='x', values='ppscore')\n",
    "  fig, ax = plt.subplots(figsize=(30,30))\n",
    "  heatmap = sns.heatmap(df_heatmap, vmin=0, vmax=1, linewidths=0.5, annot=True, ax=ax).set_title(\"Predictive power matrix\")\n",
    "  return df_matrix, heatmap\n",
    "\n",
    "def df_correlation_matrix(df: pd.DataFrame, datetime_is_numeric: Optional[bool] = False, method: str = \"pearson\", threshold: float = 0.0) -> Tuple[pd.DataFrame, plt.Axes]:\n",
    "  df = df.copy()\n",
    "  if threshold is None or threshold < 0 or threshold > 1:\n",
    "    raise ValueError(f\"threshold must be in the interval [0, 1] but was {threshold}\")\n",
    "  if datetime_is_numeric:\n",
    "    df = df_datetime_to_numeric(df)\n",
    "\n",
    "  ordered_cols, unordered_cols = df_separate_categoricals(df)\n",
    "\n",
    "  for ordinal_col in ordered_cols:\n",
    "    df[ordinal_col] = df[ordinal_col].cat.codes\n",
    "\n",
    "  if len(unordered_cols) > 0:\n",
    "    df = df.join(pd.get_dummies(df[unordered_cols], sparse=True)).drop(columns=unordered_cols)\n",
    "\n",
    "  df_corr = df.corr(method=method, numeric_only=True)\n",
    "  fig, ax = plt.subplots(figsize=(50,50))\n",
    "  heatmap = sns.heatmap(df_corr, vmin=-1, vmax=1, annot=True, ax=ax).set_title(f\"{method} correlation\")\n",
    "\n",
    "  return df_corr, heatmap\n",
    "\n",
    "def df_case_length_stats(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, result_col: str = \"Case Length\", percentiles: List[float] = np.arange(.05, 1, .05)) -> pd.DataFrame:\n",
    "  df = df.groupby(case_col).size()\n",
    "  return pd.DataFrame(data={result_col: df.describe(percentiles=percentiles)})\n",
    "\n",
    "def df_case_duration_stats(df: pd.DataFrame, case_col: str = EVENTLOG_CASE, time_col: str = EVENTLOG_TIMESTAMP, result_col: str = \"Case Duration\", percentiles: List[float] = np.arange(.05, 1, .05)) -> pd.DataFrame:\n",
    "  df = df.groupby(case_col)[time_col].max() - df.groupby(case_col)[time_col].min()\n",
    "  return pd.DataFrame(data={result_col: df.describe(percentiles=percentiles)})\n",
    "\n",
    "def df_correlation_scores(df: pd.DataFrame, label_col: str, variable_cols: Optional[Union[str, List[str]]] = None, datetime_is_numeric: Optional [bool] = False, exclude_labels: Optional[bool] = True, method: str = \"pearson\", threshold: float = 0.0) -> Tuple[pd.DataFrame, plt.Axes]:\n",
    "  df = df.copy()\n",
    "  if threshold is None or threshold < 0 or threshold > 1:\n",
    "    raise ValueError(f\"threshold must be in the interval [0, 1] but was {threshold}\")\n",
    "  if datetime_is_numeric:\n",
    "    df = df_datetime_to_numeric(df)\n",
    "  if variable_cols is None or len(variable_cols) == 0:\n",
    "    variable_cols = df.select_dtypes(include=[\"number\", \"category\"]).columns.to_list()\n",
    "\n",
    "  df = df[variable_cols]\n",
    "  ordered_cols, unordered_cols = df_separate_categoricals(df)\n",
    "\n",
    "  for ordinal_col in ordered_cols:\n",
    "    df[ordinal_col] = df[ordinal_col].cat.codes\n",
    "\n",
    "  if len(unordered_cols) > 0:\n",
    "    df = df.join(pd.get_dummies(df[unordered_cols], sparse=True)).drop(columns=unordered_cols)\n",
    "  variable_cols = df.columns.to_list()\n",
    "\n",
    "  df_corr = pd.DataFrame(index=variable_cols, columns=[f\"{method} correlation\"], dtype=\"float\")\n",
    "\n",
    "  for col in variable_cols:\n",
    "    if col == label_col:\n",
    "      continue\n",
    "    df_corr.loc[col][f\"{method} correlation\"] = df[col].dropna().astype(\"float\").corr(df[label_col])\n",
    "\n",
    "  df_corr.sort_values(by=[f\"{method} correlation\"], inplace=True)\n",
    "  fig, ax = plt.subplots(figsize=(100,100))\n",
    "  barplot = sns.barplot(data=df_corr, x=f\"{method} correlation\", y=df_corr.index, orient=\"h\", ax=ax).set_title(label_col)\n",
    "\n",
    "  return df_corr, barplot\n",
    "\n",
    "def df_visualize_strict_temporal_splitting(df_train: pd.DataFrame, df_test: pd.DataFrame, time_col: str = EVENTLOG_TIMESTAMP) -> plt.Axes:\n",
    "  s_months_train = df_train[time_col].dt.to_period('M').value_counts()\n",
    "  s_months_test_before_sep = df_test[df_test[df_find_labels(df_test)].isna().any(axis=1)][time_col].dt.to_period('M').value_counts()\n",
    "  s_months_test_after_sep = df_test[~df_test[df_find_labels(df_test)].isna().any(axis=1)][time_col].dt.to_period('M').value_counts()\n",
    "\n",
    "  df = pd.concat([\n",
    "    s_months_train.rename(\"Training Set Correct\"),\n",
    "    s_months_test_before_sep.rename(\"Training Set Wrong\"),\n",
    "    s_months_test_after_sep.rename(\"Test Set\")\n",
    "  ], axis=1).sort_index().fillna(0)\n",
    "  return df.plot(kind='bar', stacked=True, color=['green', 'red', 'grey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7oFURJN9A7J"
   },
   "source": [
    "### Eventlog Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrb3VNQH9AZ-"
   },
   "outputs": [],
   "source": [
    "def df_filter_date_range(df: pd.DataFrame, min: Union[pd.Timestamp, str, float] = 0.0, max: Union[pd.Timestamp, str, float] = 1.0, case_col: str = EVENTLOG_CASE, time_col: str = EVENTLOG_TIMESTAMP, mode: Literal['events', 'traces_contained', 'traces_intersecting'] = 'traces_contained') -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Filters a DataFrame for events or traces within a specified date range.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to filter.\n",
    "    min (Union[pd.Timestamp, str, float], optional): The minimum timestamp (inclusive)  for filtering, which can be an actual timestamp, a string representation of a date, or a quantile between 0.0 and 1.0 (default is 0.0).\n",
    "    max (Union[pd.Timestamp, str, float], optional): The maximum timestamp (inclusive) for filtering, which can be an actual timestamp, a string representation of a date, or a quantile between 0.0 and 1.0 (default is 1.0).\n",
    "    case_col (str, optional): The column name in the DataFrame that represents the case ID.\n",
    "    time_col (str, optional): The column name in the DataFrame that represents the timestamp.\n",
    "    mode (Literal['events', 'traces_contained', 'traces_intersecting'], optional): The mode determining how filtering is applied (default is 'traces_contained').\n",
    "      - 'events': keep events between min and max\n",
    "      - 'traces_contained': keep traces where all events are between min and max\n",
    "      - 'traces_intersecting': keep traces where any event is between min and max\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame filtered according to the specified date range and mode\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the `min` value is not smaller than the `max` value.\n",
    "  \"\"\"\n",
    "  df = df.copy()\n",
    "\n",
    "  if isinstance(min, float):\n",
    "    min = df[time_col].min() if 0.0 == min else df[time_col].quantile(min)\n",
    "  elif not isinstance(min, pd.Timestamp):\n",
    "    min = pd.to_datetime(min)\n",
    "  if isinstance(max, float):\n",
    "    max = df[time_col].max() if 1.0 == max else df[time_col].quantile(max)\n",
    "  elif not isinstance(max, pd.Timestamp):\n",
    "    max = pd.to_datetime(max)\n",
    "\n",
    "  if min >= max:\n",
    "    raise ValueError(f\"min {min} must be smaller than max {max}\")\n",
    "\n",
    "  if 'events' == mode:\n",
    "    df = df[(df[time_col] >= min) & (df[time_col] <= max)]\n",
    "  elif 'traces_contained' == mode:\n",
    "    df = df.groupby(case_col).filter(lambda df_group: ((df_group[time_col] >= min) & (df_group[time_col] <= max)).all())\n",
    "  elif 'traces_intersecting' == mode:\n",
    "    df = df.groupby(case_col).filter(lambda df_group: ((df_group[time_col] >= min) & (df_group[time_col] <= max)).any())\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_filter_case_size_range(df: pd.DataFrame, min: Union[int, float] = 0.0, max: Union[int, float] = 1.0, case_col: str = EVENTLOG_CASE) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Filters a DataFrame based on the number of events within each case falling between a specified range.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to filter.\n",
    "    min (Union[int, float], optional): The minimum number of events a case must contain. If a float representing a quantile is provided, the minimum number is computed based on the distribution of case sizes (default is 0.0 which represents the smallest case size).\n",
    "    max (Union[int, float], optional): The maximum number of events a case can contain. If a float representing a quantile is provided, the maximum number is computed based on the distribution of case sizes (default is 1.0 which represents the largest case size).\n",
    "    case_col (str, optional): The column name in the DataFrame that represents case IDs.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame filtered by cases with a number of events within the specified range.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the `min` value is not smaller than the `max` value.\n",
    "  \"\"\"\n",
    "  df = df.copy()\n",
    "\n",
    "  if isinstance(min, float):\n",
    "    min = df.groupby(case_col).size().min() if 0.0 == min else df.groupby(case_col).size().quantile(min)\n",
    "  if isinstance(max, float):\n",
    "    max = df.groupby(case_col).size().max() if 1.0 == max else df.groupby(case_col).size().quantile(max)\n",
    "\n",
    "  if min >= max:\n",
    "    raise ValueError(f\"min {min} must be smaller than max {max}\")\n",
    "\n",
    "  df = df.groupby(case_col).filter(lambda df_group: len(df_group) >= min and len(df_group) <= max)\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_filter_case_duration_range(df: pd.DataFrame, min: Union[pd.Timedelta, str, float] = 0.0, max: Union[pd.Timedelta, str, float] = 1.0, case_col: str = EVENTLOG_CASE, time_col: str = EVENTLOG_TIMESTAMP) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Filters a DataFrame based on the duration of cases falling between a specified time range.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to filter.\n",
    "    min (Union[pd.Timedelta, str, float], optional): The minimum duration for a case to be included. A float represents a quantile of case durations (0.0 being the shortest duration), a Timedelta or a string that can be converted to a Timedelta. The default is 0.0, which uses the shortest case duration.\n",
    "    max (Union[pd.Timedelta, str, float], optional): The maximum duration for a case to be included. A float represents a quantile of case durations (1.0 being the longest duration), a Timedelta or a string that can be converted to a Timedelta. The default is 1.0, which uses the longest case duration.\n",
    "    case_col (str, optional): The column name in the DataFrame that identifies the case ID.\n",
    "    time_col (str, optional): The column name in the DataFrame that identifies the timestamp.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame containing cases with durations within the specified range.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the `min` value is not smaller than the `max` value.\n",
    "  \"\"\"\n",
    "  df = df.copy()\n",
    "  df_dur = df.groupby(case_col)[time_col].max() - df.groupby(case_col)[time_col].min()\n",
    "\n",
    "  if isinstance(min, float):\n",
    "    min = df_dur.min() if 0.0 == min else df_dur.quantile(min)\n",
    "  elif not isinstance(min, pd.Timedelta):\n",
    "    min = pd.Timedelta(min)\n",
    "  if isinstance(max, float):\n",
    "    max = df_dur.max() if 1.0 == max else df_dur.quantile(max)\n",
    "  elif not isinstance(max, pd.Timedelta):\n",
    "    max = pd.Timedelta(max)\n",
    "\n",
    "  if min >= max:\n",
    "    raise ValueError(f\"min {min} must be smaller than max {max}\")\n",
    "\n",
    "  df_dur = df_dur[(df_dur >= min) & (df_dur <= max)]\n",
    "  df_filtered = df[df[case_col].isin(df_dur.index)]\n",
    "\n",
    "  return df_filtered.reset_index(drop=True)\n",
    "\n",
    "def df_prefix_pad_attributes(df: pd.DataFrame, attr_cols: Union[str, List[str]], case_col: str = EVENTLOG_CASE) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Performs prefix padding of attributes for events within each case by forward filling missing data. Attributes of events earlier in the case are propagated to later events, ensuring that each event within the case has the full history of attribute values up to that point.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing event log data.\n",
    "    attr_cols (Union[str, List[str]]): A list of attribute column names or a single attribute column name that should be prefix padded within each case.\n",
    "    case_col (str, optional): The column name in the DataFrame that identifies cases.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame that has been pivot transformed to have attribute columns for each event in the case with forward filled values to reflect the prefix padding.\n",
    "\n",
    "  Note:\n",
    "    If `attr_cols` includes the case ID column, it will be removed from the attributes to avoid errors in the process.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if isinstance(attr_cols, str):\n",
    "    attr_cols = [attr_cols]\n",
    "\n",
    "  if case_col in attr_cols:\n",
    "    attr_cols.remove(case_col)\n",
    "  df = df[[case_col] + attr_cols]\n",
    "\n",
    "  df[\"Event in Case\"] = df.groupby(case_col).cumcount()\n",
    "  df[\"Attribute in Case\"] = df[\"Event in Case\"]\n",
    "\n",
    "  df_pivot = df.pivot(index=[case_col, \"Event in Case\"], columns=[\"Attribute in Case\"], values=attr_cols)\n",
    "  df_pivot = df_pivot.groupby(case_col).ffill()\n",
    "\n",
    "  return df_pivot\n",
    "\n",
    "def df_encode_categories(df: pd.DataFrame, threshold: Optional[int] = None):\n",
    "  df = df.copy()\n",
    "\n",
    "  ordinal_cols, onehot_cols = df_separate_categoricals(df)\n",
    "\n",
    "  for col in ordinal_cols:\n",
    "    df[col] = df[col].cat.codes\n",
    "\n",
    "  if threshold is not None and threshold > 0:\n",
    "    onehot_cols = [col for col in onehot_cols if len(df[col].cat.categories) <= threshold]\n",
    "\n",
    "  df = pd.get_dummies(df, columns=onehot_cols)\n",
    "\n",
    "  bool_cols = df.select_dtypes('boolean')\n",
    "  for col in bool_cols:\n",
    "    df[col] = df[col].astype('Int8')\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_random_train_test_split(df: pd.DataFrame, test_len: float = 0.2, case_col: str = EVENTLOG_CASE, random_state: int = RANDOM_STATE) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "  case_ids = df[case_col].unique()\n",
    "\n",
    "  case_ids_train, case_ids_test = sl.model_selection.train_test_split(\n",
    "    case_ids,\n",
    "    test_size=test_len,\n",
    "    random_state=random_state,\n",
    "    shuffle=True\n",
    "  )\n",
    "\n",
    "  return df[df[case_col].isin(case_ids_train)].reset_index(drop=True), df[df[case_col].isin(case_ids_test)].reset_index(drop=True)\n",
    "\n",
    "def df_temporal_train_test_split(df: pd.DataFrame, test_len: float = 0.2, split_mode: Literal['event', 'case_start'] = 'event', filter_mode: Literal['events', 'traces'] = 'traces', case_col: str = EVENTLOG_CASE, time_col: str = EVENTLOG_TIMESTAMP) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "  \"\"\"\n",
    "  Splits a DataFrame into train and test sets based on a temporal split.\n",
    "\n",
    "  Parameters:\n",
    "    df (pd.DataFrame): A pandas DataFrame containing the event log data.\n",
    "    test_len (float, optional): The proportion of the data to include in the test split. Defaults to 0.2.\n",
    "    split_mode (Literal['event', 'case_start'], optional): The mode to use for splitting the data. 'event' splits based on the timestamp of individual events, while 'case_start' splits based on the timestamp of the first event in each case. Defaults to 'event'.\n",
    "    filter_mode (Literal['events', 'traces'], optional): The mode to use for filtering the data after splitting. 'events' filters individual events, while 'traces' filters entire cases. Defaults to 'traces'.\n",
    "    case_col (str, optional): The name of the column in df that represents the case identifier. Defaults to `EVENTLOG_CASE`.\n",
    "    time_col (str, optional): The name of the column in df that represents the timestamp. Defaults to `EVENTLOG_TIMESTAMP`.\n",
    "\n",
    "  Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the train and test DataFrames.\n",
    "  \"\"\"\n",
    "  if 'event' == split_mode:\n",
    "    test_start = df[time_col].quantile(q=(1 - test_len), interpolation='higher')\n",
    "  elif 'case_start' == split_mode:\n",
    "    test_start = df.groupby(case_col)[time_col].min().quantile(q=(1 - test_len), interpolation='higher')\n",
    "  else:\n",
    "    raise ValueError(f\"Unknown split mode {split_mode}\")\n",
    "\n",
    "  if 'events' == filter_mode:\n",
    "    df_test = df_filter_date_range(df, min=test_start, case_col=case_col, time_col=time_col, mode='events')\n",
    "    df_train = df_filter_date_range(df, max=test_start, case_col=case_col, time_col=time_col, mode='events')\n",
    "  elif 'traces' == filter_mode:\n",
    "    df_test = df_filter_date_range(df, min=test_start, case_col=case_col, time_col=time_col, mode='traces_contained')\n",
    "    df_train = df[~df[case_col].isin(df_test[case_col])]\n",
    "  else:\n",
    "    raise ValueError(f\"Unknown filter mode {filter_mode}\")\n",
    "\n",
    "  return df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "def df_strict_temporal_train_test_split(df: pd.DataFrame, test_len: float = 0.2, label_cols: Union[List[str], str] = [], latest_start: Optional[Union[pd.Timestamp, str]] = None, case_col: str = EVENTLOG_CASE, time_col: str = EVENTLOG_TIMESTAMP, debias_start: bool = True, debias_end: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into train and test sets based on a strict temporal split. The code is based on the works of Weytjens & De Weerdt (2022) (see https://github.com/hansweytjens/predictive-process-monitoring-benchmarks/blob/main/create_benchmarks.py) \n",
    "    \"\"\"\n",
    "    if isinstance(label_cols, str):\n",
    "      label_cols = [label_cols]\n",
    "\n",
    "    if latest_start is None:\n",
    "      df_dur = df.groupby(case_col)[time_col].max() - df.groupby(case_col)[time_col].min()\n",
    "      latest_start = df[time_col].max() - df_dur.max()\n",
    "    elif not isinstance(latest_start, pd.Timestamp):\n",
    "      latest_start = pd.Timestamp(latest_start)\n",
    "\n",
    "    case_starts_df = df.groupby(case_col)[time_col].min()\n",
    "    case_nr_list_start = case_starts_df.sort_values().index.array\n",
    "    case_stops_df = df.groupby(case_col)[time_col].max().to_frame()\n",
    "\n",
    "    ### TEST SET ###\n",
    "    first_test_case_nr = int(len(case_nr_list_start) * (1 - test_len))\n",
    "    first_test_start_time = np.sort(case_starts_df.values)[first_test_case_nr]\n",
    "    # retain cases that end after first_test_start time\n",
    "    test_case_nrs = case_stops_df[case_stops_df[time_col].values >= first_test_start_time].index.array\n",
    "    df_test_all = df[df[case_col].isin(test_case_nrs)].reset_index(drop=True)\n",
    "\n",
    "    # drop prefixes in test set that are past latest_start\n",
    "    if debias_end:\n",
    "      df_test = df_test_all[df_test_all[time_col] <= latest_start]\n",
    "    else:\n",
    "      df_test = df_test_all\n",
    "\n",
    "    if debias_start:\n",
    "    # convert targets into np.NAN for those prefixes that end before the separation time (beginning of test set)\n",
    "      df_test.loc[df_test[time_col].values < first_test_start_time, label_cols] = pd.NA\n",
    "\n",
    "    #### TRAINING SET ###\n",
    "    train_case_nrs = case_stops_df[case_stops_df[time_col].values < first_test_start_time].index.array  # added values\n",
    "    df_train = df[df[case_col].isin(train_case_nrs)].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmfbMVjoDExy"
   },
   "source": [
    "### Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFtjgnV-DEBm"
   },
   "outputs": [],
   "source": [
    "def ds_write_files(ds: tf.data.Dataset, filename: str, compression: Optional[str] = 'zip') -> None:\n",
    "  ds.save(filename, compression='GZIP' if compression is not None else compression)\n",
    "  if compression is not None:\n",
    "    shutil.make_archive(\n",
    "      filename,\n",
    "      format=compression,\n",
    "      root_dir=filename\n",
    "    )\n",
    "\n",
    "def ds_find_static_attrs(ds: tf.data.Dataset):\n",
    "  input, _ = ds.element_spec\n",
    "\n",
    "  stat_attrs = []\n",
    "  for key, value in input.items():\n",
    "    if 1 == value.shape.num_elements():\n",
    "      stat_attrs.append(key)\n",
    "\n",
    "  return stat_attrs\n",
    "\n",
    "def ds_find_dynamic_attrs(ds: tf.data.Dataset):\n",
    "  input, _ = ds.element_spec\n",
    "\n",
    "  dyn_attrs = []\n",
    "  for key, value in input.items():\n",
    "    if 1 < value.shape.num_elements():\n",
    "      dyn_attrs.append(key)\n",
    "\n",
    "  return dyn_attrs\n",
    "\n",
    "def ds_find_max_seq_len(ds: tf.data.Dataset):\n",
    "  input, _ = ds.element_spec\n",
    "\n",
    "  dyn_attrs = ds_find_dynamic_attrs(ds)\n",
    "  dyn_attr_lens = set()\n",
    "  for dyn_attr in dyn_attrs:\n",
    "    dyn_attr_lens.add(input[dyn_attr].shape.num_elements())\n",
    "\n",
    "  if len(dyn_attr_lens) > 1:\n",
    "    raise ValueError(\"Different sequence lengths for different attributes.\")\n",
    "\n",
    "  return dyn_attr_lens.pop()\n",
    "\n",
    "def ds_sequentialize_static_attrs(ds: tf.data.Dataset, activity_attr: str = \"activity\", pad_token: str = TOKEN_PADDING, pad_token_num = int(TOKEN_PADDING_NUM)) -> tf.data.Dataset:\n",
    "  max_seq_len = ds_find_max_seq_len(ds)\n",
    "  stat_attrs = ds_find_static_attrs(ds)\n",
    "\n",
    "  def ds_sequentialize_map_helper(inputs, outputs):\n",
    "    # Find the current sequence length by looking for the padding token in the 'activity' attribute\n",
    "    # Assumes 'activity' is a dynamic attribute holding the sequence\n",
    "    seq_len = tf.argmax(tf.equal(inputs['activity'], pad_token), axis=-1)\n",
    "\n",
    "    for attr in stat_attrs:\n",
    "      # Repeat static attribute for the sequence length and pad to the max sequence length\n",
    "      attr_val_repeated = tf.repeat(inputs[attr], repeats=seq_len, axis=0)\n",
    "      paddings = [[0, max_seq_len - seq_len]]\n",
    "      pad_const = pad_token_num if attr_val_repeated.dtype.is_numeric else pad_token\n",
    "      attr_val_padded = tf.pad(attr_val_repeated, paddings, 'CONSTANT', constant_values=pad_const)\n",
    "      inputs[attr] = attr_val_padded\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "  return ds.map(ds_sequentialize_map_helper)\n",
    "\n",
    "def prepare_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: Optional[pd.DataFrame] = None,\n",
    "    df_val: Optional[pd.DataFrame] = None,\n",
    "    out_dir: str = OUTPUT_DATA_DIR,\n",
    "    event_cols: Optional[Union[List[str], str]] = None,\n",
    "    case_cols: Optional[Union[List[str], str]] = None,\n",
    "    label_cols: Optional[Union[List[str], str]] = None,\n",
    "    cat_enc_factory: Optional[Callable[[], sl.base.TransformerMixin]] = lambda: sl.preprocessing.OneHotEncoder(sparse_output=False),\n",
    "    #cat_enc_factory: Optional[Callable[[], sl.base.TransformerMixin]] = lambda: ce.OneHotEncoder(return_df=False),\n",
    "    fit_complete: bool = True,\n",
    "    time_col: str = EVENTLOG_TIMESTAMP,\n",
    "    case_col: str = EVENTLOG_CASE,\n",
    "    label_prefix: str = EVENTLOG_LABEL_PREFIX,\n",
    "):\n",
    "  Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "  if event_cols is None:\n",
    "    event_cols = df_find_event_attributes(df.drop(columns=time_col), exclude_labels=True, case_col=case_col, label_prefix=label_prefix)\n",
    "  elif isinstance(event_cols, str):\n",
    "    event_cols = [event_cols]\n",
    "  if case_cols is None:\n",
    "    case_cols = df_find_case_attributes(df, exclude_labels=True, case_col=case_col, label_prefix=label_prefix)\n",
    "  elif isinstance(case_cols, str):\n",
    "    case_cols = [case_cols]\n",
    "  if label_cols is None:\n",
    "    label_cols = df_find_labels(df, label_prefix=label_prefix)\n",
    "  elif isinstance(label_cols, str):\n",
    "    label_cols = [label_cols]\n",
    "\n",
    "  train_stat = []\n",
    "  test_stat = []\n",
    "  val_stat = []\n",
    "  for case_col in case_cols:\n",
    "    if case_col in df.select_dtypes(exclude=['number', 'timedelta', 'datetime', 'datetimetz']).columns and cat_enc_factory is not None:\n",
    "      enc = cat_enc_factory()\n",
    "      if fit_complete:\n",
    "        enc.fit(np.reshape(df[case_col].to_numpy(), (-1,1)))\n",
    "      else:\n",
    "        enc.fit(np.reshape(df_train[case_col].to_numpy(), (-1,1)))\n",
    "\n",
    "      train_stat.append(enc.transform(np.reshape(df_train[case_col].to_numpy(), (-1,1))))\n",
    "      if df_test is not None:\n",
    "        test_stat.append(enc.transform(np.reshape(df_test[case_col].to_numpy(), (-1,1))))\n",
    "      if df_val is not None:\n",
    "        val_stat.append(enc.transform(np.reshape(df_val[case_col].to_numpy(), (-1,1))))\n",
    "    else:\n",
    "      train_stat.append(np.expand_dims(df_train[case_col].to_numpy(), axis=-1))\n",
    "      if df_test is not None:\n",
    "        test_stat.append(np.expand_dims(df_test[case_col].to_numpy(), axis=-1))\n",
    "      if df_val is not None:\n",
    "        val_stat.append(np.expand_dims(df_val[case_col].to_numpy(), axis=-1))\n",
    "\n",
    "  np.save(os.path.join(out_dir, \"static_train.npy\"), np.hstack(train_stat))\n",
    "  if df_test is not None:\n",
    "    np.save(os.path.join(out_dir, \"static_test.npy\"), np.hstack(test_stat))\n",
    "  if df_val is not None:\n",
    "    np.save(os.path.join(out_dir, \"static_val.npy\"), np.hstack(val_stat))\n",
    "\n",
    "  df_train_dyn = df_prefix_pad_attributes(df_train, event_cols)\n",
    "  if df_test is not None:\n",
    "    df_test_dyn = df_prefix_pad_attributes(df_test, event_cols)\n",
    "  if df_val is not None:\n",
    "    df_val_dyn = df_prefix_pad_attributes(df_val, event_cols)\n",
    "\n",
    "  for event_col in event_cols:\n",
    "    if event_col in df.select_dtypes(exclude=['number', 'timedelta', 'datetime', 'datetimetz']).columns and cat_enc_factory is not None:\n",
    "      enc = cat_enc_factory()\n",
    "      if fit_complete:\n",
    "        enc.fit(np.reshape(df[event_col].to_numpy(), (-1,1)))\n",
    "      else:\n",
    "        enc.fit(np.reshape(df_train[event_col].to_numpy(), (-1,1)))\n",
    "\n",
    "      train_dyn = np_filter_na(df_train_dyn[event_col].to_numpy(), unsqueeze=True)\n",
    "      train_dyn = np.array([enc.transform(event) for event in train_dyn], dtype='object')\n",
    "      np.save(os.path.join(out_dir, f\"timeseries_{get_valid_filename(event_col)}_train.npy\"), train_dyn)\n",
    "      if df_test is not None:\n",
    "        test_dyn = np_filter_na(df_test_dyn[event_col].to_numpy(), unsqueeze=True)\n",
    "        test_dyn = np.array([enc.transform(event) for event in test_dyn], dtype='object')\n",
    "        np.save(os.path.join(out_dir, f\"timeseries_{get_valid_filename(event_col)}_test.npy\"), test_dyn)\n",
    "      if df_val is not None:\n",
    "        val_dyn = np_filter_na(df_val_dyn[event_col].to_numpy(), unsqueeze=True)\n",
    "        val_dyn = np.array([enc.transform(event) for event in val_dyn], dtype='object')\n",
    "        np.save(os.path.join(out_dir, f\"timeseries_{get_valid_filename(event_col)}_val.npy\"), val_dyn)\n",
    "    else:\n",
    "      train_dyn = np_filter_na(df_train_dyn[event_col].to_numpy())\n",
    "      np.save(os.path.join(out_dir, f\"timeseries_{get_valid_filename(event_col)}_train.npy\"), train_dyn)\n",
    "      if df_test is not None:\n",
    "        test_dyn = np_filter_na(df_test_dyn[event_col].to_numpy())\n",
    "        np.save(os.path.join(out_dir, f\"timeseries_{get_valid_filename(event_col)}_test.npy\"), test_dyn)\n",
    "      if df_val is not None:\n",
    "        val_dyn = np_filter_na(df_val_dyn[event_col].to_numpy())\n",
    "        np.save(os.path.join(out_dir, f\"timeseries_{get_valid_filename(event_col)}_val.npy\"), val_dyn)\n",
    "\n",
    "  for label_col in label_cols:\n",
    "    np.save(os.path.join(out_dir, f\"y_train_{get_valid_filename(label_col)}.npy\"), df_train[label_col].to_numpy())\n",
    "    if df_test is not None:\n",
    "      np.save(os.path.join(out_dir, f\"y_test_{get_valid_filename(label_col)}.npy\"), df_test[label_col].to_numpy())\n",
    "    if df_val is not None:\n",
    "      np.save(os.path.join(out_dir, f\"y_val_{get_valid_filename(label_col)}.npy\"), df_val[label_col].to_numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0CFbQmCNPSz"
   },
   "source": [
    "### File Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2NSCu5SNSim"
   },
   "outputs": [],
   "source": [
    "def get_valid_filename(name):\n",
    "  \"\"\"\n",
    "  Return the given string converted to a string that can be used for a clean filename. Remove leading and trailing spaces; convert other spaces to underscores; and remove anything that is not an alphanumeric, dash, underscore, or dot.\n",
    "  \"\"\"\n",
    "  s = str(name).strip().replace(\" \", \"_\")\n",
    "  s = re.sub(r\"(?u)[^-\\w.]\", \"\", s)\n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUzHi7NyMEGY"
   },
   "source": [
    "### Graph Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIwdt5NktSAj"
   },
   "outputs": [],
   "source": [
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "  # Implementation of TGN graph attention oriented on https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py\n",
    "    def __init__(self, in_channels: int, out_channels: int, msg_dim: int, time_enc: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        self.conv = pyg.nn.TransformerConv(in_channels, out_channels // 2, heads=2, dropout=0.1, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        return self.conv(x, edge_index, edge_attr)\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    # Implementation of TGN output module oriented on https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.lin_src = pyg.nn.Linear(in_channels, in_channels)\n",
    "        self.lin_dst = pyg.nn.Linear(in_channels, in_channels)\n",
    "        self.lin_final = pyg.nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = self.lin_src(z_src) + self.lin_dst(z_dst)\n",
    "        h = h.relu()\n",
    "        return self.lin_final(h)\n",
    "\n",
    "memory_dim = time_dim = embedding_dim = EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wA5IJUN21DXj"
   },
   "outputs": [],
   "source": [
    "def df_to_pyg_temporal_data(\n",
    "    df_train: pd.DataFrame,\n",
    "    node_id_col: str,\n",
    "    dest_node_id_col: Optional[str] = None,\n",
    "    target_col: Optional[str] = None,\n",
    "    feat_cols: List[str] = [],\n",
    "    case_col: str = EVENTLOG_CASE,\n",
    "    time_col: str = EVENTLOG_TIMESTAMP,\n",
    ") -> Tuple[dict, dict, pyg.data.TemporalData]:\n",
    "  df_train = df_train.copy()\n",
    "\n",
    "  id_mapping = dict(enumerate(df_train[node_id_col].cat.categories))\n",
    "\n",
    "  feat_mapping = {}\n",
    "  for feat_col in feat_cols:\n",
    "    feat_mapping[feat_col] = df_train[feat_col].max()\n",
    "\n",
    "  def df_to_pyg_temporal_data_helper(df: Optional[pd.DataFrame]) -> pyg.data.TemporalData:\n",
    "    if df is None:\n",
    "      return None\n",
    "\n",
    "    df[df[node_id_col] == TOKEN_NA][node_id_col] = pd.NA\n",
    "\n",
    "    if dest_node_id_col is None:\n",
    "      df[\"_src_node_id\"] = df.groupby(EVENTLOG_CASE)[node_id_col].shift(1)\n",
    "      df[\"_dst_node_id\"] = df[node_id_col]\n",
    "    else:\n",
    "      df[\"_src_node_id\"] = df[node_id_col]\n",
    "      df[\"_dst_node_id\"] = df[dest_node_id_col]\n",
    "\n",
    "    df[\"_timestamp\"] = df[EVENTLOG_TIMESTAMP].view(int)\n",
    "\n",
    "    df = df.dropna(subset=[\"_src_node_id\", \"_dst_node_id\"], axis='index', how='any')\n",
    "    df = df.drop_duplicates(subset=[\"_src_node_id\", \"_dst_node_id\", \"_timestamp\"], keep='last')\n",
    "\n",
    "    df[\"_dst_node_id\"] = df[\"_dst_node_id\"].cat.codes\n",
    "    df[\"_src_node_id\"] = df[\"_src_node_id\"].cat.codes\n",
    "\n",
    "    df[\"_target\"] = 0 if target_col is None else df[target_col]\n",
    "\n",
    "    df[\"_feat_weight\"] = df.sort_values([\"_src_node_id\", \"_dst_node_id\", EVENTLOG_TIMESTAMP]).groupby(by=[\"_src_node_id\", \"_dst_node_id\"])[\"_src_node_id\"].cumcount() + 1\n",
    "    df[\"_feat_weight\"] = df[\"_feat_weight\"] / df[\"_feat_weight\"].max()\n",
    "\n",
    "    for feat_col in feat_cols:\n",
    "      df[f\"_feat_{feat_col}\"] = df[feat_col].cat.codes\n",
    "      df[f\"_feat_{feat_col}\"] = df[f\"_feat_{feat_col}\"] / feat_mapping[feat_col]\n",
    "\n",
    "    src = torch.from_numpy(df[\"_src_node_id\"].values).to(torch.long)\n",
    "    dst = torch.from_numpy(df[\"_dst_node_id\"].values).to(torch.long)\n",
    "\n",
    "    t = torch.from_numpy(df[\"_timestamp\"].values).to(torch.long)\n",
    "    y = torch.from_numpy(df[\"_target\"].values).to(torch.long)\n",
    "    msg = torch.from_numpy(df[[col for col in df if col.startswith(\"_feat_\")]].values).to(torch.float)\n",
    "\n",
    "    return pyg.data.TemporalData(src=src, dst=dst, t=t, msg=msg, y=y)\n",
    "\n",
    "  inverse_id_mapping = {v: k for k, v in id_mapping.items()}\n",
    "\n",
    "  return id_mapping, inverse_id_mapping, df_to_pyg_temporal_data_helper(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKDJMNPRgasL"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Implementation of TGN training loop https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py\n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        n_id, edge_index, e_id = neighbor_loader(batch.n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, data.t[e_id].to(device), data.msg[e_id].to(device))\n",
    "        pos_out = link_pred(z[assoc[batch.src]], z[assoc[batch.dst]])\n",
    "        neg_out = link_pred(z[assoc[batch.src]], z[assoc[batch.neg_dst]])\n",
    "\n",
    "        loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "        loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "\n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(batch.src, batch.dst, batch.t, batch.msg)\n",
    "        neighbor_loader.insert(batch.src, batch.dst)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "\n",
    "    return total_loss / train_data.num_events\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    # Implementation of TGN test loop oriented on https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py\n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "    aps, aucs = [], []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        n_id, edge_index, e_id = neighbor_loader(batch.n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, data.t[e_id].to(device), data.msg[e_id].to(device))\n",
    "        pos_out = link_pred(z[assoc[batch.src]], z[assoc[batch.dst]])\n",
    "        neg_out = link_pred(z[assoc[batch.src]], z[assoc[batch.neg_dst]])\n",
    "\n",
    "        y_pred = torch.cat([pos_out, neg_out], dim=0).sigmoid().cpu()\n",
    "        y_true = torch.cat([torch.ones(pos_out.size(0)), torch.zeros(neg_out.size(0))], dim=0)\n",
    "\n",
    "        aps.append(sl.metrics.average_precision_score(y_true, y_pred))\n",
    "        aucs.append(sl.metrics.roc_auc_score(y_true, y_pred))\n",
    "\n",
    "        memory.update_state(batch.src, batch.dst, batch.t, batch.msg)\n",
    "        neighbor_loader.insert(batch.src, batch.dst)\n",
    "    return float(torch.tensor(aps).mean()), float(torch.tensor(aucs).mean())\n",
    "\n",
    "def embed():\n",
    "  memory.eval()\n",
    "  gnn.eval()\n",
    "  link_pred.eval()\n",
    "\n",
    "  assoc = torch.empty(data.num_nodes, dtype=torch.long)\n",
    "\n",
    "  embeddings = []\n",
    "  for i in range(data.num_nodes):\n",
    "    n_id, edge_index, e_id = neighbor_loader(torch.tensor([i]))\n",
    "    assoc[n_id] = torch.arange(n_id.size(0))\n",
    "\n",
    "    z, last_update = memory(n_id)\n",
    "    z = gnn(z, last_update, edge_index, data.t[e_id], data.msg[e_id])\n",
    "    embeddings.append(z[assoc[torch.tensor([i])]].numpy(force=True))\n",
    "\n",
    "  return np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCVBtXCUglLW"
   },
   "outputs": [],
   "source": [
    "def pyg_visualize_temporal_graph(temporal_data: pyg.data.TemporalData) -> nx.DiGraph:\n",
    "  data = pyg.data.Data(edge_index=temporal_data.edge_index)\n",
    "  g = pyg.utils.to_networkx(data)\n",
    "\n",
    "  nx.draw(g, with_labels=True)\n",
    "  plt.show()\n",
    "\n",
    "  return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3AIDPxZlS-v"
   },
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAPgp73mjlFP"
   },
   "source": [
    "### A: Import from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4F4xUcGjkc0"
   },
   "outputs": [],
   "source": [
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "!cp -r \"$GDRIVE_INPUT_DIR\" \"$INPUT_DATA_DIR\"\n",
    "\n",
    "drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzhkKQsVj11R"
   },
   "source": [
    "### B: Upload from Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHaQpf2gjxzL"
   },
   "outputs": [],
   "source": [
    "#uploaded = files.upload()\n",
    "\n",
    "#for filename in uploaded.keys():\n",
    "#  target = os.path.join(INPUT_DATA_DIR, filename)\n",
    "#  !mv \"$filename\" \"$target\"\n",
    "\n",
    "# del uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17wSoMR3wuaH"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFJHEiyh9nqo"
   },
   "source": [
    "## Dataset: Incident management process enriched event log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pqeJp32LH6d"
   },
   "source": [
    "This event log is of an incident management process extracted from an instance of the ServiceNow platform used by an IT company. See also https://doi.org/10.24432/C57S4H and http://processmining.each.webhostusp.sti.usp.br/index.php/event-logs/.\n",
    "\n",
    "- **Control Attributes**:\n",
    "    - *number*: incident identifier with the same number as total cases;\n",
    "    \n",
    "    - *incident state*: attribute with eight levels controlling incident management process transitions from opening until closing the case;\n",
    "    \n",
    "    - *active*: boolean attribute indicating if record is active or closed/canceled;\n",
    "    \n",
    "    - *reassignment_count*: number of times incident has changed group or support analysts;\n",
    "    \n",
    "    - *reopen_count*: number of times incident resolution was rejected by caller;\n",
    "    \n",
    "    - *sys_mod_count*: number of incident updates until that moment;\n",
    "    \n",
    "    - *made_sla*: boolean attribute to incident exceeded target SLA or not;\n",
    "\n",
    "- **Identification and Classification Attributes**:\n",
    "    - *caller_id*: user identifier affected;\n",
    "    \n",
    "    - *opened_by*: user identifier that reported the incident;\n",
    "    \n",
    "    - *opened_at*: incident opening date and time;\n",
    "    \n",
    "    - *sys_created_by*: user identifier that registered the incident;\n",
    "    \n",
    "    - *sys_created_at*: incident creation date and time;\n",
    "    \n",
    "    - *sys_updated_by*: user identifier that made update and generated current log record;\n",
    "    \n",
    "    - *sys_updated_at*: log update date and time;\n",
    "    \n",
    "    - *contact_type*: categorical field with values indicating how incident was reported;\n",
    "    \n",
    "    - *location*: location identifier of place being affected;\n",
    "    \n",
    "    - *category*: description of the first level of service being affected;\n",
    "    \n",
    "    - *subcategory*: description of the second level of service being affected related to first level;\n",
    "    \n",
    "    - *u_symptom*: description about user perception of service availability;\n",
    "    \n",
    "    - *cmdb_ci*: (confirmation item) identifier (not mandatory) referencing homonyms relation and used to report item being affected;\n",
    "    \n",
    "    - *impact*: description of the impact caused by incident. Values are: 1-High; 2-Medium; 3-Low;\n",
    "    \n",
    "    - *urgency*: description to the urgency asked by user for incident resolution. Values are same as impact;\n",
    "    \n",
    "    - *priority*: priority calculated by system based on Impact and urgency;\n",
    "\n",
    "- **Support, Diagnosis and Other Attributes**:\n",
    "    - *assignment_group*: identifier referencing the relation Group (database relational model in ServiceNowTM) describing support group in charge of incident;\n",
    "    \n",
    "    - *assigned_to*: user identifier in charge of incident;\n",
    "    \n",
    "    - *knowledge*: boolean attribute indicating whether a knowledge base document was used to resolve incident;\n",
    "    \n",
    "    - *u_priority_confirmation*: boolean attribute indicating whether priority field was double checked;\n",
    "    \n",
    "    - *notify*: categorical attribute indicating whether notifications was generated for this incident;\n",
    "    \n",
    "    - *problem_id*: identifier referencing homonyms relation describing problem identifier associated with this incident;\n",
    "    \n",
    "    - *rfc*: (change request) identifier referencing homonyms relation describing change request identifier associated with incident;\n",
    "    \n",
    "    - *vendor*: identifier referencing homonyms relation describing vendor in charge of incident;\n",
    "    \n",
    "    - *caused_by*: relation with RFC code responsible by the incident;\n",
    "    \n",
    "    - *close_code*: resolution code of the incident;\n",
    "    \n",
    "    - *resolved_by*: user identifier who resolved the incident;\n",
    "    \n",
    "    - *resolved_at*: incident resolution date and time;\n",
    "    \n",
    "    - *closed_at*: incident close date and time;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcr6aHNqzZhO"
   },
   "source": [
    "### Read Incident management process enriched event log V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR798kUuwzYf"
   },
   "outputs": [],
   "source": [
    "dtypes_servicenow = {\n",
    "    \"number\": \"string\",\n",
    "    \"incident_state\": \"category\",\n",
    "    \"active\": \"boolean\",\n",
    "    \"reassignment_count\": \"Int16\",\n",
    "    \"reopen_count\": \"Int16\",\n",
    "    \"sys_mod_count\": \"Int16\",\n",
    "    \"made_sla\": \"boolean\",\n",
    "    \"caller_id\": \"category\",\n",
    "    \"opened_by\": \"category\",\n",
    "    \"sys_created_by\": \"category\",\n",
    "    \"sys_updated_by\": \"category\",\n",
    "    \"contact_type\": \"category\",\n",
    "    \"location\": \"category\",\n",
    "    \"category\": \"category\",\n",
    "    \"subcategory\": \"category\",\n",
    "    \"u_symptom\": \"category\",\n",
    "    \"cmdb_ci\": \"category\",\n",
    "    \"impact\": pd.CategoricalDtype(['1 - High', '2 - Medium', '3 - Low'], ordered=True),\n",
    "    \"urgency\": pd.CategoricalDtype(['1 - High', '2 - Medium', '3 - Low'], ordered=True),\n",
    "    \"priority\": pd.CategoricalDtype(['1 - Critical', '2 - High', '3 - Moderate', '4 - Low'], ordered=True),\n",
    "    \"assignment_group\": \"category\",\n",
    "    \"assigned_to\": \"category\",\n",
    "    \"knowledge\": \"boolean\",\n",
    "    \"u_priority_confirmation\": \"boolean\",\n",
    "    \"problem_id\": \"string\",\n",
    "    \"rfc\": \"string\",\n",
    "    \"vendor\": \"category\",\n",
    "    \"caused_by\": \"category\",\n",
    "    \"closed_code\": \"category\",\n",
    "    \"resolved_by\": \"category\"\n",
    "}\n",
    "\n",
    "df_servicenow = pd.read_csv(\n",
    "    os.path.join(INPUT_DATA_DIR, \"incident_event_log.csv\"),\n",
    "    header=0,\n",
    "    na_values=[\"?\"],\n",
    "    dtype=dtypes_servicenow\n",
    ")\n",
    "\n",
    "# Convert timestamps\n",
    "df_servicenow = df_convert_datetimes(\n",
    "    df_servicenow,\n",
    "    [\"opened_at\", \"sys_created_at\", \"sys_updated_at\", \"resolved_at\", \"closed_at\"],\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "# Convert booleans\n",
    "df_servicenow = df_convert_bools(\n",
    "    df_servicenow,\n",
    "    [\"notify\"],\n",
    "    \"Send Email\",\n",
    "    \"Do Not Notify\"\n",
    ")\n",
    "\n",
    "df_drop_duplicate_rows(df_servicenow)\n",
    "df_drop_na_rows_and_cols(df_servicenow)\n",
    "df_drop_single_val_cols(df_servicenow)\n",
    "\n",
    "# Sort and normalize column names\n",
    "df_format_as_eventlog(df_servicenow, case_col=\"number\", activity_col=\"incident_state\", time_col=\"sys_updated_at\", resource_col=\"assigned_to\", group_col=\"assignment_group\")\n",
    "\n",
    "# Write as Pandas CSV\n",
    "df_write_files(df_servicenow, os.path.join(INTERIM_DATA_DIR, \"incident_event_log_processed\"), skip_xes=False)\n",
    "\n",
    "print(df_servicenow.dtypes)\n",
    "df_servicenow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lk_PYXA1Fn1r"
   },
   "source": [
    "### Label Incident management process enriched event log V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2kOh8lQGMb-"
   },
   "outputs": [],
   "source": [
    "df_servicenow = df_label_next_activity(df_servicenow, eoc_token=TOKEN_EOC)\n",
    "df_servicenow = df_label_activity_duration(df_servicenow, unit='d', eoc_token=pd.Timedelta(0))\n",
    "df_servicenow = df_label_remaining_cycle_time(df_servicenow, unit='d')\n",
    "\n",
    "df_write_files(df_servicenow, os.path.join(INTERIM_DATA_DIR, \"incident_event_log_labeled\"))\n",
    "df_servicenow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VZ_4qca1SN_"
   },
   "source": [
    "### Descriptive Statistics for Incident management process enriched event log V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHzRc66QgfCu"
   },
   "outputs": [],
   "source": [
    "df_stat = df_servicenow.describe(include='all')\n",
    "df_write_files(df_stat, os.path.join(OUTPUT_DATA_DIR, \"incident_event_log_describe\"), index=True)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "238c8Ldzyx9H"
   },
   "outputs": [],
   "source": [
    "df_servicenow.hist(xrot=90, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"incident_event_log_hist.svg\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjokUaLR053J"
   },
   "outputs": [],
   "source": [
    "df_case_length_stats(df_servicenow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Zd7zp-m1liV"
   },
   "outputs": [],
   "source": [
    "df_case_duration_stats(df_servicenow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH7VNRxnvPNa"
   },
   "source": [
    "### Clean Incident management process enriched event log V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpg803mhXoqL"
   },
   "outputs": [],
   "source": [
    "df_servicenow_clean = df_servicenow.drop(columns=[\n",
    "    \"active\", # A-posteriori\n",
    "    \"made_sla\", # A-posteriori\n",
    "    \"case:opened_at\", # In sequence encoded\n",
    "    \"case:sys_created_at\", # In sequence encoded\n",
    "    \"impact\", # Correlates with priority and urgency\n",
    "    \"urgency\", # Correlates with priority and urgency\n",
    "    \"case:closed_code\", # A-posteriori\n",
    "    \"case:resolved_by\", # A-posteriori\n",
    "    \"case:resolved_at\", # A-posteriori\n",
    "    \"case:closed_at\", # A-posteriori\n",
    "])\n",
    "\n",
    "# Remove mostly empty columns\n",
    "df_drop_threshold_na_cols(df_servicenow_clean, 0.95)\n",
    "\n",
    "# Filter event log\n",
    "df_servicenow_clean = df_filter_case_duration_range(df_servicenow_clean, max=0.95)\n",
    "df_servicenow_clean = df_filter_date_range(df_servicenow_clean, max=\"2016-07-01 00:00:00\", mode='traces_included')\n",
    "\n",
    "# Create time features\n",
    "df_servicenow_clean = df_extract_elapsed_cycle_time(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_CYCLE_SUFFIX}:seconds\", unit='s')\n",
    "df_servicenow_clean = df_extract_activity_duration(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX}:seconds\", unit='s', na_token=0)\n",
    "\n",
    "df_servicenow_clean = df_extract_month_of_year(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:month\", relative=True)\n",
    "df_servicenow_clean = df_extract_day_of_year(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear\", relative=True)\n",
    "df_servicenow_clean = df_extract_day_of_month(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:day\", relative=True)\n",
    "df_servicenow_clean = df_extract_day_of_week(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:weekday\", relative=True)\n",
    "df_servicenow_clean = df_extract_hour_of_day(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:hour\", relative=True)\n",
    "\n",
    "df_servicenow_clean = df_extract_month_of_year(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:month:raw\")\n",
    "df_servicenow_clean = df_extract_day_of_year(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear:raw\")\n",
    "df_servicenow_clean = df_extract_day_of_month(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:day:raw\")\n",
    "df_servicenow_clean = df_extract_day_of_week(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:weekday:raw\")\n",
    "df_servicenow_clean = df_extract_hour_of_day(df_servicenow_clean, f\"{EVENTLOG_TIMESTAMP}:hour:raw\")\n",
    "\n",
    "# Encode categorical labels\n",
    "df_servicenow_clean = df_encode_label(df_servicenow_clean, cols=EVENTLOG_LABEL_NEXT_ACT)\n",
    "\n",
    "# Transform bools to int\n",
    "df_servicenow_clean = df_convert_bool_to_int(df_servicenow_clean)\n",
    "\n",
    "# Transform ordered cats to int\n",
    "df_servicenow_clean = df_convert_ordered_cat_to_int(df_servicenow_clean, relative=True)\n",
    "\n",
    "# Fill empty str values\n",
    "df_servicenow_clean = df_fillna_str(df_servicenow_clean, TOKEN_NA)\n",
    "\n",
    "# Fill empty cat values\n",
    "df_servicenow_clean = df_fillna_cat(df_servicenow_clean, TOKEN_NA)\n",
    "\n",
    "df_write_files(df_servicenow_clean, os.path.join(OUTPUT_DATA_DIR, \"incident_event_log_cleaned\"))\n",
    "\n",
    "df_servicenow_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRQ6Ph5tTMu3"
   },
   "outputs": [],
   "source": [
    "df_servicenow_train, df_servicenow_test = df_strict_temporal_train_test_split(\n",
    "    df_servicenow_clean,\n",
    "    0.2,\n",
    "    df_find_labels(df_servicenow_clean),\n",
    "    debias_end=False\n",
    ")\n",
    "\n",
    "axes = df_visualize_strict_temporal_splitting(df_servicenow_train, df_servicenow_test)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"incident_event_log_train_test.svg\"), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Remove overlaps\n",
    "df_servicenow_test.dropna(axis='index', how='any', subset=df_find_labels(df_servicenow_test), inplace=True)\n",
    "\n",
    "df_write_files(df_servicenow_train, os.path.join(OUTPUT_DATA_DIR, \"incident_event_log_train\"))\n",
    "df_write_files(df_servicenow_test, os.path.join(OUTPUT_DATA_DIR, \"incident_event_log_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObmdlPzBL4p5"
   },
   "outputs": [],
   "source": [
    "df_servicenow_dyn = df_prefix_pad_attributes(\n",
    "    pd.concat([df_servicenow_train, df_servicenow_test]),\n",
    "    df_find_event_attributes(df_servicenow_clean, exclude_labels=True)\n",
    ")\n",
    "\n",
    "df_servicenow_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iysTNzvxMNbP"
   },
   "outputs": [],
   "source": [
    "df_servicenow_train_dyn = df_servicenow_dyn.loc[df_servicenow_train[EVENTLOG_CASE].unique()]\n",
    "df_servicenow_train_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WEHwZ95MPXc"
   },
   "outputs": [],
   "source": [
    "df_servicenow_test_dyn = df_servicenow_dyn.loc[df_servicenow_test[EVENTLOG_CASE].unique()]\n",
    "df_servicenow_test_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ID-LaLrUcECR"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_servicenow_train,\n",
    "    df_servicenow_test,\n",
    "    EVENTLOG_LABEL_NEXT_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNyMmpEZfqFm"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_servicenow_train,\n",
    "    df_servicenow_test,\n",
    "    EVENTLOG_LABEL_REM_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5MwsjjLi8HU"
   },
   "outputs": [],
   "source": [
    "df_naive_classification_metrics(\n",
    "    df_servicenow_train,\n",
    "    df_servicenow_test,\n",
    "    EVENTLOG_LABEL_NEXT_ACT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evqGR9zegShe"
   },
   "source": [
    "### Create Graphs for Incident management process enriched event log V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l9tNvHvRX-s"
   },
   "source": [
    "#### Create Resource Graphs for Incident management process enriched event log V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjDL75N_CL8E"
   },
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, data = df_to_pyg_temporal_data(pd.concat([df_servicenow_train, df_servicenow_test], ignore_index=True), EVENTLOG_RESOURCE)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNjAq7mKnrYt"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "train_loader = pyg.loader.TemporalDataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "val_loader = pyg.loader.TemporalDataLoader(\n",
    "    val_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "test_loader = pyg.loader.TemporalDataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "\n",
    "neighbor_loader = pyg.nn.models.tgn.LastNeighborLoader(data.num_nodes, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1jlWd0_RTwl"
   },
   "outputs": [],
   "source": [
    "memory = pyg.nn.models.TGNMemory(\n",
    "    data.num_nodes,\n",
    "    data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=pyg.nn.models.tgn.IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=pyg.nn.models.tgn.LastAggregator(),\n",
    ")\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(set(memory.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=0.00001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn5Cmcugxn__"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    val_ap, val_auc = test(val_loader)\n",
    "    test_ap, test_auc = test(test_loader)\n",
    "    print(f'Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"servicenow_resource_mapping.json\"), \"w\") as f:\n",
    "  json.dump(mapping, f)\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"servicenow_resource_inverse_mapping.json\"), \"w\") as f:\n",
    "  json.dump(reverse_mapping, f)\n",
    "\n",
    "embeddings = embed()\n",
    "np.save(os.path.join(OUTPUT_DATA_DIR, \"servicenow_resource_embeddings.npy\"), embeddings, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bO7zyqZ99kgD"
   },
   "outputs": [],
   "source": [
    "arr_nan = np.full(EMBEDDING_DIM, -99.0, dtype='float32')\n",
    "\n",
    "embeddings_df_train = df_servicenow_train_dyn[EVENTLOG_RESOURCE].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "resource_graph_embeddings_train = np.empty((embeddings_df_train.shape[0], embeddings_df_train.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_train)):\n",
    "  resource_graph_embeddings_train[i] = np.stack(embeddings_df_train[i])\n",
    "\n",
    "resource_graph_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_eoT5JPMlFD"
   },
   "outputs": [],
   "source": [
    "embeddings_df_test = df_servicenow_test_dyn[EVENTLOG_RESOURCE].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "resource_graph_embeddings_test = np.empty((embeddings_df_test.shape[0], embeddings_df_test.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_test)):\n",
    "  resource_graph_embeddings_test[i] = np.stack(embeddings_df_test[i])\n",
    "\n",
    "resource_graph_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0RX7atORf7Q"
   },
   "source": [
    "#### Create Group Graphs for Incident management process enriched event log V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eA_67htRf7S"
   },
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, data = df_to_pyg_temporal_data(pd.concat([df_servicenow_train, df_servicenow_test], ignore_index=True), EVENTLOG_GROUP)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLIQFys2Rf7T"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "train_loader = pyg.loader.TemporalDataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "val_loader = pyg.loader.TemporalDataLoader(\n",
    "    val_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "test_loader = pyg.loader.TemporalDataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "\n",
    "neighbor_loader = pyg.nn.models.tgn.LastNeighborLoader(data.num_nodes, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9rPLHG_Rf7U"
   },
   "outputs": [],
   "source": [
    "memory = pyg.nn.models.TGNMemory(\n",
    "    data.num_nodes,\n",
    "    data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=pyg.nn.models.tgn.IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=pyg.nn.models.tgn.LastAggregator(),\n",
    ")\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(set(memory.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=0.00001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a70MjloTRf7V"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    val_ap, val_auc = test(val_loader)\n",
    "    test_ap, test_auc = test(test_loader)\n",
    "    print(f'Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"servicenow_group_mapping.json\"), \"w\") as f:\n",
    "  json.dump(mapping, f)\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"servicenow_group_inverse_mapping.json\"), \"w\") as f:\n",
    "  json.dump(reverse_mapping, f)\n",
    "\n",
    "embeddings = embed()\n",
    "np.save(os.path.join(OUTPUT_DATA_DIR, \"servicenow_group_embeddings.npy\"), embeddings, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZW4goZhNRf7W"
   },
   "outputs": [],
   "source": [
    "arr_nan = np.full(EMBEDDING_DIM, -99.0, dtype='float32')\n",
    "\n",
    "embeddings_df_train = df_servicenow_train_dyn[EVENTLOG_GROUP].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "group_graph_embeddings_train = np.empty((embeddings_df_train.shape[0], embeddings_df_train.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_train)):\n",
    "  group_graph_embeddings_train[i] = np.stack(embeddings_df_train[i])\n",
    "\n",
    "group_graph_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8he5npzW896W"
   },
   "outputs": [],
   "source": [
    "embeddings_df_test = df_servicenow_test_dyn[EVENTLOG_GROUP].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "group_graph_embeddings_test = np.empty((embeddings_df_test.shape[0], embeddings_df_test.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_test)):\n",
    "  group_graph_embeddings_test[i] = np.stack(embeddings_df_test[i])\n",
    "\n",
    "group_graph_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hJtMV6bMltL"
   },
   "source": [
    "### Build Dataset for Incident management process enriched event log V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEQUMgq9z9eT"
   },
   "outputs": [],
   "source": [
    "ds_servicenow_train = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_servicenow_train_dyn[EVENTLOG_ACTIVITY].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"reassignment_count\": df_servicenow_train_dyn[\"reassignment_count\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"reopen_count\": df_servicenow_train_dyn[\"reopen_count\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"sys_mod_count\": df_servicenow_train_dyn[\"sys_mod_count\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"caller_id\": df_servicenow_train_dyn[\"caller_id\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"sys_updated_by\": df_servicenow_train_dyn[\"sys_updated_by\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"contact_type\": df_servicenow_train_dyn[\"contact_type\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"location\": df_servicenow_train_dyn[\"location\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"category\": df_servicenow_train_dyn[\"category\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"subcategory\": df_servicenow_train_dyn[\"subcategory\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"u_symptom\": df_servicenow_train_dyn[\"u_symptom\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"priority\": df_servicenow_train_dyn[\"priority\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"org_group\": df_servicenow_train_dyn[\"org:group\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource\": df_servicenow_train_dyn[\"org:resource\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource_graph\": resource_graph_embeddings_train,\n",
    "  \"org_group_graph\": group_graph_embeddings_train,\n",
    "  \"knowledge\": df_servicenow_train_dyn[\"knowledge\"].astype('Int8').to_numpy(dtype='int8', na_value=-1),\n",
    "  \"u_priority_confirmation\": df_servicenow_train_dyn[\"u_priority_confirmation\"].astype('Int8').to_numpy(dtype='int8', na_value=-1),\n",
    "  \"time_timestamp_elapsedcycle\": df_servicenow_train_dyn[\"time:timestamp:elapsedcycle:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_servicenow_train_dyn[\"time:timestamp:elapsedprev:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_servicenow_train_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_servicenow_train_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_servicenow_train_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_servicenow_train_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_servicenow_train_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_servicenow_train_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_servicenow_train_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_servicenow_train_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_servicenow_train_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_servicenow_train_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"case_notify\": np.expand_dims(df_servicenow_train[\"case:notify\"].to_numpy(dtype='int8'), axis=-1),\n",
    "  \"case_opened_by\": np.expand_dims(df_servicenow_train[\"case:opened_by\"].astype('string').to_numpy(na_value=TOKEN_NA), axis=-1),\n",
    "  \"case_sys_created_by\": np.expand_dims(df_servicenow_train[\"case:sys_created_by\"].astype('string').to_numpy(na_value=TOKEN_NA), axis=-1),\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_servicenow_train[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_servicenow_train[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_servicenow_train[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_servicenow_train, os.path.join(OUTPUT_DATA_DIR, 'incident_event_log_train_dataset'))\n",
    "\n",
    "ds_servicenow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YuFqjdj8xtb"
   },
   "outputs": [],
   "source": [
    "ds_servicenow_test = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_servicenow_test_dyn[EVENTLOG_ACTIVITY].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"reassignment_count\": df_servicenow_test_dyn[\"reassignment_count\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"reopen_count\": df_servicenow_test_dyn[\"reopen_count\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"sys_mod_count\": df_servicenow_test_dyn[\"sys_mod_count\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"caller_id\": df_servicenow_test_dyn[\"caller_id\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"sys_updated_by\": df_servicenow_test_dyn[\"sys_updated_by\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"contact_type\": df_servicenow_test_dyn[\"contact_type\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"location\": df_servicenow_test_dyn[\"location\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"category\": df_servicenow_test_dyn[\"category\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"subcategory\": df_servicenow_test_dyn[\"subcategory\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"u_symptom\": df_servicenow_test_dyn[\"u_symptom\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"priority\": df_servicenow_test_dyn[\"priority\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"org_group\": df_servicenow_test_dyn[\"org:group\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource\": df_servicenow_test_dyn[\"org:resource\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource_graph\": resource_graph_embeddings_test,\n",
    "  \"org_group_graph\": group_graph_embeddings_test,\n",
    "  \"knowledge\": df_servicenow_test_dyn[\"knowledge\"].astype('Int8').to_numpy(dtype='int8', na_value=-1),\n",
    "  \"u_priority_confirmation\": df_servicenow_test_dyn[\"u_priority_confirmation\"].astype('Int8').to_numpy(dtype='int8', na_value=-1),\n",
    "  \"time_timestamp_elapsedcycle\": df_servicenow_test_dyn[\"time:timestamp:elapsedcycle:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_servicenow_test_dyn[\"time:timestamp:elapsedprev:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_servicenow_test_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_servicenow_test_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_servicenow_test_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_servicenow_test_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_servicenow_test_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_servicenow_test_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_servicenow_test_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_servicenow_test_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_servicenow_test_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_servicenow_test_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"case_notify\": np.expand_dims(df_servicenow_test[\"case:notify\"].to_numpy(dtype='int8'), axis=-1),\n",
    "  \"case_opened_by\": np.expand_dims(df_servicenow_test[\"case:opened_by\"].astype('string').to_numpy(na_value=TOKEN_NA), axis=-1),\n",
    "  \"case_sys_created_by\": np.expand_dims(df_servicenow_test[\"case:sys_created_by\"].astype('string').to_numpy(na_value=TOKEN_NA), axis=-1),\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_servicenow_test[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_servicenow_test[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_servicenow_test[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_servicenow_test, os.path.join(OUTPUT_DATA_DIR, 'incident_event_log_test_dataset'))\n",
    "\n",
    "ds_servicenow_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzxxUVrg93w6"
   },
   "source": [
    "## Dataset: Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxhz-B2iO2cQ"
   },
   "source": [
    "The event log concerns the ticketing management process of the Help desk of an Italian software company. See also https://doi.org/10.4121/uuid:0c60edf1-6f83-4e75-9367-4c63b3e9d5bb.\n",
    "\n",
    "- *Case ID*: the case identifier\n",
    "\n",
    "- *Activity*: the activity name\n",
    "\n",
    "- *Resource*: the resource who performed the action\n",
    "\n",
    "- Complete Timestamp: the timestamp of the event. Format: YYYY/MM/DD hh:mm:ss.\n",
    "\n",
    "- *Variant*: case variant\n",
    "\n",
    "- Variant index: case variant in integer format\n",
    "\n",
    "- *seriousness*: a seriousness level for the ticket\n",
    "\n",
    "- *customer*: name of the customer\n",
    "\n",
    "- *product*: name of the product\n",
    "\n",
    "- *responsible_section*: name of the responsible section\n",
    "\n",
    "- *seriousness_2*: a sub-seriousness level\n",
    "\n",
    "- *service_level*: level of the service\n",
    "\n",
    "- *service_type*: type of the service\n",
    "\n",
    "- *support_section*: name of the support section\n",
    "\n",
    "- *workgroup*: name of the workgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1CXgfZ3-E3q"
   },
   "source": [
    "### Read Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPWIQpBy-Ao4"
   },
   "outputs": [],
   "source": [
    "dtypes_italy = {\n",
    "    \"Case ID\": \"string\",\n",
    "    \"Activity\": \"category\",\n",
    "    \"Resource\": \"category\",\n",
    "    \"Variant\": \"category\",\n",
    "    \"Variant index\": \"Int32\",\n",
    "    \"seriousness\": \"category\",\n",
    "    \"customer\": \"string\",\n",
    "    \"product\": \"category\",\n",
    "    \"responsible_section\": \"category\",\n",
    "    \"seriousness_2\": pd.CategoricalDtype(['Value 1', 'Value 2', 'Value 3', 'Value 4'], ordered=True),\n",
    "    \"service_level\": pd.CategoricalDtype(['Value 1', 'Value 2', 'Value 3', 'Value 4'], ordered=True),\n",
    "    \"service_type\": \"category\",\n",
    "    \"support_section\": \"category\",\n",
    "    \"workgroup\": \"category\"\n",
    "}\n",
    "\n",
    "df_italy = pd.read_csv(\n",
    "    os.path.join(INPUT_DATA_DIR, \"finale.csv\"),\n",
    "    header=0,\n",
    "    dtype=dtypes_italy\n",
    ")\n",
    "\n",
    "df_italy = df_drop_duplicate_cols(df_italy)\n",
    "df_drop_duplicate_rows(df_italy)\n",
    "df_drop_na_rows_and_cols(df_italy)\n",
    "df_drop_single_val_cols(df_italy)\n",
    "\n",
    "# Convert timestamps\n",
    "df_italy = df_convert_datetimes(\n",
    "    df_italy,\n",
    "    [\"Complete Timestamp\"],\n",
    "    yearfirst=True\n",
    ")\n",
    "# Sort and normalize column names\n",
    "df_format_as_eventlog(df_italy, case_col=\"Case ID\", activity_col=\"Activity\", time_col=\"Complete Timestamp\", resource_col=\"Resource\", group_col=\"workgroup\")\n",
    "\n",
    "# Write as Pandas CSV\n",
    "df_write_files(df_italy, os.path.join(INTERIM_DATA_DIR, \"finale_processed\"), skip_xes=False)\n",
    "\n",
    "print(df_italy.dtypes)\n",
    "df_italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsr6vaElyBa1"
   },
   "source": [
    "### Label Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X0eYnMQyA4K"
   },
   "outputs": [],
   "source": [
    "df_italy = df_label_next_activity(df_italy, eoc_token=TOKEN_EOC)\n",
    "df_italy = df_label_activity_duration(df_italy, unit='d', eoc_token=pd.Timedelta(0))\n",
    "df_italy = df_label_remaining_cycle_time(df_italy, unit='d')\n",
    "\n",
    "df_write_files(df_italy, os.path.join(INTERIM_DATA_DIR, \"finale_labeled\"))\n",
    "df_italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlnAQIcSYksI"
   },
   "source": [
    "### Descriptive Statistics for Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKSx_9AMYpZu"
   },
   "outputs": [],
   "source": [
    "df_stat = df_italy.describe(include='all')\n",
    "df_write_files(df_stat, os.path.join(OUTPUT_DATA_DIR, \"finale_describe\"), index=True)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAYe0vWaY4Hc"
   },
   "outputs": [],
   "source": [
    "df_italy.hist(xrot=90, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"finale_hist.svg\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2lApEKt2qrO"
   },
   "outputs": [],
   "source": [
    "df_case_length_stats(df_italy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REL6LSPO2vdQ"
   },
   "outputs": [],
   "source": [
    "df_case_duration_stats(df_italy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUUePZ8zbTao"
   },
   "source": [
    "### Clean Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "de-jEyUdbSjU"
   },
   "outputs": [],
   "source": [
    "df_italy_clean = df_italy.drop(columns=[\n",
    "    \"case:Variant\", # A-posteriori\n",
    "    \"case:Variant index\", # A-posteriori\n",
    "])\n",
    "\n",
    "# Remove mostly empty columns\n",
    "df_drop_threshold_na_cols(df_italy_clean, 0.95)\n",
    "\n",
    "# Filter event log\n",
    "df_italy_clean = df_filter_case_duration_range(df_italy_clean, max=0.95)\n",
    "\n",
    "# Create time features\n",
    "df_italy_clean = df_extract_elapsed_cycle_time(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_CYCLE_SUFFIX}:seconds\", unit='s')\n",
    "df_italy_clean = df_extract_activity_duration(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX}:seconds\", unit='s', na_token=0)\n",
    "\n",
    "df_italy_clean = df_extract_month_of_year(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:month\", relative=True)\n",
    "df_italy_clean = df_extract_day_of_year(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear\", relative=True)\n",
    "df_italy_clean = df_extract_day_of_month(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:day\", relative=True)\n",
    "df_italy_clean = df_extract_day_of_week(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:weekday\", relative=True)\n",
    "df_italy_clean = df_extract_hour_of_day(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:hour\", relative=True)\n",
    "\n",
    "df_italy_clean = df_extract_month_of_year(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:month:raw\")\n",
    "df_italy_clean = df_extract_day_of_year(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear:raw\")\n",
    "df_italy_clean = df_extract_day_of_month(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:day:raw\")\n",
    "df_italy_clean = df_extract_day_of_week(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:weekday:raw\")\n",
    "df_italy_clean = df_extract_hour_of_day(df_italy_clean, f\"{EVENTLOG_TIMESTAMP}:hour:raw\")\n",
    "\n",
    "# Encode categorical labels\n",
    "df_italy_clean = df_encode_label(df_italy_clean, cols=EVENTLOG_LABEL_NEXT_ACT)\n",
    "\n",
    "# Transform bools to int\n",
    "df_italy_clean = df_convert_bool_to_int(df_italy_clean)\n",
    "\n",
    "# Transform ordered cats to int\n",
    "df_italy_clean = df_convert_ordered_cat_to_int(df_italy_clean, relative=True)\n",
    "\n",
    "# Fill empty str values\n",
    "df_italy_clean = df_fillna_str(df_italy_clean, TOKEN_NA)\n",
    "\n",
    "# Fill empty cat values\n",
    "df_italy_clean = df_fillna_cat(df_italy_clean, TOKEN_NA)\n",
    "\n",
    "df_write_files(df_italy_clean, os.path.join(OUTPUT_DATA_DIR, \"finale_cleaned\"))\n",
    "\n",
    "df_italy_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0EIcCceLuIj"
   },
   "outputs": [],
   "source": [
    "df_italy_train, df_italy_test = df_strict_temporal_train_test_split(\n",
    "    df_italy_clean,\n",
    "    0.2,\n",
    "    df_find_labels(df_italy_clean),\n",
    "    debias_end=False\n",
    ")\n",
    "\n",
    "axes = df_visualize_strict_temporal_splitting(df_italy_train, df_italy_test)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"finale_train_test.svg\"), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Remove overlaps\n",
    "df_italy_test.dropna(axis='index', how='any', subset=df_find_labels(df_italy_test), inplace=True)\n",
    "\n",
    "df_write_files(df_italy_train, os.path.join(OUTPUT_DATA_DIR, \"finale_train\"))\n",
    "df_write_files(df_italy_test, os.path.join(OUTPUT_DATA_DIR, \"finale_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EowRlrgglzE8"
   },
   "outputs": [],
   "source": [
    "df_italy_dyn = df_prefix_pad_attributes(\n",
    "    pd.concat([df_italy_train, df_italy_test]),\n",
    "    df_find_event_attributes(df_italy_clean, exclude_labels=True)\n",
    ")\n",
    "\n",
    "df_italy_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D4Peynhl_1K"
   },
   "outputs": [],
   "source": [
    "df_italy_train_dyn = df_italy_dyn.loc[df_italy_train[EVENTLOG_CASE].unique()]\n",
    "df_italy_train_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1siyfVIvl_Qg"
   },
   "outputs": [],
   "source": [
    "df_italy_test_dyn = df_italy_dyn.loc[df_italy_test[EVENTLOG_CASE].unique()]\n",
    "df_italy_test_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GL1IVrhklnVY"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_italy_train,\n",
    "    df_italy_test,\n",
    "    EVENTLOG_LABEL_NEXT_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtqhZyjzljqS"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_italy_train,\n",
    "    df_italy_test,\n",
    "    EVENTLOG_LABEL_REM_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vqhlj5eqmH3L"
   },
   "outputs": [],
   "source": [
    "df_naive_classification_metrics(\n",
    "    df_italy_train,\n",
    "    df_italy_test,\n",
    "    EVENTLOG_LABEL_NEXT_ACT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYfzMoNPi6zH"
   },
   "source": [
    "### Create Graphs for Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJEuhwerldo6"
   },
   "source": [
    "#### Create Resource Graphs for Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDZeKvj4ldo6"
   },
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, data = df_to_pyg_temporal_data(pd.concat([df_italy_train, df_italy_test], ignore_index=True), EVENTLOG_RESOURCE)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-46rA9vldo7"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "train_loader = pyg.loader.TemporalDataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "val_loader = pyg.loader.TemporalDataLoader(\n",
    "    val_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "test_loader = pyg.loader.TemporalDataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "\n",
    "neighbor_loader = pyg.nn.models.tgn.LastNeighborLoader(data.num_nodes, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xr_O3r2Dldo8"
   },
   "outputs": [],
   "source": [
    "memory = pyg.nn.models.TGNMemory(\n",
    "    data.num_nodes,\n",
    "    data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=pyg.nn.models.tgn.IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=pyg.nn.models.tgn.LastAggregator(),\n",
    ")\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(set(memory.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=0.00001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivk6qVb9ldo8"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    val_ap, val_auc = test(val_loader)\n",
    "    test_ap, test_auc = test(test_loader)\n",
    "    print(f'Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"italy_resource_mapping.json\"), \"w\") as f:\n",
    "  json.dump(mapping, f)\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"italy_resource_inverse_mapping.json\"), \"w\") as f:\n",
    "  json.dump(reverse_mapping, f)\n",
    "\n",
    "embeddings = embed()\n",
    "np.save(os.path.join(OUTPUT_DATA_DIR, \"italy_resource_embeddings.npy\"), embeddings, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38fIKws-ldo9"
   },
   "outputs": [],
   "source": [
    "arr_nan = np.full(EMBEDDING_DIM, -99.0, dtype='float32')\n",
    "\n",
    "embeddings_df_train = df_italy_train_dyn[EVENTLOG_RESOURCE].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "resource_graph_embeddings_train = np.empty((embeddings_df_train.shape[0], embeddings_df_train.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_train)):\n",
    "  resource_graph_embeddings_train[i] = np.stack(embeddings_df_train[i])\n",
    "\n",
    "resource_graph_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlHDwhwkldo-"
   },
   "outputs": [],
   "source": [
    "embeddings_df_test = df_italy_test_dyn[EVENTLOG_RESOURCE].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "resource_graph_embeddings_test = np.empty((embeddings_df_test.shape[0], embeddings_df_test.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_test)):\n",
    "  resource_graph_embeddings_test[i] = np.stack(embeddings_df_test[i])\n",
    "\n",
    "resource_graph_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50fP9VDdldo_"
   },
   "source": [
    "#### Create Group Graphs for Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgCZM2BXldo_"
   },
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, data = df_to_pyg_temporal_data(pd.concat([df_italy_train, df_italy_test], ignore_index=True), EVENTLOG_GROUP)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIYwNcFLldo_"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "train_loader = pyg.loader.TemporalDataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "val_loader = pyg.loader.TemporalDataLoader(\n",
    "    val_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "test_loader = pyg.loader.TemporalDataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "\n",
    "neighbor_loader = pyg.nn.models.tgn.LastNeighborLoader(data.num_nodes, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8dT_PgZldpA"
   },
   "outputs": [],
   "source": [
    "memory = pyg.nn.models.TGNMemory(\n",
    "    data.num_nodes,\n",
    "    data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=pyg.nn.models.tgn.IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=pyg.nn.models.tgn.LastAggregator(),\n",
    ")\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(set(memory.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=0.00001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4sj2YVWldpA"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    val_ap, val_auc = test(val_loader)\n",
    "    test_ap, test_auc = test(test_loader)\n",
    "    print(f'Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"italy_group_mapping.json\"), \"w\") as f:\n",
    "  json.dump(mapping, f)\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"italy_group_inverse_mapping.json\"), \"w\") as f:\n",
    "  json.dump(reverse_mapping, f)\n",
    "\n",
    "embeddings = embed()\n",
    "np.save(os.path.join(OUTPUT_DATA_DIR, \"italy_group_embeddings.npy\"), embeddings, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9Fjt8vMldpB"
   },
   "outputs": [],
   "source": [
    "arr_nan = np.full(EMBEDDING_DIM, -99.0, dtype='float32')\n",
    "\n",
    "embeddings_df_train = df_italy_train_dyn[EVENTLOG_GROUP].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "group_graph_embeddings_train = np.empty((embeddings_df_train.shape[0], embeddings_df_train.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_train)):\n",
    "  group_graph_embeddings_train[i] = np.stack(embeddings_df_train[i])\n",
    "\n",
    "group_graph_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3fhBeHOldpB"
   },
   "outputs": [],
   "source": [
    "embeddings_df_test = df_italy_test_dyn[EVENTLOG_GROUP].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "group_graph_embeddings_test = np.empty((embeddings_df_test.shape[0], embeddings_df_test.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_test)):\n",
    "  group_graph_embeddings_test[i] = np.stack(embeddings_df_test[i])\n",
    "\n",
    "group_graph_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIXweSytmV_t"
   },
   "source": [
    "### Build Dataset for Dataset belonging to the help desk log of an Italian Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p37AHu5KmV_u"
   },
   "outputs": [],
   "source": [
    "ds_italy_train = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_italy_train_dyn[\"concept:name\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource\": df_italy_train_dyn[\"org:resource\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"customer\": df_italy_train_dyn[\"customer\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"product\": df_italy_train_dyn[\"product\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"seriousness_2\": df_italy_train_dyn[\"seriousness_2\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"service_level\": df_italy_train_dyn[\"service_level\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"service_type\": df_italy_train_dyn[\"service_type\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_group\": df_italy_train_dyn[\"org:group\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource_graph\": resource_graph_embeddings_train,\n",
    "  \"org_group_graph\": group_graph_embeddings_train,\n",
    "  \"time_timestamp_elapsedcycle\": df_italy_train_dyn[\"time:timestamp:elapsedcycle:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_italy_train_dyn[\"time:timestamp:elapsedprev:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_italy_train_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_italy_train_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_italy_train_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_italy_train_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_italy_train_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_italy_train_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_italy_train_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_italy_train_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_italy_train_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_italy_train_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"case_responsible_section\": np.expand_dims(df_italy_train[\"case:responsible_section\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_support_section\": np.expand_dims(df_italy_train[\"case:support_section\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_italy_train[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_italy_train[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_italy_train[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_italy_train, os.path.join(OUTPUT_DATA_DIR, 'finale_train_dataset'))\n",
    "\n",
    "ds_italy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tR5UV3-znkJ_"
   },
   "outputs": [],
   "source": [
    "ds_italy_test = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_italy_test_dyn[\"concept:name\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource\": df_italy_test_dyn[\"org:resource\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"customer\": df_italy_test_dyn[\"customer\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"product\": df_italy_test_dyn[\"product\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"seriousness_2\": df_italy_test_dyn[\"seriousness_2\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"service_level\": df_italy_test_dyn[\"service_level\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"service_type\": df_italy_test_dyn[\"service_type\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_group\": df_italy_test_dyn[\"org:group\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource_graph\": resource_graph_embeddings_test,\n",
    "  \"org_group_graph\": group_graph_embeddings_test,\n",
    "  \"time_timestamp_elapsedcycle\": df_italy_test_dyn[\"time:timestamp:elapsedcycle:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_italy_test_dyn[\"time:timestamp:elapsedprev:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_italy_test_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_italy_test_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_italy_test_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_italy_test_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_italy_test_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_italy_test_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_italy_test_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_italy_test_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_italy_test_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_italy_test_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"case_responsible_section\": np.expand_dims(df_italy_test[\"case:responsible_section\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_support_section\": np.expand_dims(df_italy_test[\"case:support_section\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_italy_test[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_italy_test[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_italy_test[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_italy_test, os.path.join(OUTPUT_DATA_DIR, 'finale_test_dataset'))\n",
    "\n",
    "ds_italy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwaK3XFhJ9DB"
   },
   "source": [
    "## Dataset: BPIC 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9hLpLZ9Pqrw"
   },
   "source": [
    "This dataset is provided by the Rabobank ICT and contains information about the employed ITIL processes. See https://www.win.tue.nl/bpi/2014/challenge.html and https://data.4tu.nl/collections/dff0e630-9c91-4b8e-806d-ec9a3a0f2206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5hH98ihKejM"
   },
   "source": [
    "### Incident Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r18F3DJqRNxt"
   },
   "source": [
    "Based on an estimated Impact and Urgency, done by the SDA, an Incident-record is prioritized and gets a deadline to resolve the service disruption. A Team  eader within the Assignment Group assigns the records to an Operator. The  Operator resolves the issue for the customer, or reassigns the record to a colleague if other or more knowledge is needed. After solving the issue for the customer, the Operator relates the Incident-record to the Configuration Item (CausedBy CI) that caused the service disruption. After closing the Incident-record, the customer receives an email to inform that the issue is resolved. See also https://doi.org/10.4121/uuid:3cfa2260-f5c5-44be-afe1-b70d35288d6d.\n",
    "\n",
    "- **Dataset attributes:**  \n",
    "  - *CI Name (aff):* Configuration Item (CI) where a disruption of an ICT Service is noticed, this is what we call the \"Affected CI\". When a Service Desk Agent decides to create an Incident from an Interaction, the Affected CI is copied from the Interaction-record into the Incident-record.\n",
    "\n",
    "  - *CI Type (aff):* Every CI in the CMDB is related to an Entity Type.\n",
    "\n",
    "  - *CI Subtype (aff):* Every CI in the CMDB is related to a Subtype, which is related to a CI Type.\n",
    "\n",
    "  - *Service Comp WBS (aff):* Every CI in the CMDB is related to 1 Service Component, in order to identify which Product Manager is responsible for the CI. A Service Component is equal to a product in the Bill of Material and is part of one or more Services.\n",
    "\n",
    "  - *Incident ID:* The unique ID of an Incident-record in the Service Management tool.\n",
    "\n",
    "  - *Status:* Status of the Incident-record.\n",
    "\n",
    "  - *Impact:* Impact of the service disruption to the customer.\n",
    "\n",
    "  - *Urgency:* Indication of how urgent the customer needs a solution.\n",
    "\n",
    "  - *Priority:* Impact and Urgency lead to a Priority for the Assignment Group to resolve the service disruption.\n",
    "\n",
    "  - *Category:* In order to select and compare similar Incidents in the Service Management tool, all records are categorized. The categorization is derived from the Knowledge Document.\n",
    "\n",
    "  - *KM number:* A Knowledge Document contains default attribute values for the Interaction-record and a set of questions for a Service Desk Agent to derive which Configuration Item is disrupted and to determine Impact and Urgency for the customer.\n",
    "\n",
    "  - *Open Time:* Date and time the Incident record was opened in the Service Management tool.\n",
    "\n",
    "  - *Reopen Time:* Date and time the Incident record was reopened in the Service Management tool. This option is used when an Incident record was closed and within a short period of time it is discovered that the resolution is not effective for the customer.\n",
    "\n",
    "  - *Resolved Time:* Date and time the Service disruption is resolved.\n",
    "\n",
    "  - *Closed Time:* Date and time the Incident record is closed in the Service Management tool.\n",
    "\n",
    "  - *Handle Time (secs):* Time registered to resolve the service disruption.\n",
    "\n",
    "  - *Closure Code:* Short code to classify the type of Service disruption.\n",
    "  - *Alert Status:* Alert status of the Incident-record, during its lifecycle, based on defined Service Levels in the Service Management tool.\n",
    "\n",
    "  - *Reassignments:* Number of Incident Activities with Activity Type \"Reassignment\".\n",
    "\n",
    "  - *Related Interactions:* Number of related Interactions to this Incident. Related Interaction Record-number if only one Interaction is related to this Incident.\n",
    "\n",
    "  - *Related Incidents:* Number of similar Incidents, related to this record. The related Incidents are what we call child-records for this parent Incident-record, which is used for logging all Activities to resolve the service disruption.\n",
    "\n",
    "  - *Related Changes:* Number of related Changes to this Incident. Related Change Record-number if only one Change is related to this Incident.\n",
    "\n",
    "  - *CI Name (CBy):* Configuration Item (CI) which caused the disruption of an ICT Service, this is what we call the \"CausedBy CI\". When an Operator resolves an Incident, the CausedBy CI must be registered before closing the Incident-record.\n",
    "\n",
    "  - *CI Type (CBy):* Every CI in the CMDB is related to an Entity Type.\n",
    "\n",
    "  - *CI Subtype (CBy):* Every CI in the CMDB is related to a Subtype, which is related to a CI Type.\n",
    "\n",
    "  - *ServiceComp WBS (CBy):* Every CI in the CMDB is related to 1 Service Component, in order to identify which Product Manager is responsible for the CI. A Service Component is equal to a product in the Bill of Material.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JHowdCpvJJp"
   },
   "source": [
    "#### Read BPIC 2014 Incident Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HERyL22vIK6"
   },
   "outputs": [],
   "source": [
    "dtypes_bpic14_incident = {\n",
    "    'CI Name (aff)': 'category',\n",
    "    'CI Type (aff)': 'category',\n",
    "    'CI Subtype (aff)': 'category',\n",
    "    'Service Component WBS (aff)': 'category',\n",
    "    'Incident ID': 'string',\n",
    "    'Status': pd.CategoricalDtype(['Work in progress', 'Closed'], ordered=True),\n",
    "    'Impact': pd.CategoricalDtype(range(1, 6), ordered=True),\n",
    "    'Urgency': pd.CategoricalDtype(['1', '2', '3', '4', '5', '5 - Very Low'], ordered=True),\n",
    "    'Priority': pd.CategoricalDtype(range(1, 6), ordered=True),\n",
    "    'Category': 'category',\n",
    "    'KM number': 'category',\n",
    "    '# Reassignments': 'Int16',\n",
    "    'Alert Status': 'category',\n",
    "    'Closure Code': 'category',\n",
    "    '# Related Interactions': 'Int16',\n",
    "    'Related Interaction': 'string',\n",
    "    '# Related Incidents': 'Int16',\n",
    "    '# Related Changes': 'Int16',\n",
    "    'Related Change': 'string',\n",
    "    'CI Name (CBy)': 'category',\n",
    "    'CI Type (CBy)': 'category',\n",
    "    'CI Subtype (CBy)': 'category',\n",
    "    'ServiceComp WBS (CBy)': 'category'\n",
    "}\n",
    "\n",
    "df_bpic14_incident = pd.read_csv(\n",
    "    os.path.join(INPUT_DATA_BPIC2014_DIR, 'Detail_Incident.csv'),\n",
    "    header=0,\n",
    "    sep=';',\n",
    "    decimal=',',\n",
    "    dtype=dtypes_bpic14_incident,\n",
    "    na_values=['#MULTIVALUE', '#N/B', 'Unknown']\n",
    ")\n",
    "\n",
    "df_drop_duplicate_rows(df_bpic14_incident)\n",
    "df_drop_na_rows_and_cols(df_bpic14_incident)\n",
    "df_drop_single_val_cols(df_bpic14_incident)\n",
    "\n",
    "# Convert timestamps\n",
    "df_bpic14_incident = df_convert_datetimes(\n",
    "    df_bpic14_incident,\n",
    "    [\"Open Time\", \"Reopen Time\", \"Resolved Time\", \"Close Time\"],\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "# Convert time deltas\n",
    "df_bpic14_incident = df_convert_timedeltas(df_bpic14_incident, ['Handle Time (Hours)'], unit=\"hours\")\n",
    "\n",
    "# Unify category namings\n",
    "df_bpic14_incident = df_rename_cat_values(df_bpic14_incident, ['CI Subtype (aff)', 'CI Subtype (CBy)'], 'Iptelephony', 'IPtelephony')\n",
    "df_bpic14_incident = df_rename_cat_values(df_bpic14_incident, ['Urgency'], '5 - Very Low', '5')\n",
    "\n",
    "df_bpic14_incident.sort_values(by=['Incident ID', 'Open Time'], inplace=True, ignore_index=True)\n",
    "\n",
    "df_write_files(df_bpic14_incident, os.path.join(INTERIM_DATA_DIR, \"Detail_Incident_processed\"))\n",
    "\n",
    "print(df_bpic14_incident.dtypes)\n",
    "df_bpic14_incident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiT_ERJuc8gM"
   },
   "source": [
    "#### Descriptive Statistics for BPIC 2014 Incident Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhPqlwj1c8gQ"
   },
   "outputs": [],
   "source": [
    "df_stat = df_bpic14_incident.describe(include='all')\n",
    "df_write_files(df_stat, os.path.join(OUTPUT_DATA_DIR, \"Detail_Incident_describe\"), index=True)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4672_L7wc8gS"
   },
   "outputs": [],
   "source": [
    "df_bpic14_incident.hist(xrot=90, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"Detail_Incident_hist.svg\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp8apUseKk2a"
   },
   "source": [
    "### Change Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IL5eSDmzVTib"
   },
   "source": [
    "If particular service disruptions reoccur more often than usual, a Problem investigation is started, which will lead to an analysis and improvement plan to prevent the service disruption to happen again. The improvement plan leads to a Request for Change (RfC) on the CausedBy CI. All CI's are related to a Service Component, Risk Impact Analysis is done by an Implementation Manager assigned to changes related to the specific Service Component. See also https://doi.org/10.4121/uuid:d5ccb355-ca67-480f-8739-289b9b593aaf.\n",
    "\n",
    "- **Dataset attributes:**\n",
    "  - *CI Name (aff):* Configuration Item (CI) which will be affected by this change. A change can be related to more than one Configuration Item.\n",
    "\n",
    "  - *CI Type (aff):* Every CI in the CMDB is related to an Entity Type.\n",
    "\n",
    "  - *CI Subtype (aff):* Every CI in the CMDB is related to a Subtype, which is related to a CI Type.\n",
    "\n",
    "  - *Service Comp WBS (aff):* Every CI in the CMDB is related to 1 Service Component, in order to identify which Product Manager is responsible for the CI. A Service Component is equal to a product in the Bill of Material and is part of one or more Services.\n",
    "\n",
    "  - *Change ID:* The unique ID of a Change-record in the Service Management tool.\n",
    "\n",
    "  - *Change Type:* In order to select and compare similar Changes in the Service Management tool, all records are categorized by Change Type.\n",
    "\n",
    "  - *Risk Assessment:* Impact of change: Major Business Change, Business Change or Minor Change.\n",
    "\n",
    "  - *Emergency Change:* Indication if the change is an emergency fix.\n",
    "\n",
    "  - *CAB-approval needed:* Indication is the changes need approval by the Change Advisory Board, before implementation.\n",
    "\n",
    "  - *Planned Start Date:* Date and time the change implementation is planned to start.\n",
    "\n",
    "  - *Planned End Date:* Date and time the change implementation is planned to end.\n",
    "\n",
    "  - *Scheduled Downtime Start Date:* Date and time the Service Downtime is scheduled to start.\n",
    "\n",
    "  - *Scheduled Downtime End Date:* Date and time the Service Downtime is scheduled to end.\n",
    "\n",
    "  - *Actual Start Date:* Date and time the change implementation is actually started.\n",
    "\n",
    "  - *Actual End Date:* Date and time the change implementation actually ended.\n",
    "\n",
    "  - *Requested End Date:* Date and time before the change requestor wants the change to be implemented.\n",
    "\n",
    "  - *Change record Open Time:* Date and time the Change record was opened in the Service Management tool.\n",
    "\n",
    "  - *Change record Close Time:* Date and time the Change record is closed in the Service Management tool.\n",
    "\n",
    "  - *Originated from:* Indication if the change originated from, for instance, Problem research, or is a quick fix for an Incident.\n",
    "\n",
    "  - *# Related Interactions:* Number of Interactions caused by this change.\n",
    "\n",
    "  - *# Related Incidents:* Number of Incidents caused by this change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oh5AqnpvOJ-"
   },
   "source": [
    "#### Read BPIC 2014 Change Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jY1o43NSePme"
   },
   "outputs": [],
   "source": [
    "dtypes_bpic14_change = {\n",
    "    'CI Name (aff)': 'category',\n",
    "    'CI Type (aff)': 'category',\n",
    "    'CI Subtype (aff)': 'category',\n",
    "    'Service Component WBS (aff)': 'category',\n",
    "    'Change ID': 'string',\n",
    "    'Change Type': 'category',\n",
    "    'Risk Assessment': pd.CategoricalDtype(['Minor Change', 'Business Change', 'Major Business Change'], ordered=True),\n",
    "    'Emergency Change': 'boolean',\n",
    "    'CAB-approval needed': 'boolean',\n",
    "    'Originated from': 'category',\n",
    "    '# Related Interactions': 'Int16',\n",
    "    '# Related Incidents': 'Int16',\n",
    "}\n",
    "\n",
    "df_bpic14_change = pd.read_csv(\n",
    "    os.path.join(INPUT_DATA_BPIC2014_DIR, 'Detail_Change.csv'),\n",
    "    header=0,\n",
    "    sep=';',\n",
    "    dtype=dtypes_bpic14_change,\n",
    "    true_values=['Y'],\n",
    "    false_values=['N'],\n",
    "    na_values=['#N/B']\n",
    ")\n",
    "\n",
    "# Convert dates\n",
    "df_bpic14_change = df_convert_datetimes(\n",
    "    df_bpic14_change,\n",
    "    ['Planned Start', 'Planned End', 'Scheduled Downtime Start', 'Scheduled Downtime End', 'Actual Start', 'Actual End', 'Requested End Date', 'Change record Open Time', 'Change record Close Time'],\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "df_drop_duplicate_rows(df_bpic14_change)\n",
    "df_drop_na_rows_and_cols(df_bpic14_change)\n",
    "df_drop_single_val_cols(df_bpic14_change)\n",
    "\n",
    "# Sort by Incident ID, DateStamp and IncidentActivity_Number\n",
    "df_bpic14_change.sort_values(['Change ID', 'Change record Open Time'], ignore_index=True, inplace=True)\n",
    "\n",
    "df_write_files(df_bpic14_change, os.path.join(INTERIM_DATA_DIR, \"Detail_Change_processed\"))\n",
    "\n",
    "print(df_bpic14_change.dtypes)\n",
    "df_bpic14_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osmz_0hTd9FJ"
   },
   "source": [
    "#### Descriptive Statistics for BPIC 2014 Incident Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRGR-SZVd9FL"
   },
   "outputs": [],
   "source": [
    "df_stat = df_bpic14_change.describe(include='all')\n",
    "df_write_files(df_stat, os.path.join(OUTPUT_DATA_DIR, \"Detail_Change_describe\"), index=True)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvVN9x38d9FN"
   },
   "outputs": [],
   "source": [
    "df_bpic14_change.hist(xrot=90, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"Detail_Change_hist.svg\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJzx7M7AKrge"
   },
   "source": [
    "### Interaction Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooTr1JVCVIWy"
   },
   "source": [
    "In order to manage calls or mails from customers (Rabobank colleagues) to the Service Desk concerning disruptions of ICT-services, a Service Desk Agent (SDA) logs calls/mails in an Interaction-record and relates it to an Affected Configuration Item (CI) The SDA can either resolve the issue for the customer  directly (First Call Resolution) or create an Incident-record to assign the issue to an Assignment Group with more technical knowledge to resolve the service disruption. If similar calls/mails are received by the Service Desk, a SDA can decide to relate multiple Interaction-records to one Incident-record Further logging of Activities to resolve the service disruption will be done in the Incident-record. See also https://doi.org/10.4121/uuid:3d5ae0ce-198c-4b5c-b0f9-60d3035d07bf.\n",
    "\n",
    "\n",
    "- **Dataset attributes:**\n",
    "  - *CI Name (aff):* Configuration Item (CI) where a disruption of an ICT Service is noticed, this is what we call the \"Affected CI\". A Service Desk Agent always uses questions in a Knowledge Document (identified by a KM number) to find the correct CI in the Configuration Item Database (CMDB).\n",
    "\n",
    "  - *CI Type (aff):* Every CI in the CMDB is related to an Entity Type.\n",
    "\n",
    "  - *CI Subtype (aff):* Every CI in the CMDB is related to a Subtype, which is related to a CI Type.\n",
    "\n",
    "  - *Service Comp WBS (aff):* Every CI in the CMDB is related to 1 Service Component, in order to identify which Product Manager is responsible for the CI. A Service Component is equal to a product in the Bill of Material and is part of one or more Services.\n",
    "\n",
    "  - *Interaction ID:* The unique ID of an Interaction-record in the Service Management tool.\n",
    "\n",
    "  - *Status:* Status of the Interaction-record.\n",
    "\n",
    "  - *Impact:* Impact of the service disruption to the customer.\n",
    "\n",
    "  - *Urgency:* Indication of how urgent the customer needs a solution.\n",
    "\n",
    "  - *Priority:* Impact and Urgency lead to a Priority for the Assignment Group to resolve the service disruption.\n",
    "\n",
    "  - *Category:* In order to select and compare similar Interactions in the Service Management tool, all records are categorized. The categorization is derived from the Knowledge Document.\n",
    "\n",
    "  - *KM number:* A Knowledge Document contains default attribute values for the Interaction-record and a set of questions for a Service Desk Agent to derive which Configuration Item is disrupted and to determine Impact and Urgency for the customer.\n",
    "\n",
    "  - *Open Time (First Touch):* Date and time the Interaction record was opened in the Service Management tool.\n",
    "\n",
    "  - *Close Time:* Date and time the Interaction record is closed in the Service Management tool.\n",
    "\n",
    "  - *Closure Code:* Short code to classify the type of service disruption.\n",
    "\n",
    "  - *First Call Resolution:* Flag which indicates if the Service Desk Agent was able to provide the customer with a workaround for the Service disruption (Y) or if it was necessary to assign the record to a specialist by creating an Incident record.\n",
    "\n",
    "  - *Handle Time (secs):* Time registered to resolve the service disruption.\n",
    "\n",
    "  - *Related Incident Record:* Number of the Incident created from this Interaction-record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2aghSBavWs8"
   },
   "source": [
    "#### Read BPIC 2014 Interaction Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrhjWb-EvWJ7"
   },
   "outputs": [],
   "source": [
    "dtypes_bpic14_interaction = {\n",
    "    'CI Name (aff)': 'category',\n",
    "    'CI Type (aff)': 'category',\n",
    "    'CI Subtype (aff)': 'category',\n",
    "    'Service Comp WBS (aff)': 'category',\n",
    "    'Interaction ID': 'string',\n",
    "    'Status': 'category',\n",
    "    'Impact': 'category',\n",
    "    'Urgency': 'category',\n",
    "    'Priority': pd.CategoricalDtype(range(1, 6), ordered=True),\n",
    "    'Category': 'category',\n",
    "    'KM number': 'category',\n",
    "    'Closure Code': 'category',\n",
    "    'First Call Resolution': 'boolean',\n",
    "    'Related Incident': 'string',\n",
    "}\n",
    "\n",
    "df_bpic14_interaction = pd.read_csv(\n",
    "    os.path.join(INPUT_DATA_BPIC2014_DIR, 'Detail_Interaction.csv'),\n",
    "    header=0,\n",
    "    sep=';',\n",
    "    dtype=dtypes_bpic14_interaction,\n",
    "    true_values=['Y'],\n",
    "    false_values=['N'],\n",
    "    na_values=['#MULTIVALUE', '#N/B']\n",
    ")\n",
    "\n",
    "# Convert timestamps\n",
    "df_bpic14_interaction = df_convert_datetimes(\n",
    "    df_bpic14_interaction,\n",
    "   [\"Open Time (First Touch)\", \"Close Time\"],\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "# Convert time deltas\n",
    "df_bpic14_interaction = df_convert_timedeltas(df_bpic14_interaction, [\"Handle Time (secs)\"], unit=\"seconds\")\n",
    "\n",
    "df_drop_duplicate_rows(df_bpic14_interaction)\n",
    "df_drop_na_rows_and_cols(df_bpic14_interaction)\n",
    "df_drop_single_val_cols(df_bpic14_interaction)\n",
    "\n",
    "# Unify category namings\n",
    "df_bpic14_interaction = df_rename_cat_values(df_bpic14_interaction, 'Closure Code', 'unknown', 'Unknown')\n",
    "df_bpic14_interaction = df_rename_cat_values(df_bpic14_interaction, 'Closure Code', ['SOFTWARE', 'software'], 'Software')\n",
    "\n",
    "# Sort by Incident ID and Open Time\n",
    "df_bpic14_interaction.sort_values(['Interaction ID', 'Open Time (First Touch)'], ignore_index=True, inplace=True)\n",
    "\n",
    "df_write_files(df_bpic14_interaction, os.path.join(INTERIM_DATA_DIR, \"Detail_Interaction_processed\"))\n",
    "\n",
    "print(df_bpic14_interaction.dtypes)\n",
    "df_bpic14_interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-FAq0IKkj44"
   },
   "source": [
    "#### Descriptive Statistics for BPIC 2014 Interaction Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe7B7s01kj47"
   },
   "outputs": [],
   "source": [
    "df_stat = df_bpic14_interaction.describe(include='all')\n",
    "df_write_files(df_stat, os.path.join(OUTPUT_DATA_DIR, \"Detail_Interaction_describe\"), index=True)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmB3KEtAkj49"
   },
   "outputs": [],
   "source": [
    "df_bpic14_interaction.hist(xrot=90, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"Detail_Interaction_hist.svg\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ1CE1gSKZ2P"
   },
   "source": [
    "### Incident Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RixDh70P23d"
   },
   "source": [
    "- *Incident ID:* The unique ID of an Incident-record in the Service Management tool.\n",
    "\n",
    "- *DateStamp:* Date and time when this specific Incident Activity started.\n",
    "\n",
    "- *IncidentActivity_Number:* Unique ID for an Incident Activity.\n",
    "\n",
    "- *IncidentActivity_Type:* Short code to identify which type of Incident Activity took place.\n",
    "\n",
    "- *Interaction ID:* The unique ID of an Interaction-record in the Service Management tool.\n",
    "\n",
    "- *Assignment Group:* The team responsible for this Incident Activity.\n",
    "\n",
    "- *KM number:* A Knowledge Document contains default attribute values for the Interaction-record and a set of questions for a Service Desk Agent to derive which Configuration Item is disrupted and to determine Impact and Urgency for the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6rlZoabKC-H"
   },
   "source": [
    "#### Read BPIC 2014 Incident Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs1NKq-iWmLw"
   },
   "source": [
    "See https://doi.org/10.4121/uuid:86977bac-f874-49cf-8337-80f26bf5d2ef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k0vkSFNKHKm"
   },
   "outputs": [],
   "source": [
    "dtypes_bpic14 = {\n",
    "    'Incident ID': 'string',\n",
    "    'IncidentActivity_Number': 'string',\n",
    "    'IncidentActivity_Type': 'category',\n",
    "    'Assignment Group': 'category',\n",
    "    'KM number': 'category',\n",
    "    'Interaction ID': 'string'\n",
    "}\n",
    "\n",
    "df_bpic14 = pd.read_csv(\n",
    "    os.path.join(INPUT_DATA_BPIC2014_DIR, 'Detail_Incident_Activity.csv'),\n",
    "    header=0,\n",
    "    sep=';',\n",
    "    decimal=',',\n",
    "    dtype=dtypes_bpic14,\n",
    "    na_values=['#N/B']\n",
    ")\n",
    "\n",
    "# Convert timestamps\n",
    "df_bpic14 = df_convert_datetimes(\n",
    "    df_bpic14,\n",
    "    [\"DateStamp\"],\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "# Combine other data sources\n",
    "df_bpic14 = df_bpic14.join(\n",
    "    df_bpic14_incident.set_index('Incident ID').add_prefix(\"incident_\"),\n",
    "    how='left',\n",
    "    on='Incident ID',\n",
    "    #validate='m:1'\n",
    ").join(\n",
    "    df_bpic14_change.set_index('Change ID').add_prefix(\"change_\"),\n",
    "    how='left',\n",
    "    on='incident_Related Change',\n",
    "    #validate='m:m'\n",
    ").join(\n",
    "    df_bpic14_interaction.set_index('Interaction ID').add_prefix(\"interaction_\"),\n",
    "    how='left',\n",
    "    on='incident_Related Interaction',\n",
    "    #validate='m:m'\n",
    ")\n",
    "\n",
    "df_drop_duplicate_rows(df_bpic14)\n",
    "df_drop_na_rows_and_cols(df_bpic14)\n",
    "df_drop_single_val_cols(df_bpic14)\n",
    "\n",
    "# Sort and normalize column names\n",
    "df_format_as_eventlog(df_bpic14, case_col=\"Incident ID\", activity_col=\"IncidentActivity_Type\", time_col=\"DateStamp\", group_col=\"Assignment Group\")\n",
    "\n",
    "# Write as Pandas CSV\n",
    "df_write_files(df_bpic14, os.path.join(INTERIM_DATA_DIR, \"Detail_Incident_Activity_processed\"), skip_xes=False)\n",
    "\n",
    "print(df_bpic14.dtypes)\n",
    "df_bpic14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5J7mf75J1fU3"
   },
   "source": [
    "#### Label BPIC 2014 Incident Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxCDfVDe1dJm"
   },
   "outputs": [],
   "source": [
    "df_bpic14 = df_label_next_activity(df_bpic14, eoc_token=TOKEN_EOC)\n",
    "df_bpic14 = df_label_activity_duration(df_bpic14, unit='d', eoc_token=pd.Timedelta(0))\n",
    "df_bpic14 = df_label_remaining_cycle_time(df_bpic14, unit='d')\n",
    "\n",
    "#df_write_files(df_bpic14, os.path.join(INTERIM_DATA_DIR, \"Detail_Incident_Activity_labeled\"))\n",
    "df_bpic14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQCJd8Tp4u24"
   },
   "source": [
    "\n",
    "#### Descriptive Statistics for BPIC 2014 Incident Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDN6fClV4tsV"
   },
   "outputs": [],
   "source": [
    "df_stat = df_bpic14.describe(include='all')\n",
    "df_write_files(df_stat, os.path.join(OUTPUT_DATA_DIR, \"Detail_Incident_Activity_describe\"), index=True)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwXVpTfS6Di5"
   },
   "outputs": [],
   "source": [
    "df_bpic14.hist(xrot=90, figsize=(20, 20))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"Detail_Incident_Activity_hist.svg\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Xo7hmr6L8w5"
   },
   "outputs": [],
   "source": [
    "df_case_length_stats(df_bpic14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLev9duT4Pg-"
   },
   "outputs": [],
   "source": [
    "df_case_duration_stats(df_bpic14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGoND8NOeEnh"
   },
   "source": [
    "#### Clean BPIC 2014 Incident Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWfE_0ereDLg"
   },
   "outputs": [],
   "source": [
    "# Filter open incidents\n",
    "df_bpic14_clean = df_bpic14[df_bpic14[\"case:incident_Status\"] == \"Closed\"]\n",
    "\n",
    "df_bpic14_clean = df_bpic14_clean.drop(columns=[\n",
    "    \"IncidentActivity_Number\", # ID\n",
    "    \"case:incident_# Reassignments\", # Correlated to change_CI Name (aff)\n",
    "    \"case:change_CAB-approval needed\", # Correlated to change_CI Name (aff) and case:change_Change Type\n",
    "    \"case:interaction_CI Name (aff)\", # Correlated to case:incident_CI Name (aff)\n",
    "    \"case:interaction_CI Type (aff)\", # Correlated to case:incident_CI Type (aff)\n",
    "    \"case:interaction_CI Subtype (aff)\", # Correlated to case:incident_CI Subtype (aff)\n",
    "    \"case:incident_ServiceComp WBS (CBy)\", # Correlated to case:incident_Service Component WBS (aff); a-posteriori\n",
    "    \"case:interaction_Service Comp WBS (aff)\", # Correlated to 'case:incident_Service Component WBS (aff)'\n",
    "    \"case:incident_Impact\", # Correlated to case:incident_Urgency and case:incident_Priority\n",
    "    \"case:incident_Urgency\", # Correlated to case:incident_Impact and case:incident_Priority\n",
    "    \"case:interaction_Category\", # Correlated to case:interaction_Category\n",
    "    \"case:interaction_Closure Code\", # Correlated to case:interaction_Closure Code\n",
    "    \"case:interaction_Impact\", # Correlated to case:interaction_Urgency and case:interaction_Priority\n",
    "    \"case:interaction_Urgency\", # Correlated to case:interaction_Impact and case:interaction_Priority\n",
    "    \"case:interaction_KM number\", # Correlated to case:incident_KM number\n",
    "    \"case:incident_Open Time\", # In sequence encoded\n",
    "    \"case:incident_Reopen Time\", # In sequence encoded, a-posteriori\n",
    "    \"case:incident_Resolved Time\", # A-posteriori\n",
    "    \"case:incident_Close Time\", # A-posteriori\n",
    "    \"case:incident_Handle Time (Hours)\", # A-posteriori,\n",
    "    \"case:incident_Closure Code\", # A-posteriori\n",
    "    \"case:incident_# Related Interactions\", # A-posteriori\n",
    "    \"case:incident_Related Interaction\", # ID\n",
    "    \"case:incident_# Related Incidents\", # A-posteriori\n",
    "    \"case:incident_# Related Changes\", # A-posteriori\n",
    "    \"case:incident_Related Change\", # ID\n",
    "    \"case:interaction_Related Incident\", # ID\n",
    "    \"case:interaction_Handle Time (secs)\", # A-posteriori\n",
    "    \"case:Interaction ID\", # ID\n",
    "    \"case:incident_Status\", # Single value\n",
    "    \"case:change_Actual Start\",\n",
    "    \"case:change_Actual End\",\n",
    "    \"case:change_Change record Open Time\",\n",
    "    \"change_Change record Close Time\",\n",
    "    \"case:interaction_Open Time (First Touch)\",\n",
    "    \"case:change_Planned Start\",\n",
    "    \"case:change_Planned End\",\n",
    "    \"case:change_Scheduled Downtime Start\",\n",
    "    \"case:change_Scheduled Downtime End\",\n",
    "    \"case:change_Requested End Date\",\n",
    "    \"case:interaction_Close Time\"\n",
    "])\n",
    "\n",
    "# Filter event log\n",
    "df_bpic14_clean = df_filter_case_duration_range(df_bpic14_clean, max=0.95)\n",
    "df_bpic14_clean = df_filter_date_range(df_bpic14_clean, min=\"2013-09-01 00:00:00\", mode='traces_included')\n",
    "\n",
    "# Remove mostly empty columns\n",
    "df_drop_threshold_na_cols(df_bpic14_clean, 0.95)\n",
    "\n",
    "# Create time features\n",
    "df_bpic14_clean = df_extract_elapsed_cycle_time(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_CYCLE_SUFFIX}:seconds\", unit='s')\n",
    "df_bpic14_clean = df_extract_activity_duration(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX}:seconds\", unit='s', na_token=0)\n",
    "\n",
    "df_bpic14_clean = df_extract_month_of_year(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:month\", relative=True)\n",
    "df_bpic14_clean = df_extract_day_of_year(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear\", relative=True)\n",
    "df_bpic14_clean = df_extract_day_of_month(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:day\", relative=True)\n",
    "df_bpic14_clean = df_extract_day_of_week(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:weekday\", relative=True)\n",
    "df_bpic14_clean = df_extract_hour_of_day(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:hour\", relative=True)\n",
    "\n",
    "df_bpic14_clean = df_extract_month_of_year(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:month:raw\")\n",
    "df_bpic14_clean = df_extract_day_of_year(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear:raw\")\n",
    "df_bpic14_clean = df_extract_day_of_month(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:day:raw\")\n",
    "df_bpic14_clean = df_extract_day_of_week(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:weekday:raw\")\n",
    "df_bpic14_clean = df_extract_hour_of_day(df_bpic14_clean, f\"{EVENTLOG_TIMESTAMP}:hour:raw\")\n",
    "\n",
    "# Encode categorical labels\n",
    "df_bpic14_clean = df_encode_label(df_bpic14_clean, cols=EVENTLOG_LABEL_NEXT_ACT)\n",
    "\n",
    "# Transform bools to int\n",
    "df_bpic14_clean = df_convert_bool_to_int(df_bpic14_clean)\n",
    "\n",
    "# Transform ordered cats to int\n",
    "df_bpic14_clean = df_convert_ordered_cat_to_int(df_bpic14_clean, relative=True)\n",
    "\n",
    "# Fill empty str values\n",
    "df_bpic14_clean = df_fillna_str(df_bpic14_clean, TOKEN_NA)\n",
    "\n",
    "# Fill empty cat values\n",
    "df_bpic14_clean = df_fillna_cat(df_bpic14_clean, TOKEN_NA)\n",
    "\n",
    "df_write_files(df_bpic14_clean, os.path.join(OUTPUT_DATA_DIR, \"Detail_Incident_Activity_cleaned\"))\n",
    "\n",
    "df_bpic14_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRNLRrxESrfo"
   },
   "outputs": [],
   "source": [
    "df_bpic14_train, df_bpic14_test = df_strict_temporal_train_test_split(\n",
    "    df_bpic14_clean,\n",
    "    0.2,\n",
    "    df_find_labels(df_bpic14_clean),\n",
    "    debias_end=False\n",
    ")\n",
    "\n",
    "axes = df_visualize_strict_temporal_splitting(df_bpic14_train, df_bpic14_test)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"Detail_Incident_Activity_train_test.svg\"), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Remove overlaps\n",
    "df_bpic14_test.dropna(axis='index', how='any', subset=df_find_labels(df_bpic14_test), inplace=True)\n",
    "\n",
    "# Remove two very long cases\n",
    "df_bpic14_train = df_bpic14_train[~df_bpic14_train[EVENTLOG_CASE].isin([\"IM0000944\", \"IM0018374\"])]\n",
    "\n",
    "df_write_files(df_bpic14_train, os.path.join(OUTPUT_DATA_DIR, \"Detail_Incident_Activity_train\"))\n",
    "df_write_files(df_bpic14_test, os.path.join(OUTPUT_DATA_DIR, \"Detail_Incident_Activity_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrcwEsdU2dIv"
   },
   "outputs": [],
   "source": [
    "df_bpic14_dyn = df_prefix_pad_attributes(\n",
    "    pd.concat([df_bpic14_train, df_bpic14_test]),\n",
    "    df_find_event_attributes(df_bpic14_clean, exclude_labels=True)\n",
    ")\n",
    "\n",
    "df_bpic14_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-odohffZ4NLP"
   },
   "outputs": [],
   "source": [
    "df_bpic14_train_dyn = df_bpic14_dyn.loc[df_bpic14_train[EVENTLOG_CASE].unique()]\n",
    "df_bpic14_train_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcpsO1G14N4x"
   },
   "outputs": [],
   "source": [
    "df_bpic14_test_dyn = df_bpic14_dyn.loc[df_bpic14_test[EVENTLOG_CASE].unique()]\n",
    "df_bpic14_test_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QN6OESm2uxzn"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_bpic14_train,\n",
    "    df_bpic14_test,\n",
    "    EVENTLOG_LABEL_NEXT_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03-ZimseuqPv"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_bpic14_train,\n",
    "    df_bpic14_test,\n",
    "    EVENTLOG_LABEL_REM_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2X5lDiwu3h4"
   },
   "outputs": [],
   "source": [
    "df_naive_classification_metrics(\n",
    "    df_bpic14_train,\n",
    "    df_bpic14_test,\n",
    "    EVENTLOG_LABEL_NEXT_ACT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeuZc-IBsCba"
   },
   "source": [
    "#### Create Graphs for BPIC 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53rRGCc-sCbh"
   },
   "source": [
    "##### Create Group Graphs for BPIC 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecIr5BqWsCbi"
   },
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, data = df_to_pyg_temporal_data(pd.concat([df_bpic14_train, df_bpic14_test], ignore_index=True), EVENTLOG_GROUP)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6jtDXXzsCbj"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "train_loader = pyg.loader.TemporalDataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "val_loader = pyg.loader.TemporalDataLoader(\n",
    "    val_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "test_loader = pyg.loader.TemporalDataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "\n",
    "neighbor_loader = pyg.nn.models.tgn.LastNeighborLoader(data.num_nodes, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-sVx3b7sCbj"
   },
   "outputs": [],
   "source": [
    "memory = pyg.nn.models.TGNMemory(\n",
    "    data.num_nodes,\n",
    "    data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=pyg.nn.models.tgn.IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=pyg.nn.models.tgn.LastAggregator(),\n",
    ")\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(set(memory.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=0.00001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9KOn8k4sCbk"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    val_ap, val_auc = test(val_loader)\n",
    "    test_ap, test_auc = test(test_loader)\n",
    "    print(f'Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"bpic14_group_mapping.json\"), \"w\") as f:\n",
    "  json.dump(mapping, f)\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"bpic14_group_inverse_mapping.json\"), \"w\") as f:\n",
    "  json.dump(reverse_mapping, f)\n",
    "\n",
    "embeddings = embed()\n",
    "np.save(os.path.join(OUTPUT_DATA_DIR, \"bpic14_group_embeddings.npy\"), embeddings, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLy-NNpOsCbk"
   },
   "outputs": [],
   "source": [
    "arr_nan = np.full(EMBEDDING_DIM, -99.0, dtype='float32')\n",
    "\n",
    "embeddings_df_train = df_bpic14_train_dyn[EVENTLOG_GROUP].map(lambda x: embeddings[int(reverse_mapping[x])] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "group_graph_embeddings_train = np.empty((embeddings_df_train.shape[0], embeddings_df_train.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_train)):\n",
    "  group_graph_embeddings_train[i] = np.stack(embeddings_df_train[i])\n",
    "\n",
    "group_graph_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ut7i6-KUsCbk"
   },
   "outputs": [],
   "source": [
    "arr_nan = np.full(EMBEDDING_DIM, -99.0, dtype='float32')\n",
    "\n",
    "embeddings_df_test = df_bpic14_test_dyn[EVENTLOG_GROUP].map(lambda x: embeddings[int(reverse_mapping[x])] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "group_graph_embeddings_test = np.empty((embeddings_df_test.shape[0], embeddings_df_test.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_test)):\n",
    "  group_graph_embeddings_test[i] = np.stack(embeddings_df_test[i])\n",
    "\n",
    "group_graph_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gn0G4EpfEmv4"
   },
   "source": [
    "#### Create Dataset from BPIC 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnjYWxNVFhos"
   },
   "outputs": [],
   "source": [
    "ds_bpic14_train = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_bpic14_train_dyn[\"concept:name\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_group\": df_bpic14_train_dyn[\"org:group\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_group_graph\": group_graph_embeddings_train,\n",
    "\n",
    "  \"time_timestamp_elapsedcycle\": df_bpic14_train_dyn[\"time:timestamp:elapsedcycle:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_bpic14_train_dyn[\"time:timestamp:elapsedprev:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_bpic14_train_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_bpic14_train_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_bpic14_train_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_bpic14_train_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_bpic14_train_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_bpic14_train_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_bpic14_train_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_bpic14_train_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_bpic14_train_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_bpic14_train_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "\n",
    "  \"case_km_number\": np.expand_dims(df_bpic14_train[\"case:KM number\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_type_aff\": np.expand_dims(df_bpic14_train[\"case:incident_CI Type (aff)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_subtype_aff\": np.expand_dims(df_bpic14_train[\"case:incident_CI Subtype (aff)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_service_component_aff\": np.expand_dims(df_bpic14_train[\"case:incident_Service Component WBS (aff)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_priority\": np.expand_dims(df_bpic14_train[\"case:incident_Priority\"].to_numpy(dtype='float32', na_value=-1), axis=-1),\n",
    "  \"case_incident_category\": np.expand_dims(df_bpic14_train[\"case:incident_Category\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_name_cby\": np.expand_dims(df_bpic14_train[\"case:incident_CI Name (CBy)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_type_cby\": np.expand_dims(df_bpic14_train[\"case:incident_CI Type (CBy)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_subtype_cby\": np.expand_dims(df_bpic14_train[\"case:incident_CI Subtype (CBy)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_interaction_priority\": np.expand_dims(df_bpic14_train[\"case:interaction_Priority\"].to_numpy(dtype='float32', na_value=-1), axis=-1),\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_bpic14_train[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_bpic14_train[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_bpic14_train[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "# Batch dataset to avoid OOM error\n",
    "ds_write_files(ds_bpic14_train.batch(128), os.path.join(OUTPUT_DATA_DIR, 'Detail_Incident_Activity_train_dataset'))\n",
    "\n",
    "ds_bpic14_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9NPKXiXKXWL"
   },
   "outputs": [],
   "source": [
    "ds_bpic14_test = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_bpic14_test_dyn[\"concept:name\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_group\": df_bpic14_test_dyn[\"org:group\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_group_graph\": group_graph_embeddings_test,\n",
    "\n",
    "  \"time_timestamp_elapsedcycle\": df_bpic14_test_dyn[\"time:timestamp:elapsedcycle:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_bpic14_test_dyn[\"time:timestamp:elapsedprev:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_bpic14_test_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_bpic14_test_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_bpic14_test_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_bpic14_test_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_bpic14_test_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_bpic14_test_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_bpic14_test_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_bpic14_test_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_bpic14_test_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_bpic14_test_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "\n",
    "  \"case_km_number\": np.expand_dims(df_bpic14_test[\"case:KM number\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_type_aff\": np.expand_dims(df_bpic14_test[\"case:incident_CI Type (aff)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_subtype_aff\": np.expand_dims(df_bpic14_test[\"case:incident_CI Subtype (aff)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_service_component_aff\": np.expand_dims(df_bpic14_test[\"case:incident_Service Component WBS (aff)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_priority\": np.expand_dims(df_bpic14_test[\"case:incident_Priority\"].to_numpy(dtype='float32', na_value=-1), axis=-1),\n",
    "  \"case_incident_category\": np.expand_dims(df_bpic14_test[\"case:incident_Category\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_name_cby\": np.expand_dims(df_bpic14_test[\"case:incident_CI Name (CBy)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_type_cby\": np.expand_dims(df_bpic14_test[\"case:incident_CI Type (CBy)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_incident_ci_subtype_cby\": np.expand_dims(df_bpic14_test[\"case:incident_CI Subtype (CBy)\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_interaction_priority\": np.expand_dims(df_bpic14_test[\"case:interaction_Priority\"].to_numpy(dtype='float32', na_value=-1), axis=-1),\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_bpic14_test[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_bpic14_test[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_bpic14_test[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_bpic14_test, os.path.join(OUTPUT_DATA_DIR, 'Detail_Incident_Activity_test_dataset'))\n",
    "\n",
    "ds_bpic14_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYcrP6TFjucY"
   },
   "source": [
    "## Dataset: Helpdesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB5VvTy3oE48"
   },
   "source": [
    "This data set contains a helpdesk event logs and is available here: https://data.mendeley.com/datasets/39bp3vv62t/1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfFzt-NirMwx"
   },
   "source": [
    "### Read Helpdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVB0X3VGju4d"
   },
   "outputs": [],
   "source": [
    "dtypes_helpdesk = {\n",
    "    'CaseID': 'string',\n",
    "    'ActivityID': 'string',\n",
    "}\n",
    "df_helpdesk = pd.read_csv(\n",
    "    os.path.join(INPUT_DATA_DIR, 'helpdesk.csv'),\n",
    "    header=0,\n",
    "    dtype=dtypes_helpdesk\n",
    ")\n",
    "\n",
    "df_drop_duplicate_rows(df_helpdesk)\n",
    "df_drop_na_rows_and_cols(df_helpdesk)\n",
    "df_drop_single_val_cols(df_helpdesk)\n",
    "\n",
    "# Convert timestamps\n",
    "df_helpdesk = df_convert_datetimes(\n",
    "    df_helpdesk,\n",
    "    [\"CompleteTimestamp\"],\n",
    "    yearfirst=True\n",
    ")\n",
    "# Sort and normalize column names\n",
    "df_format_as_eventlog(df_helpdesk, case_col=\"CaseID\", activity_col=\"ActivityID\", time_col=\"CompleteTimestamp\")\n",
    "\n",
    "# Write as Pandas CSV\n",
    "df_write_files(df_helpdesk, os.path.join(INTERIM_DATA_DIR, \"helpdesk_processed\"), skip_xes=False)\n",
    "\n",
    "print(df_helpdesk.dtypes)\n",
    "df_helpdesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgiN-SaR1zOx"
   },
   "source": [
    "### Label Helpdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzscWIES1yr4"
   },
   "outputs": [],
   "source": [
    "df_helpdesk = df_label_next_activity(df_helpdesk, eoc_token=TOKEN_EOC)\n",
    "df_helpdesk = df_label_activity_duration(df_helpdesk, unit='d', eoc_token=pd.Timedelta(0))\n",
    "df_helpdesk = df_label_remaining_cycle_time(df_helpdesk, unit='d')\n",
    "\n",
    "df_write_files(df_helpdesk, os.path.join(INTERIM_DATA_DIR, \"helpdesk_labeled\"))\n",
    "df_helpdesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHcivU7byGht"
   },
   "source": [
    "### Descriptive Statistics for Helpdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1m3UlfcyG_5"
   },
   "outputs": [],
   "source": [
    "df_stat = df_helpdesk.describe(include='all')\n",
    "df_write_files(df_stat, os.path.join(OUTPUT_DATA_DIR, \"helpdesk_describe\"), index=True)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtnP20ARyTGT"
   },
   "outputs": [],
   "source": [
    "df_helpdesk.hist(xrot=90, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"helpdesk_hist.svg\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ck3Jfpi43F6B"
   },
   "outputs": [],
   "source": [
    "df_case_length_stats(df_helpdesk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMMrw2os3J7S"
   },
   "outputs": [],
   "source": [
    "df_case_duration_stats(df_helpdesk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt4jjdgv_uuT"
   },
   "source": [
    "### Clean Helpdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWnLkuyzPrBJ"
   },
   "outputs": [],
   "source": [
    "df_helpdesk_clean = df_helpdesk.copy()\n",
    "\n",
    "# Create time features\n",
    "df_helpdesk_clean = df_extract_elapsed_cycle_time(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_CYCLE_SUFFIX}:seconds\", unit='s')\n",
    "df_helpdesk_clean = df_extract_activity_duration(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX}:seconds\", unit='s', na_token=0)\n",
    "\n",
    "df_helpdesk_clean = df_extract_month_of_year(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:month\", relative=True)\n",
    "df_helpdesk_clean = df_extract_day_of_year(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear\", relative=True)\n",
    "df_helpdesk_clean = df_extract_day_of_month(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:day\", relative=True)\n",
    "df_helpdesk_clean = df_extract_day_of_week(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:weekday\", relative=True)\n",
    "df_helpdesk_clean = df_extract_hour_of_day(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:hour\", relative=True)\n",
    "\n",
    "df_helpdesk_clean = df_extract_month_of_year(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:month:raw\")\n",
    "df_helpdesk_clean = df_extract_day_of_year(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear:raw\")\n",
    "df_helpdesk_clean = df_extract_day_of_month(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:day:raw\")\n",
    "df_helpdesk_clean = df_extract_day_of_week(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:weekday:raw\")\n",
    "df_helpdesk_clean = df_extract_hour_of_day(df_helpdesk_clean, f\"{EVENTLOG_TIMESTAMP}:hour:raw\")\n",
    "\n",
    "df_helpdesk_clean = df_filter_case_duration_range(df_helpdesk_clean, max=0.95)\n",
    "\n",
    "# Encode categorical labels\n",
    "df_helpdesk_clean = df_encode_label(df_helpdesk_clean, cols=EVENTLOG_LABEL_NEXT_ACT)\n",
    "\n",
    "# Transform bools to int\n",
    "df_helpdesk_clean = df_convert_bool_to_int(df_helpdesk_clean)\n",
    "\n",
    "# Transform ordered cats to int\n",
    "df_helpdesk_clean = df_convert_ordered_cat_to_int(df_helpdesk_clean, relative=True)\n",
    "\n",
    "# Fill empty str values\n",
    "df_helpdesk_clean = df_fillna_str(df_helpdesk_clean, TOKEN_NA)\n",
    "\n",
    "# Fill empty cat values\n",
    "df_helpdesk_clean = df_fillna_cat(df_helpdesk_clean, TOKEN_NA)\n",
    "\n",
    "df_write_files(df_helpdesk_clean, os.path.join(OUTPUT_DATA_DIR, \"helpdesk_cleaned\"))\n",
    "\n",
    "df_helpdesk_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcFhH7_irgbh"
   },
   "outputs": [],
   "source": [
    "df_helpdesk_train, df_helpdesk_test = df_strict_temporal_train_test_split(\n",
    "    df_helpdesk_clean,\n",
    "    0.2,\n",
    "    df_find_labels(df_helpdesk_clean),\n",
    "    debias_end=False\n",
    ")\n",
    "\n",
    "axes = df_visualize_strict_temporal_splitting(df_helpdesk_train, df_helpdesk_test)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"helpdesk_train_test.svg\"), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Remove overlaps\n",
    "df_helpdesk_test.dropna(axis='index', how='any', subset=df_find_labels(df_helpdesk_test), inplace=True)\n",
    "\n",
    "df_write_files(df_helpdesk_train, os.path.join(OUTPUT_DATA_DIR, \"helpdesk_train\"))\n",
    "df_write_files(df_helpdesk_test, os.path.join(OUTPUT_DATA_DIR, \"helpdesk_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "811uXpg63Pkv"
   },
   "outputs": [],
   "source": [
    "df_helpdesk_dyn = df_prefix_pad_attributes(\n",
    "    pd.concat([df_helpdesk_train, df_helpdesk_test]),\n",
    "    df_find_event_attributes(df_helpdesk_clean, exclude_labels=True)\n",
    ")\n",
    "\n",
    "df_helpdesk_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPvok6-f3bnp"
   },
   "outputs": [],
   "source": [
    "df_helpdesk_train_dyn = df_helpdesk_dyn.loc[df_helpdesk_train[EVENTLOG_CASE].unique()]\n",
    "df_helpdesk_train_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMJ7HRAo3cRg"
   },
   "outputs": [],
   "source": [
    "df_helpdesk_test_dyn = df_helpdesk_dyn.loc[df_helpdesk_test[EVENTLOG_CASE].unique()]\n",
    "df_helpdesk_test_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBTZ0o_aNgGK"
   },
   "outputs": [],
   "source": [
    "df_helpdesk_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv0P5OjHsulw"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_helpdesk_train,\n",
    "    df_helpdesk_test,\n",
    "    EVENTLOG_LABEL_NEXT_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC7oZ0MEsjyK"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_helpdesk_train,\n",
    "    df_helpdesk_test,\n",
    "    EVENTLOG_LABEL_REM_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBmsVFcBs3Vr"
   },
   "outputs": [],
   "source": [
    "df_naive_classification_metrics(\n",
    "    df_helpdesk_train,\n",
    "    df_helpdesk_test,\n",
    "    EVENTLOG_LABEL_NEXT_ACT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap3ZdNGcj_h8"
   },
   "source": [
    "### Create Dataset from Helpdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCdVGKFSuLcd"
   },
   "outputs": [],
   "source": [
    "ds_helpdesk_train = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_helpdesk_train_dyn[\"concept:name\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "\n",
    "  \"time_timestamp_elapsedcycle\": df_helpdesk_train_dyn[\"time:timestamp:elapsedcycle:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_helpdesk_train_dyn[\"time:timestamp:elapsedprev:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_helpdesk_train_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_helpdesk_train_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_helpdesk_train_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_helpdesk_train_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_helpdesk_train_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_helpdesk_train_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_helpdesk_train_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_helpdesk_train_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_helpdesk_train_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_helpdesk_train_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_helpdesk_train[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_helpdesk_train[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_helpdesk_train[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_helpdesk_train, os.path.join(OUTPUT_DATA_DIR, 'helpdesk_train_dataset'))\n",
    "\n",
    "ds_helpdesk_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUNDqyihuQK-"
   },
   "outputs": [],
   "source": [
    "ds_helpdesk_test = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_helpdesk_test_dyn[\"concept:name\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "\n",
    "  \"time_timestamp_elapsedcycle\": df_helpdesk_test_dyn[\"time:timestamp:elapsedcycle:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_helpdesk_test_dyn[\"time:timestamp:elapsedprev:seconds\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_helpdesk_test_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_helpdesk_test_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_helpdesk_test_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_helpdesk_test_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_helpdesk_test_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_helpdesk_test_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_helpdesk_test_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_helpdesk_test_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_helpdesk_test_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_helpdesk_test_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_helpdesk_test[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_helpdesk_test[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_helpdesk_test[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_helpdesk_test, os.path.join(OUTPUT_DATA_DIR, 'helpdesk_test_dataset'))\n",
    "\n",
    "ds_helpdesk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjBMR7v0jlM8"
   },
   "source": [
    "## Dataset: BPIC 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tr3FUEMHokcJ"
   },
   "source": [
    "This data set contains information about the IT service management processes at Volvo IT. See http://www.win.tue.nl/bpi/2013/challenge and https://data.4tu.nl/collections/BPI_Challenge_2013/5065448 for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsPCrN5YKvn5"
   },
   "source": [
    "### Incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGUu8RHcpGZO"
   },
   "source": [
    "**Dataset attributes**:\n",
    "- *Involved ST*: The actual team that will try to solve the incident\n",
    "- *Owner Country*: The country taking ownership of the incident\n",
    "- *Owner First Name*: The owning person's first name\n",
    "- *Involved Org line 3*: the business area of the user reporting the problem to the helpdesk\n",
    "- *Involved ST Function Div*: The IT organization is divided into functions (mostly technology wise)\n",
    "- *Status*: Current activity of the incident\n",
    "- *SR Latest Impact*: Impact is a measure of the business criticality of an Incident often equal to the extent to which an Incident leads to degradation of agreed service levels.\n",
    "- *Product*: Affected product\n",
    "- *Country*: Country as a code\n",
    "- *Sub Status*: Lifecycle transition of the incident\n",
    "- *SR Number*: The case id as the incident's identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgkk-dJ-rFhe"
   },
   "source": [
    "#### Read BPIC 2013 Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sZmwriMjlm-"
   },
   "outputs": [],
   "source": [
    "dtypes_bpic13 = {\n",
    "    \"Involved ST\": \"category\",\n",
    "    \"Owner Country\": \"category\",\n",
    "    \"Owner First Name\": \"category\",\n",
    "    \"Involved Org line 3\": \"category\",\n",
    "    \"Involved ST Function Div\": \"category\",\n",
    "    \"Status\": \"category\",\n",
    "    \"SR Latest Impact\": pd.CategoricalDtype(['Low', 'Medium', 'High', 'Major'], ordered=True),\n",
    "    \"Product\": \"category\",\n",
    "    \"Country\": \"category\",\n",
    "    \"Sub Status\": \"category\",\n",
    "    \"SR Number\": \"string\"\n",
    "}\n",
    "\n",
    "df_bpic13 = pd.read_csv(\n",
    "    os.path.join(INPUT_DATA_BPIC2013_DIR, \"VINST cases incidents.csv\"),\n",
    "    header=0,\n",
    "    sep=';',\n",
    "    encoding='cp1252',\n",
    "    dtype=dtypes_bpic13\n",
    ")\n",
    "\n",
    "\n",
    "df_bpic13 = df_convert_datetimes(\n",
    "    df_bpic13,\n",
    "    [\"Change Date+Time\"],\n",
    "    yearfirst=True\n",
    ")\n",
    "\n",
    "# Concat activities\n",
    "df_bpic13[EVENTLOG_ACTIVITY] = (df_bpic13[\"Status\"].astype('string') + \"_\" + df_bpic13[\"Sub Status\"].astype('string')).astype('string')\n",
    "\n",
    "df_drop_duplicate_rows(df_bpic13)\n",
    "df_drop_na_rows_and_cols(df_bpic13)\n",
    "df_drop_single_val_cols(df_bpic13)\n",
    "\n",
    "df_bpic13 = df_rename_cat_values(df_bpic13, 'Country', 'SE', 'se')\n",
    "df_bpic13 = df_rename_cat_values(df_bpic13, 'Owner First Name', 'Perjohan', 'Per-Johan')\n",
    "df_bpic13 = df_rename_cat_values(df_bpic13, 'Owner First Name', 'Jan Erik', 'Jan-Erik')\n",
    "\n",
    "df_format_as_eventlog(df_bpic13, case_col=\"SR Number\", time_col=\"Change Date+Time\", group_col=\"Involved ST\", role_col=\"Involved ST Function Div\", resource_col=\"Owner First Name\")\n",
    "\n",
    "# Write as Pandas CSV\n",
    "df_write_files(df_bpic13, os.path.join(INTERIM_DATA_DIR, \"BPI_Challenge_2013_incidents_processed\"))\n",
    "\n",
    "print(df_bpic13.dtypes)\n",
    "df_bpic13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz22Rc3epnAd"
   },
   "source": [
    "#### Label BPIC 2013 Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7U_OmAgLprWW"
   },
   "outputs": [],
   "source": [
    "df_bpic13 = df_label_next_activity(df_bpic13, eoc_token=TOKEN_EOC)\n",
    "df_bpic13 = df_label_activity_duration(df_bpic13, unit='d', eoc_token=pd.Timedelta(0))\n",
    "df_bpic13 = df_label_remaining_cycle_time(df_bpic13, unit='d')\n",
    "\n",
    "df_write_files(df_bpic13, os.path.join(INTERIM_DATA_DIR, \"BPI_Challenge_2013_incidents_labeled\"))\n",
    "df_bpic13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKezms6SIt47"
   },
   "source": [
    "#### Descriptive Statistics for BPIC 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9z_tyVsMk3Ub"
   },
   "outputs": [],
   "source": [
    "df_bpic13.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2H83IHh-IuV-"
   },
   "outputs": [],
   "source": [
    "df_stat = df_bpic13.describe(include='all')\n",
    "df_write_files(df_stat, os.path.join(OUTPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_describe\"), index=True)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0E5-Y80PZbB"
   },
   "outputs": [],
   "source": [
    "df_bpic13.hist(xrot=90, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"BPI_Challenge_2013_incidents_hist.svg\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LEuJcUV3crC"
   },
   "outputs": [],
   "source": [
    "df_case_length_stats(df_bpic13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fA1Ona4P34HQ"
   },
   "outputs": [],
   "source": [
    "df_case_duration_stats(df_bpic13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNXO7xX5KyGY"
   },
   "source": [
    "#### Clean BPIC 2013 Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NID8gH_HPGnw"
   },
   "outputs": [],
   "source": [
    "df_bpic13_clean = df_bpic13.copy()\n",
    "\n",
    "# Filter event log\n",
    "df_bpic13_clean = df_filter_case_duration_range(df_bpic13_clean, max=0.95)\n",
    "df_bpic13_clean = df_filter_date_range(df_bpic13_clean, min=\"2012-04-01 00:00:00\", mode='traces_included')\n",
    "\n",
    "# Remove mostly empty columns\n",
    "df_drop_threshold_na_cols(df_bpic13_clean, 0.95)\n",
    "\n",
    "# Create time features\n",
    "df_bpic13_clean = df_extract_elapsed_cycle_time(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_CYCLE_SUFFIX}\", unit='s')\n",
    "df_bpic13_clean = df_extract_activity_duration(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}{EVENTLOG_FEAT_TIME_ELAPSED_PREV_SUFFIX}\", unit='s', na_token=0)\n",
    "\n",
    "df_bpic13_clean = df_extract_month_of_year(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:month\", relative=True)\n",
    "df_bpic13_clean = df_extract_day_of_year(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear\", relative=True)\n",
    "df_bpic13_clean = df_extract_day_of_month(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:day\", relative=True)\n",
    "df_bpic13_clean = df_extract_day_of_week(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:weekday\", relative=True)\n",
    "df_bpic13_clean = df_extract_hour_of_day(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:hour\", relative=True)\n",
    "\n",
    "df_bpic13_clean = df_extract_month_of_year(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:month:raw\")\n",
    "df_bpic13_clean = df_extract_day_of_year(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:dayofyear:raw\")\n",
    "df_bpic13_clean = df_extract_day_of_month(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:day:raw\")\n",
    "df_bpic13_clean = df_extract_day_of_week(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:weekday:raw\")\n",
    "df_bpic13_clean = df_extract_hour_of_day(df_bpic13_clean, f\"{EVENTLOG_TIMESTAMP}:hour:raw\")\n",
    "\n",
    "# Encode categorical labels\n",
    "df_bpic13_clean = df_encode_label(df_bpic13_clean, cols=EVENTLOG_LABEL_NEXT_ACT)\n",
    "\n",
    "# Transform bools to int\n",
    "df_bpic13_clean = df_convert_bool_to_int(df_bpic13_clean)\n",
    "\n",
    "# Transform ordered cats to int\n",
    "df_bpic13_clean = df_convert_ordered_cat_to_int(df_bpic13_clean, relative=True)\n",
    "\n",
    "# Fill empty str values\n",
    "df_bpic13_clean = df_fillna_str(df_bpic13_clean, TOKEN_NA)\n",
    "\n",
    "# Fill empty cat values\n",
    "df_bpic13_clean = df_fillna_cat(df_bpic13_clean, TOKEN_NA)\n",
    "\n",
    "df_write_files(df_bpic13_clean, os.path.join(OUTPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_cleaned\"))\n",
    "\n",
    "df_bpic13_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xY3A1ggP-DE"
   },
   "outputs": [],
   "source": [
    "df_bpic13_train, df_bpic13_test = df_strict_temporal_train_test_split(\n",
    "    df_bpic13_clean,\n",
    "    0.2,\n",
    "    df_find_labels(df_bpic13_clean),\n",
    "    debias_end=False\n",
    ")\n",
    "\n",
    "axes = df_visualize_strict_temporal_splitting(df_bpic13_train, df_bpic13_test)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(GRAPHIC_DIR, \"BPI_Challenge_2013_incidents_train_test.svg\"), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Remove overlaps\n",
    "df_bpic13_test.dropna(axis='index', how='any', subset=df_find_labels(df_bpic13_test), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LHdmr_gAWQl"
   },
   "outputs": [],
   "source": [
    "df_bpic13_train, df_bpic13_test = df_temporal_train_test_split(df_bpic13_clean, 0.25, split_mode='case_start', filter_mode='traces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHvSFjR_rHpH"
   },
   "outputs": [],
   "source": [
    "df_write_files(df_bpic13_train, os.path.join(OUTPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_train\"))\n",
    "df_bpic13_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ip_gmSthrI9Z"
   },
   "outputs": [],
   "source": [
    "df_write_files(df_bpic13_test, os.path.join(OUTPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_test\"))\n",
    "df_bpic13_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQVKrT7cASFo"
   },
   "outputs": [],
   "source": [
    "df_bpic13_dyn = df_prefix_pad_attributes(\n",
    "    pd.concat([df_bpic13_train, df_bpic13_test]),\n",
    "    df_find_event_attributes(df_bpic13_clean, exclude_labels=True)\n",
    ")\n",
    "\n",
    "df_bpic13_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WA7zO-TiHbK_"
   },
   "outputs": [],
   "source": [
    "df_bpic13_train_dyn = df_bpic13_dyn.loc[df_bpic13_train[EVENTLOG_CASE].unique()]\n",
    "df_bpic13_train_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jRbSvR_Hsol"
   },
   "outputs": [],
   "source": [
    "df_bpic13_test_dyn = df_bpic13_dyn.loc[df_bpic13_test[EVENTLOG_CASE].unique()]\n",
    "df_bpic13_test_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsWbJRaCIOBo"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_bpic13_train,\n",
    "    df_bpic13_test,\n",
    "    EVENTLOG_LABEL_NEXT_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GdeN2tNIOBp"
   },
   "outputs": [],
   "source": [
    "df_naive_regression_metrics(\n",
    "    df_bpic13_train,\n",
    "    df_bpic13_test,\n",
    "    EVENTLOG_LABEL_REM_TIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Oi8rgTWIOBp"
   },
   "outputs": [],
   "source": [
    "df_naive_classification_metrics(\n",
    "    df_bpic13_train,\n",
    "    df_bpic13_test,\n",
    "    EVENTLOG_LABEL_NEXT_ACT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vGCZV4pQf7H"
   },
   "source": [
    "#### Create Graphs for BPIC 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBhmqq_8Qf7J"
   },
   "source": [
    "##### Create Resource Graphs for BPIC 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wRIWP48Qf7J"
   },
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, data = df_to_pyg_temporal_data(pd.concat([df_bpic13_train, df_bpic13_test], ignore_index=True), EVENTLOG_RESOURCE)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-LW_U6YQf7J"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "train_loader = pyg.loader.TemporalDataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "val_loader = pyg.loader.TemporalDataLoader(\n",
    "    val_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "test_loader = pyg.loader.TemporalDataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "\n",
    "neighbor_loader = pyg.nn.models.tgn.LastNeighborLoader(data.num_nodes, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7o1YOzQQf7K"
   },
   "outputs": [],
   "source": [
    "memory = pyg.nn.models.TGNMemory(\n",
    "    data.num_nodes,\n",
    "    data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=pyg.nn.models.tgn.IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=pyg.nn.models.tgn.LastAggregator(),\n",
    ")\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(set(memory.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=0.00001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjGiX4PCQf7L"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    val_ap, val_auc = test(val_loader)\n",
    "    test_ap, test_auc = test(test_loader)\n",
    "    print(f'Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"bpic13_resource_mapping.json\"), \"w\") as f:\n",
    "  json.dump(mapping, f)\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"bpic13_resource_inverse_mapping.json\"), \"w\") as f:\n",
    "  json.dump(reverse_mapping, f)\n",
    "\n",
    "embeddings = embed()\n",
    "np.save(os.path.join(OUTPUT_DATA_DIR, \"bpic13_resource_embeddings.npy\"), embeddings, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8Kl8aFAQf7L"
   },
   "outputs": [],
   "source": [
    "arr_nan = np.full(EMBEDDING_DIM, -99.0, dtype='float32')\n",
    "\n",
    "embeddings_df_train = df_bpic13_train_dyn[EVENTLOG_RESOURCE].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "resource_graph_embeddings_train = np.empty((embeddings_df_train.shape[0], embeddings_df_train.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_train)):\n",
    "  resource_graph_embeddings_train[i] = np.stack(embeddings_df_train[i])\n",
    "\n",
    "resource_graph_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3_Kd4JUQf7M"
   },
   "outputs": [],
   "source": [
    "embeddings_df_test = df_bpic13_test_dyn[EVENTLOG_RESOURCE].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "resource_graph_embeddings_test = np.empty((embeddings_df_test.shape[0], embeddings_df_test.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_test)):\n",
    "  resource_graph_embeddings_test[i] = np.stack(embeddings_df_test[i])\n",
    "\n",
    "resource_graph_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZmPvgV_Qf7M"
   },
   "source": [
    "##### Create Group Graphs for BPIC 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WE6soopQf7N"
   },
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, data = df_to_pyg_temporal_data(pd.concat([df_bpic13_train, df_bpic13_test], ignore_index=True), EVENTLOG_GROUP)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKanylyAQf7N"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "train_loader = pyg.loader.TemporalDataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "val_loader = pyg.loader.TemporalDataLoader(\n",
    "    val_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "test_loader = pyg.loader.TemporalDataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "\n",
    "neighbor_loader = pyg.nn.models.tgn.LastNeighborLoader(data.num_nodes, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13KDf1DzQf7O"
   },
   "outputs": [],
   "source": [
    "memory = pyg.nn.models.TGNMemory(\n",
    "    data.num_nodes,\n",
    "    data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=pyg.nn.models.tgn.IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=pyg.nn.models.tgn.LastAggregator(),\n",
    ")\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(set(memory.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=0.00001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7cIu8ldQf7O"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    val_ap, val_auc = test(val_loader)\n",
    "    test_ap, test_auc = test(test_loader)\n",
    "    print(f'Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"bpic13_group_mapping.json\"), \"w\") as f:\n",
    "  json.dump(mapping, f)\n",
    "with open(os.path.join(OUTPUT_DATA_DIR, \"bpic13_group_inverse_mapping.json\"), \"w\") as f:\n",
    "  json.dump(reverse_mapping, f)\n",
    "\n",
    "embeddings = embed()\n",
    "np.save(os.path.join(OUTPUT_DATA_DIR, \"bpic13_group_embeddings.npy\"), embeddings, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ques06NuQf7P"
   },
   "outputs": [],
   "source": [
    "arr_nan = np.full(EMBEDDING_DIM, -99.0, dtype='float32')\n",
    "\n",
    "embeddings_df_train = df_bpic13_train_dyn[EVENTLOG_GROUP].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "group_graph_embeddings_train = np.empty((embeddings_df_train.shape[0], embeddings_df_train.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_train)):\n",
    "  group_graph_embeddings_train[i] = np.stack(embeddings_df_train[i])\n",
    "\n",
    "group_graph_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "La7uLVJSQf7P"
   },
   "outputs": [],
   "source": [
    "embeddings_df_test = df_bpic13_test_dyn[EVENTLOG_GROUP].map(lambda x: embeddings[reverse_mapping[x]] if x is not None and not pd.isna(x) else arr_nan, na_action=None).to_numpy()\n",
    "group_graph_embeddings_test = np.empty((embeddings_df_test.shape[0], embeddings_df_test.shape[1], EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(len(embeddings_df_test)):\n",
    "  group_graph_embeddings_test[i] = np.stack(embeddings_df_test[i])\n",
    "\n",
    "group_graph_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RYuJhMUIxEH"
   },
   "source": [
    "#### Create Dataset from BPIC 2013 Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpTdc-kVIwjy"
   },
   "outputs": [],
   "source": [
    "ds_bpic13_train = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_bpic13_train_dyn[\"concept:name\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"status\": df_bpic13_train_dyn[\"Status\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"sub_status\": df_bpic13_train_dyn[\"Sub Status\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_role\": df_bpic13_train_dyn[\"org:role\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_line\": df_bpic13_train_dyn[\"Involved Org line 3\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_group\": df_bpic13_train_dyn[\"org:group\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"owner_country\": df_bpic13_train_dyn[\"Owner Country\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource\": df_bpic13_train_dyn[\"org:resource\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource_graph\": resource_graph_embeddings_train,\n",
    "  \"org_group_graph\": group_graph_embeddings_train,\n",
    "\n",
    "  \"time_timestamp_elapsedcycle\": df_bpic13_train_dyn[\"time:timestamp:elapsedcycle\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_bpic13_train_dyn[\"time:timestamp:elapsedprev\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_bpic13_train_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_bpic13_train_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_bpic13_train_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_bpic13_train_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_bpic13_train_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_bpic13_train_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_bpic13_train_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_bpic13_train_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_bpic13_train_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_bpic13_train_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "\n",
    "  \"case_product\": np.expand_dims(df_bpic13_train[\"case:Product\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_country\": np.expand_dims(df_bpic13_train[\"case:Country\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_latest_impact\": np.expand_dims(df_bpic13_train[\"case:SR Latest Impact\"].to_numpy(dtype='float32', na_value=-1), axis=-1),\n",
    "\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_bpic13_train[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_bpic13_train[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_bpic13_train[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_bpic13_train, os.path.join(OUTPUT_DATA_DIR, 'BPI_Challenge_2013_incidents_train_dataset'))\n",
    "\n",
    "print(len(ds_bpic13_train))\n",
    "ds_bpic13_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smlpiDADL3aW"
   },
   "outputs": [],
   "source": [
    "ds_bpic13_test = tf.data.Dataset.from_tensor_slices(({\n",
    "  \"activity\": df_bpic13_test_dyn[\"concept:name\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"status\": df_bpic13_test_dyn[\"Status\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"sub_status\": df_bpic13_test_dyn[\"Sub Status\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_role\": df_bpic13_test_dyn[\"org:role\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_line\": df_bpic13_test_dyn[\"Involved Org line 3\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_group\": df_bpic13_test_dyn[\"org:group\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"owner_country\": df_bpic13_test_dyn[\"Owner Country\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource\": df_bpic13_test_dyn[\"org:resource\"].to_numpy(na_value=TOKEN_PADDING),\n",
    "  \"org_resource_graph\": resource_graph_embeddings_test,\n",
    "  \"org_group_graph\": group_graph_embeddings_test,\n",
    "\n",
    "  \"time_timestamp_elapsedcycle\": df_bpic13_test_dyn[\"time:timestamp:elapsedcycle\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_elapsedprev\": df_bpic13_test_dyn[\"time:timestamp:elapsedprev\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month\": df_bpic13_test_dyn[\"time:timestamp:month\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear\": df_bpic13_test_dyn[\"time:timestamp:dayofyear\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day\": df_bpic13_test_dyn[\"time:timestamp:day\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday\": df_bpic13_test_dyn[\"time:timestamp:weekday\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour\": df_bpic13_test_dyn[\"time:timestamp:hour\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_month_raw\": df_bpic13_test_dyn[\"time:timestamp:month:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_dayofyear_raw\": df_bpic13_test_dyn[\"time:timestamp:dayofyear:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_day_raw\": df_bpic13_test_dyn[\"time:timestamp:day:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_weekday_raw\": df_bpic13_test_dyn[\"time:timestamp:weekday:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "  \"time_timestamp_hour_raw\": df_bpic13_test_dyn[\"time:timestamp:hour:raw\"].to_numpy(dtype='float32', na_value=-1),\n",
    "\n",
    "  \"case_product\": np.expand_dims(df_bpic13_test[\"case:Product\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_country\": np.expand_dims(df_bpic13_test[\"case:Country\"].to_numpy(na_value=TOKEN_PADDING), axis=-1),\n",
    "  \"case_latest_impact\": np.expand_dims(df_bpic13_test[\"case:SR Latest Impact\"].to_numpy(dtype='float32', na_value=-1), axis=-1),\n",
    "\n",
    "}, {\n",
    "  \"next_activity\": np.expand_dims(df_bpic13_test[\"label:concept:name:next\"].to_numpy(dtype='int16'), axis=-1),\n",
    "  \"next_time\": np.expand_dims(df_bpic13_test[\"label:time:timestamp:next\"].to_numpy(dtype='float32'), axis=-1),\n",
    "  \"remaining_time\": np.expand_dims(df_bpic13_test[\"label:time:timestamp:last\"].to_numpy(dtype='float32'), axis=-1),\n",
    "}))\n",
    "\n",
    "ds_write_files(ds_bpic13_test, os.path.join(OUTPUT_DATA_DIR, 'BPI_Challenge_2013_incidents_test_dataset'))\n",
    "\n",
    "print(len(ds_bpic13_test))\n",
    "ds_bpic13_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEnsomiJlYF_"
   },
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbnZaIXvlpIp"
   },
   "outputs": [],
   "source": [
    "output_file = f\"results_{datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S%z')}.zip\"\n",
    "\n",
    "!zip -r \"$output_file\" \"$DATA_DIR\" \"$GRAPHIC_DIR\" \"$MODEL_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jT5qK1yAleCQ"
   },
   "source": [
    "## A: Export to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeUyaAkGmCts"
   },
   "outputs": [],
   "source": [
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "Path(GDRIVE_OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "!cp \"$output_file\" \"$GDRIVE_OUTPUT_DIR\"\n",
    "\n",
    "drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6l3vuhklh7h"
   },
   "source": [
    "## B: Download to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBY3BvZTl3_0"
   },
   "outputs": [],
   "source": [
    "files.download(output_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
