{"cells":[{"cell_type":"markdown","metadata":{"id":"4Rws3ICU-8ZJ"},"source":["# Model Development\n","\n","This Notebook contains all information pertaining to the development of the models in the paper \"Leveraging Temporal Graphs for Enhancing Transformer-based Predictive Process Monitoring\" and was developed by Marc C. Hennig (mhennig@hm.edu). Requires the preprocessed event logs in the first file."]},{"cell_type":"markdown","metadata":{"id":"TQbWuHdWhhbN"},"source":["# Environment\n","\n","Initialization of the environment runnable in Google Colab and contianing all dependency installations and global variables used in the paper."]},{"cell_type":"markdown","metadata":{"id":"LKQT5Dhuhni0"},"source":["## Dependency installation"]},{"cell_type":"markdown","metadata":{"id":"RwEddQ2GrlRy"},"source":["### A. PIP Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0luyK5STpuf"},"outputs":[],"source":["!pip install ipdb keras==3.3.3 numpy==1.26.4 keras_nlp keras-tuner tensorboard-plugin-profile\n","!pip freeze > requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"HBG__Y3whra8"},"source":["## Dependency Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3h6Sq8RoTiLC"},"outputs":[],"source":["# Python dependencies\n","import os\n","import sys\n","import re\n","import math\n","import datetime\n","import random\n","import json\n","import time\n","import shutil\n","import warnings\n","import functools\n","from pathlib import Path\n","from typing import List, Tuple, Union, Optional, Literal, Callable, Dict\n","from collections import namedtuple\n","from enum import Enum\n","\n","# Debugging\n","import ipdb\n","import importlib\n","\n","# Colab dependencies\n","from google.colab import files, drive\n","\n","# Basic dependencies\n","import numpy as np\n","import pandas as pd\n","import scipy as sp\n","import statsmodels as sm\n","\n","# Plotting dependencies\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","# Machine learning depenencies\n","import sklearn as sl\n","import sklearn.metrics\n","\n","# Neural network dependencies\n","%load_ext tensorboard\n","\n","import tensorflow as tf\n","from tensorflow.data import Dataset\n","\n","import keras\n","import keras_nlp\n","import keras_tuner\n","\n","from keras import backend as K"]},{"cell_type":"markdown","metadata":{"id":"jSaBMrqDhuKF"},"source":["## Variables & Global Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7Tg7Nrrh1zG"},"outputs":[],"source":["# Assign a random seed for reproduceability\n","RANDOM_STATE = 1337\n","\n","os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n","random.seed(RANDOM_STATE)\n","np.random.seed(RANDOM_STATE)\n","\n","# Keras backend\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","# Show all Pandas columns\n","pd.set_option(\"display.max_columns\", None)\n","\n","# Set Matplotlib and Seaborn color scheme\n","plt.rcParams[\"image.cmap\"] = \"Blues\"\n","sns.set_palette(\"Blues\")\n","\n","# Masking is handled manually\n","warnings.filterwarnings(\"ignore\", message=r\"(absl:)?Layer '\\w+' \\(of type \\w+\\) was passed an input with a mask attached to it\\. However, this layer does not support masking and will therefore destroy the mask information\\. Downstream layers will not see the mask\\.\")\n","warnings.filterwarnings(\"ignore\", message=r\"(absl:)?You are explicitly setting `\\w+` while the `\\w+` have built-in mask, so the built-in mask is ignored\\.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eh37EJKuirTG"},"outputs":[],"source":["# Google Drive folders\n","GDRIVE_INPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/TGN-AST/Eventlogs\"\n","GDRIVE_OUTPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/TGN-AST/Results\"\n","\n","# Local Colab folders\n","UTIL_DIR = os.path.join(\".\", \"Util\")\n","DATA_DIR = os.path.join(\".\", \"Data\")\n","INPUT_DATA_DIR = os.path.join(DATA_DIR, \"Input\")\n","INPUT_DATA_BPIC2013_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2013\")\n","INPUT_DATA_BPIC2014_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2014\")\n","INTERIM_DATA_DIR = os.path.join(DATA_DIR, \"Interim\")\n","OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"Output\")\n","OUTPUT_LOG_DATA_DIR = os.path.join(OUTPUT_DATA_DIR, \"Logs\")\n","\n","GRAPHIC_DIR = os.path.join(\".\", \"Graphics\")\n","MODEL_DIR = os.path.join(\".\", \"Models\")\n","MODEL_CHECKPOINT_DIR = os.path.join(MODEL_DIR, \"Checkpoints\")\n","MODEL_BACKUP_DIR = os.path.join(MODEL_DIR, \"Backups\")\n","\n","Path(DATA_DIR).mkdir(exist_ok=True)\n","Path(INTERIM_DATA_DIR).mkdir(exist_ok=True)\n","Path(OUTPUT_DATA_DIR).mkdir(exist_ok=True)\n","Path(OUTPUT_LOG_DATA_DIR).mkdir(exist_ok=True)\n","Path(GRAPHIC_DIR).mkdir(exist_ok=True)\n","Path(MODEL_DIR).mkdir(exist_ok=True)\n","Path(MODEL_BACKUP_DIR).mkdir(exist_ok=True)\n","Path(MODEL_CHECKPOINT_DIR).mkdir(exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEgmgkls_kzl"},"outputs":[],"source":["EVENTLOG_CASE = \"case:concept:name\"\n","EVENTLOG_ACTIVITY = \"concept:name\"\n","EVENTLOG_TIMESTAMP = \"time:timestamp\"\n","EVENTLOG_GROUP = \"org:group\"\n","EVENTLOG_RESOURCE = \"org:resource\"\n","EVENTLOG_CASE_PREFIX = \"case:\"\n","EVENTLOG_LABEL_PREFIX = \"label:\"\n","\n","TOKEN_PAD = \"[PAD]\"\n","TOKEN_PAD_NUM = -1.0\n","TOKEN_OOV = \"[OOV]\"\n","TOKEN_NA = \"[NA]\"\n","TOKEN_EOC = \"[EOC]\"\n","\n","DEFAULT_LEARNING_RATE = 0.00003\n","DEFAULT_MIN_LEARNING_RATE = 1e-6\n","\n","DEFAULT_WARMUP_EPOCHS = 10\n","DEFAULT_EPOCHS = 200\n","DEFAULT_BATCH_SIZE = 64\n","\n","TARGET_NEXT_ACTIVITY = \"next_activity\"\n","TARGET_NEXT_TIME = \"next_time\"\n","TARGET_REMAINING_TIME = \"remaining_time\"\n","\n","DEFAULT_REMAINING_TIME_OUTPUT = \"remaining_time\"\n","DEFAULT_NEXT_ACTIVITY_OUTPUT = \"next_activity\"\n","DEFAULT_NEXT_TIME_OUTPUT = \"next_time\""]},{"cell_type":"markdown","metadata":{"id":"r3AIDPxZlS-v"},"source":["## Data Import"]},{"cell_type":"markdown","metadata":{"id":"XAPgp73mjlFP"},"source":["### A: Import from Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4F4xUcGjkc0"},"outputs":[],"source":["drive.mount(\"/content/drive\")\n","\n","!cp -r \"$GDRIVE_INPUT_DIR\" \"$INPUT_DATA_DIR\"\n","\n","drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"FzhkKQsVj11R"},"source":["### B: Upload from Local Machine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHaQpf2gjxzL"},"outputs":[],"source":["#uploaded = files.upload()\n","\n","#for filename in uploaded.keys():\n","#  target = os.path.join(INPUT_DATA_DIR, filename)\n","#  !mv \"$filename\" \"$target\"\n","\n","#del uploaded"]},{"cell_type":"markdown","metadata":{"id":"mjH_DYlyUYgo"},"source":["## Common Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQVkTJTpfzeL"},"outputs":[],"source":["def df_write_files(df: pd.DataFrame, filename: str, index: bool = False) -> None:\n","  df.to_csv(f\"{filename}.csv\", index=index)\n","  df.to_pickle(f\"{filename}.pkl.gz\")\n","  try:\n","    df.reset_index().to_feather(f\"{filename}.feather\")\n","  except Exception as e:\n","    print(f\"Skipping feather: {e}\")\n","  try:\n","    df.to_parquet(f\"{filename}.parquet\", index=index)\n","  except Exception as e:\n","    print(f\"Skipping parquet: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"mzktkdae5f2O"},"source":["### Dataset Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEpnKtOo_mf_"},"outputs":[],"source":["def ds_write_files(ds: tf.data.Dataset, filename: str, compression: Optional[str] = 'zip') -> None:\n","  ds.save(filename, compression='GZIP')\n","  shutil.make_archive(\n","      filename,\n","      format=compression,\n","      root_dir=filename\n","  )\n","\n","def ds_find_static_attrs(ds: tf.data.Dataset) -> List[str]:\n","  input, _ = ds.element_spec\n","\n","  stat_attrs = []\n","  for key, value in input.items():\n","    if 1 == value.shape.num_elements():\n","      stat_attrs.append(key)\n","\n","  return stat_attrs\n","\n","def ds_find_dynamic_attrs(ds: tf.data.Dataset) -> List[str]:\n","  input, _ = ds.element_spec\n","\n","  dyn_attrs = []\n","  for key, value in input.items():\n","    if 1 < value.shape.num_elements():\n","      dyn_attrs.append(key)\n","\n","  return dyn_attrs\n","\n","def ds_find_max_seq_len(ds: tf.data.Dataset) -> int:\n","  input, _ = ds.element_spec\n","\n","  dyn_attrs = ds_find_dynamic_attrs(ds)\n","  dyn_attr_lens = set()\n","  for dyn_attr in dyn_attrs:\n","    dyn_attr_lens.add(input[dyn_attr].shape[0])\n","\n","  if len(dyn_attr_lens) > 1:\n","    raise ValueError(f\"Different sequence lengths for different attributes {dyn_attr_lens}.\")\n","\n","  return dyn_attr_lens.pop()\n","\n","def ds_calculate_class_weights(ds: tf.data.Dataset, y_key: str = TARGET_NEXT_ACTIVITY) -> dict:\n","  y_train = np.concatenate([y[y_key] for x, y in ds], axis=0).flatten()\n","\n","  class_weights = sl.utils.class_weight.compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n","  )\n","  return dict(enumerate(class_weights))\n","\n","def ds_rename_attr(ds: tf.data.Dataset, old_attr: str, new_attr: str, copy: bool = False) -> tf.data.Dataset:\n","\n","  def ds_rename_attr_map_helper(inputs, outputs):\n","    inputs[new_attr] = inputs[old_attr]\n","    if not copy:\n","      inputs.pop(old_attr)\n","\n","    return inputs, outputs\n","\n","  return ds.map(ds_rename_attr_map_helper, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","def ds_desequentialize_dynamic_attrs(ds: tf.data.Dataset, dynamic_attrs: Optional[Union[List[str], str]] = None, strategy: Literal['last', 'first', 'mean', 'median'] = 'last', activity_attr: str = \"activity\", pad_token: str = TOKEN_PAD) -> tf.data.Dataset:\n","  if dynamic_attrs is None:\n","    dynamic_attrs = ds_find_dynamic_attrs(ds)\n","  elif isinstance(dynamic_attrs, str):\n","    dynamic_attrs = [dynamic_attrs]\n","\n","\n","  def ds_desequentialize_map_helper(inputs, outputs):\n","    # Find the current sequence length by looking for the padding token in the 'activity' attribute\n","    seq_len = tf.argmax(tf.equal(inputs[activity_attr], pad_token), axis=-1)\n","    for attr in dynamic_attrs:\n","      if 'last' == strategy:\n","        inputs[attr] = [inputs[attr][seq_len - 1]]\n","      elif 'first' == strategy:\n","        inputs[attr] = [inputs[attr][0]]\n","      elif 'mean' == strategy:\n","        inputs[attr] = [tf.reduce_mean(inputs[attr][:seq_len], axis=0)]\n","      elif 'median' == strategy:\n","        inputs[attr] = [tfp.stats.percentile(inputs[attr][:seq_len], 50)]\n","      else:\n","        raise ValueError(f\"Unknown strategy {strategy}\")\n","\n","    return inputs, outputs\n","\n","  return ds.map(ds_desequentialize_map_helper, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","def ds_sequentialize_static_attrs(ds: tf.data.Dataset, static_attrs: Optional[Union[List[str], str]] = None, activity_attr: str = \"activity\", pad_token: str = TOKEN_PAD, pad_token_num = int(TOKEN_PAD_NUM)) -> tf.data.Dataset:\n","  if static_attrs is None:\n","    static_attrs = ds_find_static_attrs(ds)\n","  elif isinstance(static_attrs, str):\n","    static_attrs = [static_attrs]\n","\n","  max_seq_len = ds_find_max_seq_len(ds)\n","\n","  def ds_sequentialize_map_helper(inputs, outputs):\n","    # Find the current sequence length by looking for the padding token in the 'activity' attribute\n","    # Assumes 'activity' is a dynamic attribute holding the sequence\n","    seq_len = tf.argmax(tf.equal(inputs[activity_attr], pad_token), axis=-1)\n","\n","    for attr in static_attrs:\n","      # Repeat static attribute for the sequence length and pad to the max sequence length\n","      attr_val_repeated = tf.repeat(inputs[attr], repeats=seq_len, axis=0)\n","      paddings = [[0, max_seq_len - seq_len]]\n","      pad_const = pad_token_num if attr_val_repeated.dtype.is_numeric else pad_token\n","      attr_val_padded = tf.pad(attr_val_repeated, paddings, 'CONSTANT', constant_values=pad_const)\n","      inputs[attr] = attr_val_padded\n","      inputs[attr].set_shape((seq_len,))\n","\n","    return inputs, outputs\n","\n","  return ds.map(ds_sequentialize_map_helper, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","def ds_window_dynamic_attrs(ds: tf.data.Dataset, window: int, activity_attr: str = \"activity\", pad_token: str = TOKEN_PAD, pad_token_num = int(TOKEN_PAD_NUM)) -> tf.data.Dataset:\n","  max_seq_len = ds_find_max_seq_len(ds)\n","  dyn_attrs = ds_find_dynamic_attrs(ds)\n","\n","  if max_seq_len <= window:\n","    return ds\n","\n","  def ds_window_map_helper(inputs, outputs):\n","    # Find the current sequence length by looking for the padding token in the 'activity' attribute. Assumes 'activity' is a dynamic attribute holding the sequence and all dynamic attributes are of equal length\n","    seq_len = tf.argmax(tf.equal(inputs[activity_attr], pad_token), axis=-1)\n","    begin = tf.math.maximum(tf.zeros((), tf.int64), seq_len - window)\n","    end = begin + window\n","\n","    for attr in dyn_attrs:\n","      inputs[attr] = inputs[attr][begin:end]\n","      shape = tuple([window] + inputs[attr].shape.as_list()[1:])\n","      inputs[attr].set_shape(shape)\n","\n","    return inputs, outputs\n","\n","  return ds.map(ds_window_map_helper, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","def ds_to_single_target(ds: tf.data.Dataset, target_attr: str) -> tf.data.Dataset:\n","  return ds.map(lambda inputs, outputs: (inputs, outputs[target_attr]), num_parallel_calls=tf.data.AUTOTUNE)\n","\n","def ds_find_target_classes(ds: tf.data.Dataset, target_attr: str) -> List[str]:\n","  return np.unique(np.concatenate([y[target_attr] for x, y in ds], axis=0).flatten())\n","\n","def ds_sparse_target_to_onehot(ds: tf.data.Dataset, target_attr: str) -> tf.data.Dataset:\n","  num_classes = len(ds_find_target_classes(ds, target_attr))\n","  return ds.map(lambda inputs, outputs: (inputs, keras.utils.to_categorical(outputs[target_attr], num_classes=num_classes)), num_parallel_calls=tf.data.AUTOTUNE)\n","\n","def ds_cache_and_batch(ds: tf.data.Dataset, batch_size: int = DEFAULT_BATCH_SIZE, cache_file: Optional[str] = None, shuffle: bool = False) -> tf.data.Dataset:\n","  if cache_file is not None:\n","    ds = ds.cache(filename=cache_file)\n","  else:\n","    ds = ds.cache()\n","\n","  # Build the cache\n","  _ = list(ds.as_numpy_iterator())\n","\n","  if shuffle:\n","    ds = ds.shuffle(len(ds))\n","\n","  return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","def ds_filter_length(ds: tf.data.Dataset, min: int = 0, max: int = float('inf'), activity_attr: str = \"activity\", pad_token: str = TOKEN_PAD) -> tf.data.Dataset:\n","  if min > max:\n","    raise ValueError(f\"min {min} must be smaller than the max {max}\")\n","  if max < 0:\n","    raise ValueError(f\"max {max} must be a positive number\")\n","\n","  def ds_filter_length_helper(inputs, outputs):\n","    seq_len = keras.ops.argmax(keras.ops.equal(inputs[activity_attr], pad_token), axis=-1)\n","    gte = keras.ops.greater_equal(seq_len, min)\n","    lt = keras.ops.less(seq_len, max)\n","    return keras.ops.logical_and(gte, lt)\n","\n","  ds = ds.filter(ds_filter_length_helper)\n","\n","  ds_len = 0\n","  for _ in ds:\n","    ds_len += 1\n","\n","  ds = ds.apply(tf.data.experimental.assert_cardinality(ds_len))\n","\n","  return ds\n"]},{"cell_type":"markdown","metadata":{"id":"OODAQuXGi6XC"},"source":["### Model Functions"]},{"cell_type":"markdown","metadata":{"id":"hBrbMltDFPr7"},"source":["#### Tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyE9NEWfXfvx"},"outputs":[],"source":["class LayerNameMixin:\n","  \"\"\"A mixin class that provides functionality to generate unique layer names.\n","\n","    This class is designed to be used in conjunction with Keras models.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","    \"\"\"\n","  def _generate_layer_name(self, prefix: str, suffix: Optional[str]):\n","    \"\"\"Generates a unique layer name based on the given prefix and suffix.\n","\n","      The method ensures that the generated name is unique among the existing layer names\n","      in the `self.layers` collection. If the combination of prefix and suffix already\n","      exists, it appends a numerical suffix to make the name unique.\n","\n","      Args:\n","          prefix (str): The base prefix for the layer name.\n","          suffix (Optional[str]): An optional suffix to append to the prefix. If None, only the prefix is used.\n","\n","      Returns:\n","          str: A unique layer name that does not conflict with existing layer names.\n","\n","      Example:\n","          If `prefix` is \"conv\" and `suffix` is \"1\", and \"conv_1\" already exists,\n","          the method will return \"conv_1_1\" (or the next available unique name).\n","    \"\"\"\n","    layer_names = set(layer.name for layer in self.layers)\n","\n","    name = prefix if suffix is None else f\"{prefix}_{suffix}\"\n","\n","    if name in layer_names:\n","      i = 1\n","      while f\"{name}_{i}\" in layer_names:\n","        i += 1\n","      name = f\"{name}_{i}\"\n","\n","    return name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoNgsnY5gVlK"},"outputs":[],"source":["class BaseModelMixin:\n","  \"\"\"\n","    A mixin class that provides default configurations for machine learning models,\n","    including optimizers, loss functions, metrics, and callbacks.\n","\n","    This class is designed to be extended by specific model types (e.g., regression,\n","    binary classification, multiclass classification) to provide default behaviors tailored to those tasks.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  @property\n","  def default_optimizer(self) -> keras.Optimizer:\n","    #return keras.optimizers.AdamW(learning_rate=self.default_learning_rate, weight_decay=0.0003, clipnorm=1.0)\n","    return keras.optimizers.AdamW(learning_rate=self.default_learning_rate, weight_decay=0.01, clipnorm=3.0)\n","\n","  def is_regression(self, activation: str) -> bool:\n","    return True if activation in ['linear', 'relu'] else False\n","\n","  def is_binary_classification(self, activation: str) -> bool:\n","    return True if activation in ['sigmoid'] else False\n","\n","  def is_multiclass_classification(self, activation: str) -> bool:\n","    return True if activation in ['softmax'] else False\n","\n","  @property\n","  def default_monitor_metric(self):\n","    return 'val_loss'\n","\n","  @property\n","  def default_learning_rate(self) -> Union[float, keras.optimizers.schedules.LearningRateSchedule]:\n","    return keras.optimizers.schedules.CosineDecay(\n","      initial_learning_rate=3e-5,\n","      decay_steps=TRAIN_STEPS_EPOCH*10\n","    )\n","\n","  def find_default_loss(self, activation: str) -> keras.losses.Loss:\n","    if self.is_regression(activation):\n","      return self.default_regression_loss\n","    elif self.is_binary_classification(activation):\n","      return self.default_binary_classification_loss\n","    elif self.is_multiclass_classification(activation):\n","      return self.default_multiclass_classification_loss\n","    else:\n","      raise ValueError(f\"Unknown activation {activation}\")\n","\n","  def find_default_metrics(self, activation: str) -> List[keras.metrics.Metric]:\n","    if self.is_regression(activation):\n","      return self.default_regression_metrics\n","    elif self.is_binary_classification(activation):\n","      return self.default_binary_classification_metrics\n","    elif self.is_multiclass_classification(activation):\n","      return self.default_multiclass_classification_metrics\n","    else:\n","      raise ValueError(f\"Unknown activation {activation}\")\n","\n","  @property\n","  def default_regression_loss(self) -> keras.losses.Loss:\n","    return keras.losses.LogCosh()\n","\n","  @property\n","  def default_binary_classification_loss(self) -> keras.losses.Loss:\n","    return keras.losses.BinaryCrossentropy()\n","\n","  @property\n","  def default_multiclass_classification_loss(self) -> keras.losses.Loss:\n","    return keras.losses.SparseCategoricalCrossentropy()\n","\n","  @property\n","  def default_regression_metrics(self) -> List[keras.metrics.Metric]:\n","    return [\n","        keras.metrics.MeanAbsoluteError(),\n","        keras.metrics.MeanSquaredError(),\n","        keras.metrics.RootMeanSquaredError(),\n","        keras.metrics.MeanSquaredLogarithmicError(),\n","        keras.metrics.MeanAbsolutePercentageError(),\n","        keras.metrics.LogCoshError(),\n","    ]\n","\n","  @property\n","  def default_binary_classification_metrics(self) -> List[keras.metrics.Metric]:\n","    return [\n","        keras.metrics.Accuracy(),\n","        keras.metrics.SparseCategoricalAccuracy(),\n","        keras.metrics.F1Score(name=\"f1_score\"),\n","        keras.metrics.AUC(curve=\"ROC\", name='roc_auc'),\n","        keras.metrics.AUC(curve=\"PR\", name='pr_auc'),\n","        keras.metrics.BinaryCrossentropy(),\n","    ]\n","\n","  @property\n","  def default_multiclass_classification_metrics(self) -> List[keras.metrics.Metric]:\n","    return [\n","        keras.metrics.SparseCategoricalAccuracy(),\n","        keras.metrics.F1Score(average='macro', name=\"f1_score_macro\"),\n","        keras.metrics.F1Score(average='micro', name=\"f1_score_micro\"),\n","        keras.metrics.F1Score(average='weighted', name=\"f1_score_weighted\"),\n","        keras.metrics.SparseCategoricalCrossentropy(),\n","    ]\n","\n","  def default_callbacks(\n","    self,\n","    log_file: Optional[str] = None,\n","    tensorboard_dir: Optional[str] = None,\n","    backup_dir: Optional[str] = None,\n","    checkpoint_file: Optional[str] = None,\n","    **kwargs\n","  ) -> List[keras.callbacks.Callback]:\n","\n","    callbacks = []\n","    #callbacks.append(keras.callbacks.ReduceLROnPlateau(patience=5, min_lr=DEFAULT_MIN_LEARNING_RATE, factor=0.5 monitor=self.default_monitor_metric))\n","\n","    if log_file is not None:\n","      callbacks.append(keras.callbacks.CSVLogger(\n","        filename=log_file if log_file.endswith(\".csv\") else f\"{log_file}.csv\",\n","        separator=\";\",\n","        append=backup_dir is not None,\n","      ))\n","\n","    if checkpoint_file is not None:\n","      callbacks.append(keras.callbacks.ModelCheckpoint(\n","        filepath=checkpoint_file,\n","        monitor=self.default_monitor_metric,\n","        save_best_only=True,\n","        save_weights_only=False,\n","        mode='auto',\n","        save_freq='epoch',\n","        verbose=1,\n","      ))\n","\n","    if tensorboard_dir is not None:\n","      callbacks.append(keras.callbacks.TensorBoard(\n","        log_dir=tensorboard_dir,\n","        update_freq='epoch',\n","        histogram_freq=5,\n","        embeddings_freq=5,\n","        write_graph=True,\n","        write_images=True,\n","        #profile_batch='10,30',\n","      ))\n","\n","    if backup_dir is not None:\n","      callbacks.append(keras.callbacks.BackupAndRestore(\n","        backup_dir,\n","        save_freq='epoch',\n","        delete_checkpoint=False,\n","      ))\n","\n","    callbacks = callbacks + [\n","      keras.callbacks.TerminateOnNaN(),\n","      keras.callbacks.EarlyStopping(\n","        monitor=self.default_monitor_metric,\n","        mode='auto',\n","        patience=10,\n","        verbose=1,\n","        restore_best_weights=True,\n","        start_from_epoch=0,\n","      ),\n","    ]\n","    return callbacks\n","\n","  __author__ = \"Marc C. Hennig\"\n","\n","class RegressionModelMixin(BaseModelMixin):\n","  \"\"\"A mixin class for regression models. Extends `BaseModelMixin` to provide\n","    default loss and metrics for regression tasks.\n","\n","    Author:\n","      Marc C. Hennig (mhennig@hm.edu)\n","   \"\"\"\n","  @property\n","  def default_loss(self) -> keras.losses.Loss:\n","    return self.default_regression_loss\n","\n","  @property\n","  def default_metrics(self) -> List[keras.metrics.Metric]:\n","    return self.default_regression_metrics\n","\n","class MulticlassModelMixin(BaseModelMixin):\n","  \"\"\"A mixin class for multiclass classification models. Extends `BaseModelMixin` to provide\n","    default loss and metrics for multiclass classification tasks.\n","\n","    Author:\n","      Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  @property\n","  def default_loss(self) -> keras.losses.Loss:\n","    return self.default_multiclass_classification_loss\n","\n","  @property\n","  def default_metrics(self) -> List[keras.metrics.Metric]:\n","    return self.default_multiclass_classification_metrics\n","\n","class BinaryclassModelMixin(BaseModelMixin):\n","  \"\"\"A mixin class for binary classification models. Extends `BaseModelMixin` to provide\n","    default loss and metrics for multiclass classification tasks.\n","\n","    Author:\n","      Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  @property\n","  def default_loss(self) -> keras.losses.Loss:\n","    return self.default_binary_classification_loss\n","\n","  @property\n","  def default_metrics(self) -> List[keras.metrics.Metric]:\n","    return self.default_binary_classification_metrics\n","\n","def get_default_callbacks(\n","    log_file: Optional[str] = None,\n","    tensorboard_dir: Optional[str] = None,\n","    backup_dir: Optional[str] = None,\n","    checkpoint_file: Optional[str] = None,\n","  ) -> List[keras.callbacks.Callback]:\n","\n","    def learning_rate_scheduler(epoch: int, learning_rate: float):\n","      if learning_rate_warmup is not None and epoch <= learning_rate_warmup.warmup_epochs:\n","        return learning_rate_warmup(epoch, learning_rate)\n","      elif learning_rate_decay is not None:\n","        return learning_rate_decay(epoch, learning_rate)\n","      else:\n","        return learning_rate\n","\n","    callbacks = []\n","    if log_file is not None:\n","      callbacks.append(keras.callbacks.CSVLogger(\n","        filename=log_file if log_file.endswith(\".csv\") else f\"{log_file}.csv\",\n","        separator=\";\",\n","        append=backup_dir is not None,\n","      ))\n","\n","    if checkpoint_file is not None:\n","      callbacks.append(keras.callbacks.ModelCheckpoint(\n","        filepath=checkpoint_file,\n","        monitor='val_loss',\n","        save_best_only=True,\n","        save_weights_only=False,\n","        mode='auto',\n","        save_freq='epoch',\n","        verbose=1,\n","      ))\n","\n","    if tensorboard_dir is not None:\n","      callbacks.append(keras.callbacks.TensorBoard(\n","        log_dir=tensorboard_dir,\n","        update_freq='epoch',\n","        histogram_freq=5,\n","        embeddings_freq=5,\n","        write_graph=True,\n","        write_images=True,\n","        #profile_batch='10,30',\n","      ))\n","\n","    if backup_dir is not None:\n","      callbacks.append(keras.callbacks.BackupAndRestore(\n","        backup_dir,\n","        save_freq='epoch',\n","        delete_checkpoint=False,\n","      ))\n","    callbacks = callbacks + [\n","      keras.callbacks.TerminateOnNaN(),\n","      keras.callbacks.LearningRateScheduler(learning_rate_scheduler, verbose=1),\n","      keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        mode='auto',\n","        patience=20,\n","        verbose=1,\n","        restore_best_weights=True,\n","        start_from_epoch=20,\n","      ),\n","    ]\n","    return callbacks\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8DwTrK5r5UB"},"outputs":[],"source":["def model_visualize_history(model: keras.Model, history = None):\n","  if history is not None:\n","    history = history.history\n","  else:\n","    history = model.history.history\n","\n","  for metric in [key for key in history.keys() if not key.startswith('val_')]:\n","    if metric.startswith(TARGET_NEXT_ACTIVITY):\n","      title = TARGET_NEXT_ACTIVITY.replace(\"_\", \" \").title()\n","      y_label = metric.replace(TARGET_NEXT_ACTIVITY, '').replace(\"_\", \" \").title()\n","    elif metric.startswith(TARGET_NEXT_TIME):\n","      title = TARGET_NEXT_TIME.replace(\"_\", \" \").title()\n","      y_label = metric.replace(TARGET_NEXT_TIME, '').replace(\"_\", \" \").title()\n","    elif metric.startswith(TARGET_REMAINING_TIME):\n","      title = TARGET_REMAINING_TIME.replace(\"_\", \" \").title()\n","      y_label = metric.replace(TARGET_REMAINING_TIME, '').replace(\"_\", \" \").title()\n","    else:\n","      title = \"\"\n","      y_label = metric.replace(\"_\", \" \").title()\n","\n","    plt.plot(history[metric])\n","    if f\"val_{metric}\" in history:\n","      plt.plot(history[f\"val_{metric}\"])\n","\n","    plt.title(title)\n","    plt.ylabel(y_label)\n","    plt.xlabel(\"Epoch\")\n","    if f\"val_{metric}\" in history:\n","      plt.legend([\"train\", \"test\"], loc='upper left')\n","    plt.tight_layout()\n","    plt.savefig(\n","        os.path.join(GRAPHIC_DIR, f\"{model.name}_{metric}.svg\"),\n","        bbox_inches='tight'\n","    )\n","    plt.show()\n","\n","def model_regression_report(model: keras.Model, ds: tf.data.Dataset, target: str, archive: bool = False):\n","  y_true = np.concatenate([y[target] for x, y in ds], axis=0).flatten()\n","  y_pred = model.predict(ds)\n","\n","  if len(model.output_names) > 1:\n","    target_idx = model.output_names.index(target)\n","    y_pred = y_pred[target_idx]\n","\n","  y_pred = y_pred.flatten()\n","  result = {\n","      \"MAE\": float(sl.metrics.mean_absolute_error(y_true, y_pred)),\n","      \"MSE\": float(sl.metrics.mean_squared_error(y_true, y_pred)),\n","      \"RMSE\": float(sl.metrics.root_mean_squared_error(y_true, y_pred)),\n","      \"MEDAE\": float(sl.metrics.median_absolute_error(y_true, y_pred)),\n","      \"MAPE\": float(sl.metrics.mean_absolute_percentage_error(y_true, y_pred)),\n","  }\n","\n","  if y_true.min() >= 0 and y_pred.min() >= 0:\n","    result.update({\n","      \"MSLE\": float(sl.metrics.mean_squared_log_error(y_true, y_pred)),\n","      \"RMSLE\": float(sl.metrics.root_mean_squared_log_error(y_true, y_pred)),\n","    })\n","\n","  filename_prefix = model.name if target in model_name else f\"{model.name}_{target}\"\n","\n","  try:\n","    if archive:\n","      np.savez_compressed(os.path.join(OUTPUT_DATA_DIR, f\"{filename_prefix}.npz\"), ground_truth=y_true.astype(float), predictions=y_true.astype(float), allow_pickle=False)\n","    else:\n","      np.save(os.path.join(OUTPUT_DATA_DIR, f\"{filename_prefix}_predictions.npy\"), y_pred.astype(float), allow_pickle=False)\n","      np.save(os.path.join(OUTPUT_DATA_DIR, f\"{filename_prefix}_groundtruth.npy\"), y_true.astype(float), allow_pickle=False)\n","  except Exception as e:\n","    print(f\"Failed to save predicitons: {e}\")\n","\n","  print(\"\\t\\t\".join(result.keys()))\n","  print(\"\\t\\t\".join([\"%.4f\" % result for result in result.values()]))\n","\n","  with open(os.path.join(OUTPUT_DATA_DIR, f\"{filename_prefix}_regression_report.json\"), 'w') as file:\n","    json.dump(result, file)\n","\n","  return result\n","\n","def model_classification_report(model: keras.Model, ds: tf.data.Dataset, target: str, archive: bool = False):\n","  y_true = np.concatenate([y[target] for x, y in ds], axis=0).flatten()\n","  y_pred = model.predict(ds)\n","\n","  if len(model.output_names) > 1:\n","    target_idx = model.output_names.index(target)\n","    y_pred = y_pred[target_idx]\n","\n","  filename_prefix = model.name if target in model_name else f\"{model.name}_{target}\"\n","\n","  try:\n","    if archive:\n","      np.savez_compressed(os.path.join(OUTPUT_DATA_DIR, f\"{filename_prefix}.npz\"), ground_truth=y_true.astype(float), predictions=y_true.astype(float), allow_pickle=False)\n","    else:\n","      np.save(os.path.join(OUTPUT_DATA_DIR, f\"{filename_prefix}_predictions.npy\"), y_pred.astype(float), allow_pickle=False)\n","      np.save(os.path.join(OUTPUT_DATA_DIR, f\"{filename_prefix}_groundtruth.npy\"), y_true.astype(float), allow_pickle=False)\n","\n","  except Exception as e:\n","    print(f\"Failed to save predictions: {e}\")\n","\n","  y_pred = np.argmax(y_pred, axis=1)\n","\n","  print(sl.metrics.classification_report(y_true, y_pred, digits=4))\n","  report = sl.metrics.classification_report(y_true, y_pred, digits=4, output_dict=True)\n","\n","  report.update({\n","      \"balanced_accuracy\": sl.metrics.balanced_accuracy_score(y_true, y_pred),\n","  })\n","\n","  with open(os.path.join(OUTPUT_DATA_DIR, f\"{filename_prefix}_classification_report.json\"), 'w') as file:\n","    json.dump(report, file)\n","\n","  confusion_matrix = sl.metrics.confusion_matrix(y_true, y_pred)\n","  confusion_disp = sl.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n","  confusion_disp.plot()\n","  plt.title(\"Confusion Matrix\")\n","  plt.savefig(os.path.join(GRAPHIC_DIR, f\"{filename_prefix}_confusion_matrix.svg\"))\n","  plt.show()\n","\n","def model_save_files(model: keras.Model, ds: tf.data.Dataset = None):\n","  history = model.history.history\n","  with open(os.path.join(OUTPUT_DATA_DIR, f\"{model.name}_history.json\"), 'w') as file:\n","    json.dump(history, file)\n","\n","  model.save(\n","      os.path.join(MODEL_DIR, f\"{model.name}.keras\"),\n","      overwrite=True\n","  )\n","\n","  if ds is not None:\n","    evaluation = model.evaluate(\n","        ds,\n","        return_dict=True,\n","        verbose=1,\n","    )\n","    with open(os.path.join(OUTPUT_DATA_DIR, f\"{model.name}_evaluation.json\"), 'w') as file:\n","      json.dump(evaluation, file)\n","\n","  keras.utils.plot_model(\n","    model,\n","    to_file=os.path.join(GRAPHIC_DIR, f\"{model.name}.png\"),\n","    show_shapes=True,\n","    show_dtype=True,\n","    show_layer_names=False,\n","    dpi=400,\n","    show_layer_activations=True,\n","  )\n","  try:\n","    keras.utils.plot_model(\n","      model,\n","      to_file=os.path.join(GRAPHIC_DIR, f\"{model.name}.svg\"),\n","      show_shapes=True,\n","      show_dtype=True,\n","      show_layer_names=False,\n","      show_layer_activations=True,\n","    )\n","  except:\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"tpOclu7boQBd"},"source":["### Model Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5HRdWiWHKkT"},"outputs":[],"source":["def evaluate_regression(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","  if y_pred.ndim > 1:\n","      y_pred = np.argmax(y_pred, axis=1)\n","\n","  def logcosh_error(y_true, y_pred):\n","    error = np.subtract(y_pred, y_true)\n","    return np.mean(np.log((np.exp(error) + np.exp(-error))/2))\n","\n","  return {\n","    'mae': sl.metrics.mean_absolute_error(y_true, y_pred),\n","    'mse': sl.metrics.mean_squared_error(y_true, y_pred),\n","    'rmse': sl.metrics.root_mean_squared_error(y_true, y_pred),\n","    'mape': sl.metrics.mean_absolute_percentage_error(y_true, y_pred),\n","    'msle': sl.metrics.mean_squared_log_error(y_true, y_pred),\n","    'medae': sl.metrics.median_absolute_error(y_true, y_pred),\n","    'logcosh': logcosh_error(y_true, y_pred),\n","    'max_error': sl.metrics.max_error(y_true, y_pred),\n","  }\n","\n","def evaluate_multiclass_classification(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","  if y_pred.ndim > 1:\n","      y_pred = np.argmax(y_pred, axis=1)\n","\n","  return {\n","    'accuracy': sl.metrics.accuracy_score(y_true, y_pred),\n","    'accuracy_balanced': sl.metrics.balanced_accuracy_score(y_true, y_pred),\n","    'accuracy_balanced_adjusted': sl.metrics.balanced_accuracy_score(y_true, y_pred, adjusted=True),\n","    'f1_micro': sl.metrics.f1_score(y_true, y_pred, average='micro'),\n","    'f1_macro': sl.metrics.f1_score(y_true, y_pred, average='macro'),\n","    'f1_weighted': sl.metrics.f1_score(y_true, y_pred, average='weighted'),\n","    'precision_micro': sl.metrics.precision_score(y_true, y_pred, average='micro', zero_division='warn'),\n","    'precision_macro': sl.metrics.precision_score(y_true, y_pred, average='macro'),\n","    'precision_weighted': sl.metrics.precision_score(y_true, y_pred, average='weighted'),\n","    'recall_micro': sl.metrics.recall_score(y_true, y_pred, average='micro'),\n","    'recall_macro': sl.metrics.recall_score(y_true, y_pred, average='macro'),\n","    'recall_weighted': sl.metrics.recall_score(y_true, y_pred, average='weighted'),\n","  }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9s7JdNd_GK3-"},"outputs":[],"source":["def regression_paired_differences(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, error_type: Literal['absolute', 'squared'] = 'absolute') -> np.ndarray:\n","  if 'absolute' == error_type:\n","    model_1_errors = np.abs(y_true - y_pred_model_1)\n","    model_2_errors = np.abs(y_true - y_pred_model_2)\n","  elif 'squared' == error_type:\n","    model_1_errors = np.square(y_true - y_pred_model_1)\n","    model_2_errors = np.square(y_true - y_pred_model_2)\n","\n","  paired_differences = model_2_errors - model_1_errors # Positive means model 1 is better\n","  return paired_differences\n","\n","def classification_paired_differences(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray) -> np.ndarray:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","\n","  model_1_probs_correct = y_probs_model_1[np.arange(len(y_true)), y_true]\n","  model_2_probs_correct = y_probs_model_2[np.arange(len(y_true)), y_true]\n","\n","  # Calculate paired differences in confidence for the true class\n","  paired_differences = model_1_probs_correct - model_2_probs_correct\n","\n","  return paired_differences\n","\n","def regression_bootstrap_twosided_confidence_intervals(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, metric: Callable[[np.ndarray, np.ndarray], np.ndarray] = sl.metrics.mean_absolute_error, n_iterations: int = 10000, alpha: float = 0.05) -> Tuple[float, float]:\n","  return bootstrap_twosided_confidence_intervals(y_true, y_pred_model_1, y_pred_model_2, metric, n_iterations, alpha)\n","\n","def classification_bootstrap_twosided_confidence_intervals(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, metric: Callable[[np.ndarray, np.ndarray], np.ndarray] = sl.metrics.accuracy_score, n_iterations: int = 10000, alpha: float = 0.05) -> Tuple[float, float]:\n","  return bootstrap_twosided_confidence_intervals(y_true, y_pred_model_1, y_pred_model_2, metric, n_iterations, alpha)\n","\n","def bootstrap_twosided_confidence_intervals(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, metric: Callable[[np.ndarray, np.ndarray], np.ndarray], n_iterations: int = 10000, alpha: float = 0.05) -> Tuple[float, float]:\n","  sample_size = len(y_true)\n","\n","  boot_diffs = []\n","  for _ in range(n_iterations):\n","    # Resample indices with replacement\n","    indices = np.random.choice(range(sample_size), size=sample_size, replace=True)\n","\n","    y_true_sample = y_true[indices]\n","    y_pred_1_sample = y_pred_model_1[indices]\n","    y_pred_2_sample = y_pred_model_2[indices]\n","\n","    metric_1 = metric(y_true_sample, y_pred_1_sample)\n","    metric_2 = metric(y_true_sample, y_pred_2_sample)\n","\n","    boot_diffs.append(metric_1 - metric_2)\n","\n","  lower = np.percentile(boot_diffs, 100 * alpha / 2)\n","  upper = np.percentile(boot_diffs, 100 * (1 - alpha / 2))\n","\n","  return lower, upper\n","\n","def bootstrap_paired_differences_twosided_confidence_interval(paired_differences: np.ndarray, n_iterations: int = 10000, alpha: float = 0.05) -> Tuple[float, float]:\n","  sample_size = len(paired_differences)\n","\n","  boot_means = []\n","  for _ in range(n_iterations):\n","    # Resample indices with replacement\n","    indices = np.random.choice(paired_differences, size=sample_size, replace=True)\n","\n","    paired_diff_sample = np.random.choice(paired_differences, size=len(paired_differences), replace=True)\n","    boot_means.append(np.mean(paired_diff_sample))\n","\n","  lower = np.percentile(boot_means, 100 * alpha / 2)\n","  upper = np.percentile(boot_means, 100 * (1 - alpha / 2))\n","\n","  return lower, upper\n","\n","def test_mcnemar(y_true: np.ndarray, y_pred_model_1: np.ndarray, y_pred_model_2: np.ndarray, exact: bool = True) -> Tuple[float, float]:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","  if y_pred_model_1.ndim > 1:\n","      y_pred_model_1 = np.argmax(y_pred_model_1, axis=1)\n","  if y_pred_model_2.ndim > 1:\n","      y_pred_model_2 = np.argmax(y_pred_model_2, axis=1)\n","\n","  # Build the contingency table\n","  a = np.sum((y_true == y_pred_model_1) & (y_true == y_pred_model_2)) # both correct\n","  b = np.sum((y_true == y_pred_model_1) & (y_true != y_pred_model_2)) # m1 correct, m2 incorrect\n","  c = np.sum((y_true != y_pred_model_1) & (y_true == y_pred_model_2)) # m1 incorrect, m2 correct\n","  d = np.sum((y_true != y_pred_model_1) & (y_true != y_pred_model_2)) # both incorrect\n","\n","  table = np.array([[a, b],\n","                    [c, d]])\n","\n","  stat, p_value = sm.stats.contingency_tables.mcnemar(contingency_table, exact=exact)\n","\n","  # Effect sizes\n","  prop_diff = (b - c) / n\n","  odds_ratio = b / c if c != 0 else np.nan\n","  cohen_g = (b - c) / (b + c) if (b + c) != 0 else np.nan\n","\n","  return {\n","    \"test_statistic\": stat,\n","    \"p_value\": p_value,\n","    \"contingency_table\": table,\n","    \"proportion_difference\": prop_diff,\n","    \"odds_ratio\": odds_ratio,\n","    \"cohens_g\": cohen_g,\n","  }\n","\n","def test_wilcoxon_signed_rank(paired_differences: np.ndarray) -> Tuple[float, float]:\n","  stat, p_value = sp.stats.wilcoxon(paired_differences, alternative='two-sided')\n","\n","  n = len(paired_differences)\n","  mean_w = n * (n + 1) / 4\n","  std_w = np.sqrt(n * (n + 1) * (2 * n + 1) / 24)\n","\n","  # Calculate the z-score using the Wilcoxon test statistic\n","  z_score = (stat - mean_w) / std_w\n","\n","  # Calculate Rosenthal's r effect size\n","  effect_size = abs(z_score) / np.sqrt(n)\n","\n","  return {\n","    \"test_statistic\": stat,\n","    \"p_value\": p_value,\n","    \"z_score\": z_score,\n","    \"rosenthals_r\": effect_size,\n","  }\n","\n","def interpret_effect_size(effect_size: float) -> Literal['negligible', 'small', 'medium', 'large']:\n","  if effect_size < 0.1:\n","    effect_interpretation = \"negligible\"\n","  elif effect_size < 0.3:\n","    effect_interpretation = \"small\"\n","  elif effect_size < 0.5:\n","    effect_interpretation = \"medium\"\n","  else:\n","    effect_interpretation = \"large\"\n","\n","  return effect_interpretation\n","\n","def visualize_paired_differences(\n","    paired_differences: np.ndarray,\n","    confidence_level: float = 0.95,\n","    ci_lower: Optional[float] = None,\n","    ci_upper: Optional[float] = None,\n","    name_model_1: str = \"Model 1\",\n","    name_model_2: str = \"Model 2\"\n","  ):\n","  mean_diff = np.mean(paired_differences)\n","  median_diff = np.median(paired_differences)\n","\n","  # Create visualization\n","  plt.figure(figsize=(12, 6))\n","\n","  # Main histogram\n","  plt.hist(paired_differences, bins='auto', density=True, alpha=0.7, color='skyblue', edgecolor='black')\n","\n","  # Add kernel density estimate\n","  kde = sp.stats.gaussian_kde(paired_differences)\n","  x_range = np.linspace(min(paired_differences), max(paired_differences), 200)\n","  plt.plot(x_range, kde(x_range), 'r-', lw=2, label='KDE')\n","\n","  # Reference lines\n","  plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5, label=\"No difference\")\n","  plt.axvline(x=mean_diff, color='green', linestyle='-', label=\"Mean\")\n","  plt.axvline(x=median_diff, color='blue', linestyle='-', label=\"Median\")\n","\n","  # Confidence interval\n","  if ci_lower is not None and ci_upper is not None:\n","    plt.axvspan(ci_lower, ci_upper, alpha=0.2, color='green', label=f'{confidence_level*100:.0f}% CI')\n","\n","  plt.xlabel(f\"Performance Difference ({name_model_1} - {name_model_2})\")\n","  plt.ylabel(\"Density\")\n","  plt.title(\"Distribution of Paired Differences\")\n","  plt.legend()\n","  plt.grid(True, alpha=0.3)\n"]},{"cell_type":"markdown","metadata":{"id":"4eI16TYHc40F"},"source":["## General Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBtnLFVic3W2"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class MinMaxScaler(keras.src.layers.preprocessing.tf_data_layer.TFDataLayer):\n","  \"\"\" Implementation of a MinMax scaler layer for Keras/Tensorflow preprocessing.\n","\n","  The code is based on the Keras implementation of a norm layer (https://github.com/keras-team/keras/blob/v3.3.3/keras/src/layers/preprocessing/normalization.py) and the scikit-learn implementation (https://github.com/scikit-learn/scikit-learn/blob/6a0838c41/sklearn/preprocessing/_data.py#L288).\n","\n","  Author:\n","      Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  def __init__(\n","    self,\n","    axis: Optional[int] = -1,\n","    feature_range: Tuple[float, float] = (0, 1),\n","    min: Optional[float] = None,\n","    max: Optional[float] = None,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    # Set `min` and `max` if passed.\n","    if feature_range is None or len(feature_range) != 2:\n","      raise ValueError(f\"Feature range must be a tuple of length 2. Received feature_range={feature_range}\")\n","    elif feature_range[0] >= feature_range[1]:\n","      raise ValueError(f\"Feature range minimum must be less than the maximum. Received feature_range={feature_range}\")\n","    else:\n","      feature_range_min, feature_range_max = feature_range\n","\n","    # Standardize `axis` to a tuple.\n","    if axis is None:\n","      axis = ()\n","    elif isinstance(axis, int):\n","      axis = (axis,)\n","    else:\n","      axis = tuple(axis)\n","\n","    self.axis = axis\n","\n","    self.input_min = min\n","    self.input_max = max\n","\n","    self.min = None\n","    self.max = None\n","\n","    self.input_feature_range_min = feature_range_min\n","    self.input_feature_range_max = feature_range_max\n","\n","    self.feature_range_min = None\n","    self.feature_range_max = None\n","\n","    self.supports_masking = True\n","    self._build_input_shape = None\n","\n","  def build(self, input_shape):\n","    if input_shape is None:\n","      return\n","\n","    ndim = len(input_shape)\n","    self._build_input_shape = input_shape\n","\n","    if any(a < -ndim or a >= ndim for a in self.axis):\n","      raise ValueError(f\"All `axis` values must be in the range [-ndim, ndim). Received inputs with ndim={ndim}, while axis={self.axis}\")\n","\n","    # Axes to be kept, replacing negative values with positive equivalents.\n","    # Sorted to avoid transposing axes.\n","    self._keep_axis = tuple(sorted([d if d >= 0 else d + ndim for d in self.axis]))\n","    # All axes to be kept should have known shape.\n","    for d in self._keep_axis:\n","      if input_shape[d] is None:\n","        raise ValueError(f\"All `axis` values to be kept must have a known shape. Received axis={self.axis}, inputs.shape={input_shape}, with unknown axis at index {d}\")\n","\n","    # Axes to be reduced.\n","    self._reduce_axis = tuple(d for d in range(ndim) if d not in self._keep_axis)\n","    # 1 if an axis should be reduced, 0 otherwise.\n","    self._reduce_axis_mask = [0 if d in self._keep_axis else 1 for d in range(ndim)]\n","    # Broadcast any reduced axes.\n","    self._broadcast_shape = [input_shape[d] if d in self._keep_axis else 1 for d in range(ndim)]\n","    min_max_shape = tuple(input_shape[d] for d in self._keep_axis)\n","    self.min_max_shape = min_max_shape\n","\n","    self.feature_range_min = keras.ops.convert_to_tensor(self.input_feature_range_min)\n","    self.feature_range_max = keras.ops.convert_to_tensor(self.input_feature_range_max)\n","\n","    if self.input_min is None:\n","      self.adapt_min = self.add_weight(\n","        name=\"min\",\n","        shape=min_max_shape,\n","        initializer=\"zeros\",\n","        trainable=False,\n","      )\n","      self.min = keras.ops.reshape(self.adapt_min, self._broadcast_shape)\n","    else:\n","      self.min = keras.ops.reshape(keras.ops.convert_to_tensor(self.input_min), self._broadcast_shape)\n","\n","    if self.input_max is None:\n","      self.adapt_max = self.add_weight(\n","        name=\"max\",\n","        shape=min_max_shape,\n","        initializer=\"zeros\",\n","        trainable=False,\n","      )\n","      self.max = keras.ops.reshape(self.adapt_max, self._broadcast_shape)\n","\n","    else:\n","      self.max = keras.ops.reshape(keras.ops.convert_to_tensor(self.input_max), self._broadcast_shape)\n","\n","    self.min = keras.ops.cast(self.min, self.compute_dtype)\n","    self.max = keras.ops.cast(self.max, self.compute_dtype)\n","\n","    self.built = True\n","\n","  def adapt(self, data):\n","    if isinstance(data, np.ndarray) or keras.ops.is_tensor(data):\n","      input_shape = data.shape\n","    elif isinstance(data, tf.data.Dataset):\n","      input_shape = tuple(data.element_spec.shape)\n","      if len(input_shape) == 1:\n","        # Batch dataset if it isn't batched\n","        data = data.batch(128)\n","      input_shape = tuple(data.element_spec.shape)\n","    else:\n","      raise ValueError(f\"Unsupported data type: {type(data)}\")\n","\n","    if not self.built:\n","      self.build(input_shape)\n","    else:\n","      for d in self._keep_axis:\n","        if input_shape[d] != self._build_input_shape[d]:\n","          raise ValueError(f\"The layer was built with input_shape={self._build_input_shape}, but adapt() is being called with data with an incompatible shape, data.shape={input_shape}\")\n","\n","    if isinstance(data, np.ndarray):\n","      total_min = np.min(data, axis=self._reduce_axis)\n","      total_max = np.max(data, axis=self._reduce_axis)\n","    elif keras.ops.is_tensor(data):\n","      total_min = keras.ops.min(data, axis=self._reduce_axis)\n","      total_max = keras.ops.max(data, axis=self._reduce_axis)\n","    elif isinstance(data, tf.data.Dataset):\n","      total_min = keras.ops.full(self._broadcast_shape, data.element_spec.dtype.max)\n","      total_max = keras.ops.full(self._broadcast_shape, data.element_spec.dtype.min)\n","      for batch in data:\n","        batch = keras.ops.convert_to_tensor(batch, dtype=self.compute_dtype)\n","\n","        batch_min = keras.ops.min(batch, axis=self._reduce_axis)\n","        batch_max = keras.ops.max(batch, axis=self._reduce_axis)\n","\n","        total_min = keras.ops.minimum(total_min, batch_min)\n","        total_max = keras.ops.maximum(total_max, batch_max)\n","    else:\n","      raise ValueError(f\"Unsupported data type: {type(data)}\")\n","\n","    self.adapt_min.assign(total_min)\n","    self.adapt_max.assign(total_max)\n","    self.finalize_state()\n","\n","  def finalize_state(self):\n","    if self.input_min is not None or self.input_max is not None or not self.built:\n","      return\n","\n","    # In the adapt case, we make constant tensors for mean and variance with\n","    # proper broadcast shape and dtype each time `finalize_state` is called.\n","    self.min = keras.ops.reshape(self.adapt_min, self._broadcast_shape)\n","    self.min = keras.ops.cast(self.min, self.compute_dtype)\n","    self.max = keras.ops.reshape(self.adapt_max, self._broadcast_shape)\n","    self.max = keras.ops.cast(self.max, self.compute_dtype)\n","\n","  def call(self, inputs):\n","    # This layer can be called in tf.data\n","    # even with another backend after it has been adapted.\n","    # However it must use backend-native logic for adapt().\n","    if self.min is None or self.max is None:\n","      # May happen when in tf.data when mean/var was passed explicitly\n","      raise ValueError(\"You must call `.build(input_shape)` on the layer before using it.\")\n","\n","    inputs = self.backend.numpy.convert_to_tensor(inputs, dtype=self.compute_dtype)\n","    # Ensure the weights are in the correct backend. Without this, it is\n","    # possible to cause breakage when using this layer in tf.data.\n","    #min = self.convert_weight(self.min)\n","    #max = self.convert_weight(self.max)\n","    min = self.min\n","    max = self.max\n","\n","    #feature_range_min = self.convert_weight(self.feature_range_min)\n","    #feature_range_max = self.convert_weight(self.feature_range_max)\n","    feature_range_min = self.feature_range_min\n","    feature_range_max = self.feature_range_max\n","\n","\n","    inputs_std = self.backend.numpy.divide(\n","     self.backend.numpy.subtract(inputs, min),\n","     self.backend.numpy.subtract(max, min)\n","    )\n","\n","    return self.backend.numpy.add(\n","        self.backend.numpy.multiply(\n","          inputs_std,\n","          self.backend.numpy.subtract(feature_range_max, feature_range_min)\n","    ), feature_range_min)\n","\n","  def compute_output_shape(self, input_shape):\n","    return input_shape\n","\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update(\n","      {\n","        \"axis\": self.axis,\n","        \"invert\": self.invert,\n","        \"mean\": np.array(self.input_mean).tolist(),\n","        \"variance\": np.array(self.input_variance).tolist(),\n","      }\n","    )\n","    return config\n","\n","  def load_own_variables(self, store):\n","    super().load_own_variables(store)\n","    # Ensure that we call finalize_state after variable loading.\n","    self.finalize_state()\n","\n","  def get_build_config(self):\n","    if self._build_input_shape:\n","      return {\"input_shape\": self._build_input_shape}\n","\n","  def build_from_config(self, config):\n","    if config:\n","      self.build(config[\"input_shape\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVBIlhaSt7i9"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class MeanScaler(keras.src.layers.preprocessing.tf_data_layer.TFDataLayer):\n","  \"\"\" Implementation of a Mean scaler layer for Keras/Tensorflow preprocessing.\n","\n","  The code is based on the Keras implementation of a norm layer (https://github.com/keras-team/keras/blob/v3.3.3/keras/src/layers/preprocessing/normalization.py) and the scikit-learn implementation (https://github.com/scikit-learn/scikit-learn/blob/6a0838c41/sklearn/preprocessing/_data.py#L288).\n","\n","  Author:\n","      Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  def __init__(\n","    self,\n","    axis: Optional[int] = -1,\n","    mean: Optional[float] = None,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    # Standardize `axis` to a tuple.\n","    if axis is None:\n","      axis = ()\n","    elif isinstance(axis, int):\n","      axis = (axis,)\n","    else:\n","      axis = tuple(axis)\n","\n","    self.axis = axis\n","\n","    self.input_mean = mean\n","\n","    self.supports_masking = True\n","    self._build_input_shape = None\n","\n","  def build(self, input_shape):\n","    if input_shape is None:\n","      return\n","\n","    ndim = len(input_shape)\n","    self._build_input_shape = input_shape\n","\n","    if any(a < -ndim or a >= ndim for a in self.axis):\n","      raise ValueError(f\"All `axis` values must be in the range [-ndim, ndim). Received inputs with ndim={ndim}, while axis={self.axis}\")\n","\n","    # Axes to be kept, replacing negative values with positive equivalents.\n","    # Sorted to avoid transposing axes.\n","    self._keep_axis = tuple(sorted([d if d >= 0 else d + ndim for d in self.axis]))\n","    # All axes to be kept should have known shape.\n","    for d in self._keep_axis:\n","      if input_shape[d] is None:\n","        raise ValueError(f\"All `axis` values to be kept must have a known shape. Received axis={self.axis}, inputs.shape={input_shape}, with unknown axis at index {d}\")\n","\n","    # Axes to be reduced.\n","    self._reduce_axis = tuple(d for d in range(ndim) if d not in self._keep_axis)\n","    # 1 if an axis should be reduced, 0 otherwise.\n","    self._reduce_axis_mask = [0 if d in self._keep_axis else 1 for d in range(ndim)]\n","    # Broadcast any reduced axes.\n","    self._broadcast_shape = [input_shape[d] if d in self._keep_axis else 1 for d in range(ndim)]\n","    min_max_shape = tuple(input_shape[d] for d in self._keep_axis)\n","    self.min_max_shape = min_max_shape\n","\n","    if self.input_mean is None:\n","      self.adapt_mean = self.add_weight(\n","        name=\"mean\",\n","        shape=min_max_shape,\n","        initializer=\"zeros\",\n","        trainable=False,\n","      )\n","      self.mean = keras.ops.reshape(self.adapt_mean, self._broadcast_shape)\n","    else:\n","      self.mean = keras.ops.reshape(keras.ops.convert_to_tensor(self.input_mean), self._broadcast_shape)\n","\n","    self.mean = keras.ops.cast(self.mean, self.compute_dtype)\n","\n","    self.built = True\n","\n","\n","  def adapt(self, data):\n","    if isinstance(data, np.ndarray) or keras.ops.is_tensor(data):\n","      input_shape = data.shape\n","    elif isinstance(data, tf.data.Dataset):\n","      input_shape = tuple(data.element_spec.shape)\n","      if len(input_shape) == 1:\n","        # Batch dataset if it isn't batched\n","        data = data.batch(128)\n","      input_shape = tuple(data.element_spec.shape)\n","    else:\n","      raise ValueError(f\"Unsupported data type: {type(data)}\")\n","\n","    if not self.built:\n","      self.build(input_shape)\n","    else:\n","      for d in self._keep_axis:\n","        if input_shape[d] != self._build_input_shape[d]:\n","          raise ValueError(f\"The layer was built with input_shape={self._build_input_shape}, but adapt() is being called with data with an incompatible shape, data.shape={input_shape}\")\n","\n","    if isinstance(data, np.ndarray):\n","      total_mean = np.mean(data, axis=self._reduce_axis)\n","    elif keras.ops.is_tensor(data):\n","      total_mean = keras.ops.mean(data, axis=self._reduce_axis)\n","    elif isinstance(data, tf.data.Dataset):\n","      total_mean = keras.ops.zeros(self._mean_and_var_shape)\n","      total_count = 0\n","      for batch in data:\n","        batch = keras.ops.convert_to_tensor(batch, dtype=self.compute_dtype)\n","\n","        batch_mean = keras.ops.mean(batch, axis=self._reduce_axis)\n","\n","        if self._reduce_axis:\n","          batch_reduce_shape = (batch.shape[d] for d in self._reduce_axis)\n","          batch_count = math.prod(batch_reduce_shape)\n","        else:\n","          batch_count = 1\n","\n","        total_count += batch_count\n","        batch_weight = float(batch_count) / total_count\n","        existing_weight = 1.0 - batch_weight\n","        total_mean = total_mean * existing_weight + batch_mean * batch_weight\n","    else:\n","      raise ValueError(f\"Unsupported data type: {type(data)}\")\n","\n","\n","    self.adapt_mean.assign(total_mean)\n","    self.finalize_state()\n","\n","  def finalize_state(self):\n","    if self.input_mean is not None or not self.built:\n","      return\n","\n","    # In the adapt case, we make constant tensors for mean and variance with\n","    # proper broadcast shape and dtype each time `finalize_state` is called.\n","    self.mean = keras.ops.reshape(self.adapt_mean, self._broadcast_shape)\n","    self.mean = keras.ops.cast(self.mean, self.compute_dtype)\n","\n","  def call(self, inputs):\n","    # This layer can be called in tf.data\n","    # even with another backend after it has been adapted.\n","    # However it must use backend-native logic for adapt().\n","    if self.mean is None:\n","      raise ValueError(\"You must call `.build(input_shape)` on the layer before using it.\")\n","\n","    inputs = self.backend.numpy.convert_to_tensor(inputs, dtype=self.compute_dtype)\n","    # Ensure the weights are in the correct backend. Without this, it is\n","    # possible to cause breakage when using this layer in tf.data.\n","    # mean = self.convert_weight(self.mean)\n","    mean = self.mean\n","\n","    return self.backend.numpy.divide(\n","        inputs,\n","        mean\n","    )\n","\n","  def compute_output_shape(self, input_shape):\n","    return input_shape\n","\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update(\n","      {\n","        \"axis\": self.axis,\n","        \"mean\": np.array(self.input_mean).tolist(),\n","      }\n","    )\n","    return config\n","\n","  def load_own_variables(self, store):\n","    super().load_own_variables(store)\n","    # Ensure that we call finalize_state after variable loading.\n","    self.finalize_state()\n","\n","  def get_build_config(self):\n","    if self._build_input_shape:\n","      return {\"input_shape\": self._build_input_shape}\n","\n","  def build_from_config(self, config):\n","    if config:\n","      self.build(config[\"input_shape\"])"]},{"cell_type":"markdown","metadata":{"id":"L01u4ZWC8nSw"},"source":["### GLU and GRN Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_q9cdgRC8s0e"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class GatedLinearUnit(keras.Layer):\n","  \"\"\"A Gated Linear Unit layer implementation for Keras. Based on the Temporal Fusion Transformer of Lim et al. 2019 (https://arxiv.org/abs/1912.09363).\n","\n","    This layer applies a gating mechanism to the input, where the output is the elementwise product of a linear transformation of the input and a gated (typically sigmoid) transformation of the input.\n","\n","    Args:\n","        units (int): The dimension of the output space.\n","        gate_activation (str, optional): Activation function for the gate. Defaults to 'sigmoid'.\n","        kernel_initializer (Union[str, keras.Initializer], optional): Initializer for the kernel weights. Defaults to 'glorot_uniform'.\n","        bias_initializer (Union[str, keras.Initializer], optional): Initializer for the bias vector. Defaults to 'zeros'.\n","        kernel_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for kernel weights. Defaults to 'l2'.\n","        bias_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for bias vector. Defaults to None.\n","        activity_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for layer output. Defaults to None.\n","        kernel_constraint (Optional[Union[str, keras.constraints.Constraint]], optional): Constraint for kernel weights. Defaults to None.\n","        bias_constraint (Optional[Union[str, keras.constraints.Constraint]], optional): Constraint for bias vector. Defaults to None.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","    \"\"\"\n","  def __init__(self,\n","    units: int,\n","    gate_activation: str = 'sigmoid',\n","    kernel_initializer: Union[str, keras.Initializer] = 'glorot_uniform',\n","    bias_initializer: Union[str, keras.Initializer] = 'zeros',\n","    kernel_regularizer: Optional[Union[str, keras.Regularizer]] = 'l2',\n","    bias_regularizer: Optional[Union[str, keras.Regularizer]] = None,\n","    activity_regularizer: Optional[Union[str, keras.Regularizer]] = None,\n","    kernel_constraint: Optional[Union[str, keras.constraints.Constraint]] = None,\n","    bias_constraint: Optional[Union[str, keras.constraints.Constraint]] = None,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","    self.linear = keras.layers.Dense(\n","      units=units,\n","      kernel_initializer=kernel_initializer,\n","      bias_initializer=bias_initializer,\n","      kernel_regularizer=kernel_regularizer,\n","      bias_regularizer=bias_regularizer,\n","      activity_regularizer=activity_regularizer,\n","      kernel_constraint=kernel_constraint,\n","      bias_constraint=bias_constraint,\n","    )\n","    self.gate = keras.layers.Dense(\n","      units=units,\n","      activation=gate_activation,\n","      kernel_initializer=kernel_initializer,\n","      bias_initializer=bias_initializer,\n","      kernel_regularizer=kernel_regularizer,\n","      bias_regularizer=bias_regularizer,\n","      activity_regularizer=activity_regularizer,\n","      kernel_constraint=kernel_constraint,\n","      bias_constraint=bias_constraint,\n","    )\n","\n","  def call(self, inputs):\n","    return self.linear(inputs) * self.gate(inputs)\n","\n","@keras.saving.register_keras_serializable()\n","class GatedResidualNetwork(keras.Layer):\n","  \"\"\"A Gated Residual Network layer implementation for Keras. Based on the Temporal Fusion Transformer of Lim et al. 2019 (https://arxiv.org/abs/1912.09363).\n","\n","    This layer implements a residual connection with a gating mechanism. It consists of\n","    a non-linear transformation followed by a linear transformation, with dropout and\n","    layer normalization applied to the output.\n","\n","    Args:\n","        units (int): The dimension of the output space.\n","        dropout (float, optional): Dropout rate. Defaults to 0.0.\n","        non_linear_activation (str, optional): Activation function for non-linear transformation. Defaults to 'elu'.\n","        glu_activation (str, optional): Activation function for the gating mechanism. Defaults to 'sigmoid'.\n","        kernel_initializer (Union[str, keras.Initializer], optional): Initializer for kernel weights. Defaults to 'glorot_uniform'.\n","        bias_initializer (Union[str, keras.Initializer], optional): Initializer for bias vectors. Defaults to 'zeros'.\n","        kernel_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for kernel weights. Defaults to 'l2'.\n","        bias_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for bias vectors. Defaults to None.\n","        activity_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for layer output. Defaults to None.\n","        kernel_constraint (Optional[Union[str, keras.constraints.Constraint]], optional): Constraint for kernel weights. Defaults to None.\n","        bias_constraint (Optional[Union[str, keras.constraints.Constraint]], optional): Constraint for bias vectors. Defaults to None.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","    \"\"\"\n","  def __init__(\n","    self,\n","    units: int,\n","    dropout: float = 0.0,\n","    non_linear_activation: str = 'elu',\n","    glu_activation: str = 'sigmoid',\n","    kernel_initializer: Union[str, keras.Initializer] = 'glorot_uniform',\n","    bias_initializer: Union[str, keras.Initializer] = 'zeros',\n","    kernel_regularizer: Optional[Union[str, keras.Regularizer]] = 'l2',\n","    bias_regularizer: Optional[Union[str, keras.Regularizer]] = None,\n","    activity_regularizer: Optional[Union[str, keras.Regularizer]] = None,\n","    kernel_constraint: Optional[Union[str, keras.constraints.Constraint]] = None,\n","    bias_constraint: Optional[Union[str, keras.constraints.Constraint]] = None,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","    self.units = units\n","    self.dropout = dropout\n","    self.non_linear_activation = non_linear_activation\n","    self.glu_activation = glu_activation\n","\n","    self.kernel_initializer = kernel_initializer\n","    self.bias_initializer = bias_initializer\n","    self.kernel_regularizer = kernel_regularizer\n","    self.bias_regularizer = bias_regularizer\n","    self.activity_regularizer = activity_regularizer\n","    self.kernel_constraint = kernel_constraint\n","    self.bias_constraint = bias_constraint\n","\n","  def build(self, input_shape: Tuple[int, int]):\n","    self.projection_dim = input_shape[-1]\n","\n","    self.non_linear_dense = keras.layers.Dense(\n","      self.units,\n","      activation=self.non_linear_activation,\n","      kernel_initializer=self.kernel_initializer,\n","      bias_initializer=self.bias_initializer,\n","      kernel_regularizer=self.kernel_regularizer,\n","      bias_regularizer=self.bias_regularizer,\n","      activity_regularizer=self.activity_regularizer,\n","      kernel_constraint=self.kernel_constraint,\n","      bias_constraint=self.bias_constraint,\n","    )\n","    self.linear_dense = keras.layers.Dense(\n","      self.units,\n","      activation='linear',\n","      kernel_initializer=self.kernel_initializer,\n","      bias_initializer=self.bias_initializer,\n","      kernel_regularizer=self.kernel_regularizer,\n","      bias_regularizer=self.bias_regularizer,\n","      activity_regularizer=self.activity_regularizer,\n","      kernel_constraint=self.kernel_constraint,\n","      bias_constraint=self.bias_constraint,\n","    )\n","    self.dense_dropout = keras.layers.Dropout(self.dropout)\n","    self.gated_linear_unit = GatedLinearUnit(\n","      self.units,\n","      self.glu_activation,\n","      kernel_initializer=self.kernel_initializer,\n","      bias_initializer=self.bias_initializer,\n","      kernel_regularizer=self.kernel_regularizer,\n","      bias_regularizer=self.bias_regularizer,\n","      activity_regularizer=self.activity_regularizer,\n","      kernel_constraint=self.kernel_constraint,\n","      bias_constraint=self.bias_constraint,\n","    )\n","\n","    if self.projection_dim != self.units:\n","      self.projection = keras.layers.Dense(\n","        self.units,\n","        kernel_initializer=self.kernel_initializer,\n","        bias_initializer=self.bias_initializer,\n","        kernel_regularizer=self.kernel_regularizer,\n","        bias_regularizer=self.bias_regularizer,\n","        activity_regularizer=self.activity_regularizer,\n","        kernel_constraint=self.kernel_constraint,\n","        bias_constraint=self.bias_constraint,\n","      )\n","    else:\n","      self.projection = keras.layers.Identity()\n","\n","    self.layer_norm = keras.layers.LayerNormalization()\n","\n","    self.built = True\n","\n","  def call(self, inputs):\n","    x = self.non_linear_dense(inputs)\n","    x = self.linear_dense(x)\n","    x = self.dense_dropout(x)\n","\n","    x = self.projection(inputs) + self.gated_linear_unit(x)\n","    x = self.layer_norm(x)\n","    return x\n","\n","@keras.saving.register_keras_serializable()\n","class VariableSelectionNetwork(keras.layers.Layer):\n","  \"\"\"A Variable Selection Network layer implementation for Keras. Based on the Temporal Fusion Transformer of Lim et al. 2019 (https://arxiv.org/abs/1912.09363).\n","\n","    This layer implements a network that learns to select the most relevant variables\n","    from a set of input features. It uses multiple Gated Residual Networks to process\n","    each input feature and combines them using learned weights.\n","\n","    Args:\n","        units (int): The dimension of the output space.\n","        dropout (float, optional): Dropout rate. Defaults to 0.0.\n","        grn_activation (str, optional): Activation function for the Gated Residual Networks.\n","            Defaults to 'elu'.\n","        grn_depth (int, optional): Number of Gated Residual Network layers to stack.\n","            Defaults to 0.\n","        glu_activation (str, optional): Activation function for the Gated Linear Units.\n","            Defaults to 'sigmoid'.\n","        kernel_initializer (Union[str, keras.Initializer], optional): Initializer for kernel weights.\n","            Defaults to 'glorot_uniform'.\n","        bias_initializer (Union[str, keras.Initializer], optional): Initializer for bias vectors.\n","            Defaults to 'zeros'.\n","        kernel_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for kernel weights.\n","            Defaults to 'l2'.\n","        bias_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for bias vectors.\n","            Defaults to None.\n","        activity_regularizer (Optional[Union[str, keras.Regularizer]], optional): Regularizer for layer output.\n","            Defaults to None.\n","        kernel_constraint (Optional[Union[str, keras.constraints.Constraint]], optional): Constraint for kernel weights.\n","            Defaults to None.\n","        bias_constraint (Optional[Union[str, keras.constraints.Constraint]], optional): Constraint for bias vectors.\n","            Defaults to None.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","    \"\"\"\n","  def __init__(\n","    self,\n","    units: int,\n","    dropout: float = 0.0,\n","    grn_activation: str = 'elu',\n","    grn_depth: int = 0,\n","    glu_activation: str = 'sigmoid',\n","    kernel_initializer: Union[str, keras.Initializer] = 'glorot_uniform',\n","    bias_initializer: Union[str, keras.Initializer] = 'zeros',\n","    kernel_regularizer: Optional[Union[str, keras.Regularizer]] = 'l2',\n","    bias_regularizer: Optional[Union[str, keras.Regularizer]] = None,\n","    activity_regularizer: Optional[Union[str, keras.Regularizer]] = None,\n","    kernel_constraint: Optional[Union[str, keras.constraints.Constraint]] = None,\n","    bias_constraint: Optional[Union[str, keras.constraints.Constraint]] = None,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    self.units = units\n","    self.dropout = dropout\n","    self.grn_activation = grn_activation\n","    self.grn_depth = grn_depth\n","    self.glu_activation = glu_activation\n","\n","    self.kernel_initializer = kernel_initializer\n","    self.bias_initializer = bias_initializer\n","    self.kernel_regularizer = kernel_regularizer\n","    self.bias_regularizer = bias_regularizer\n","    self.activity_regularizer = activity_regularizer\n","    self.kernel_constraint = kernel_constraint\n","    self.bias_constraint = bias_constraint\n","\n","  def build(self, input_shape: List[Tuple[int, int]]):\n","    self.num_feats = len(input_shape)\n","\n","    self.gated_res_nets = []\n","    for feat_i in range(self.num_feats):\n","      grn = keras.models.Sequential()\n","      for _ in range(self.grn_depth):\n","        grn.add(GatedResidualNetwork(\n","          input_shape[feat_i][-1],\n","          self.dropout,\n","          self.grn_activation,\n","          self.glu_activation,\n","          kernel_initializer=self.kernel_initializer,\n","          bias_initializer=self.bias_initializer,\n","          kernel_regularizer=self.kernel_regularizer,\n","          bias_regularizer=self.bias_regularizer,\n","          activity_regularizer=self.activity_regularizer,\n","          kernel_constraint=self.kernel_constraint,\n","          bias_constraint=self.bias_constraint,\n","        ))\n","\n","      grn.add(GatedResidualNetwork(\n","        self.units,\n","        self.dropout,\n","        self.grn_activation,\n","        self.glu_activation,\n","        kernel_initializer=self.kernel_initializer,\n","        bias_initializer=self.bias_initializer,\n","        kernel_regularizer=self.kernel_regularizer,\n","        bias_regularizer=self.bias_regularizer,\n","        activity_regularizer=self.activity_regularizer,\n","        kernel_constraint=self.kernel_constraint,\n","        bias_constraint=self.bias_constraint,\n","      ))\n","      self.gated_res_nets.append(grn)\n","\n","    self.gated_res_net_concat = GatedResidualNetwork(\n","      self.units,\n","      self.dropout,\n","      self.grn_activation,\n","      self.glu_activation,\n","      kernel_initializer=self.kernel_initializer,\n","      bias_initializer=self.bias_initializer,\n","      kernel_regularizer=self.kernel_regularizer,\n","      bias_regularizer=self.bias_regularizer,\n","      activity_regularizer=self.activity_regularizer,\n","      kernel_constraint=self.kernel_constraint,\n","      bias_constraint=self.bias_constraint,\n","    )\n","    self.dense = keras.layers.Dense(units=self.num_feats, activation='linear')\n","    self.softmax = keras.layers.Softmax()\n","\n","    self.built = True\n","\n","  def call(self, inputs, mask=None):\n","    v = keras.layers.concatenate(inputs)\n","    v = self.gated_res_net_concat(v)\n","    v = self.dense(v)\n","    v = self.softmax(v, mask=mask)\n","    v = keras.ops.expand_dims(v, axis=-1)\n","\n","    x = []\n","    #keras.ops.fori_loop(0, self.num_feats, lambda i, x_arr: (i + 1, x_arr + [self.gated_res_nets[i](inputs[i])]), [])\n","    for i, input in enumerate(inputs):\n","      x.append(self.gated_res_nets[i](input))\n","\n","    x = keras.ops.stack(x, axis=1)\n","\n","    outputs = keras.ops.squeeze(\n","      keras.ops.matmul(keras.ops.swapaxes(v, 1, 2), x),\n","      #tf.matmul(v, x, transpose_a=True),\n","      axis=1,\n","    )\n","    return outputs"]},{"cell_type":"markdown","metadata":{"id":"WwtFIFHzaORQ"},"source":["### Embedding Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86FAutTwEhN0"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class StringEmbedding(keras.Layer):\n","  def __init__(\n","    self,\n","    embed_dim: int,\n","    vocab: List[str],\n","    embed_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    mask_zero: bool = True,\n","    num_oov_indices: int = 1,\n","    mask_token: str = \"[PAD]\",\n","    oov_token: str = \"[UNK]\",\n","    output_mode: str = 'int',\n","    pad_to_max_tokens: bool = False,\n","    name: Optional[str] = None,\n","  ):\n","    super().__init__(name=name)\n","\n","    self.embed_dim = embed_dim\n","    self.vocab = vocab\n","    self.embed_init = embed_init\n","    self.mask_zero = mask_zero\n","    self.num_oov_indices = num_oov_indices\n","    self.mask_token = mask_token\n","    self.oov_token = oov_token\n","    self.output_mode = output_mode\n","    self.pad_to_max_tokens = pad_to_max_tokens\n","\n","    self.lookup = keras.layers.StringLookup(\n","      num_oov_indices=self.num_oov_indices,\n","      mask_token=self.mask_token,\n","      oov_token=self.oov_token,\n","      output_mode=self.output_mode,\n","      pad_to_max_tokens=self.pad_to_max_tokens,\n","    )\n","    self.lookup.adapt(self.vocab)\n","\n","    self.embedding = keras.layers.Embedding(\n","      input_dim=len(self.lookup.get_vocabulary()),\n","      output_dim=self.embed_dim,\n","      embeddings_initializer=self.embed_init,\n","      mask_zero=self.mask_zero,\n","    )\n","\n","  def build(self, input_shape):\n","    self.built = True\n","\n","  def get_config(self):\n","    base_config = super().get_config()\n","    config = {\n","      \"embed_dim\": self.embed_dim,\n","      \"vocab\": json.dumps(np.array(self.vocab).tolist()),\n","      \"embed_init\": self.embed_init,\n","      \"mask_zero\": self.mask_zero,\n","      \"num_oov_indices\": self.num_oov_indices,\n","      \"mask_token\": self.mask_token,\n","      \"oov_token\": self.oov_token,\n","      \"output_mode\": self.output_mode,\n","      \"pad_to_max_tokens\": self.pad_to_max_tokens,\n","      \"name\": self.name,\n","    }\n","    return {**base_config, **config}\n","\n","  @classmethod\n","  def from_config(cls, config):\n","    embed_dim = config[\"embed_dim\"]\n","    vocab = json.loads(config[\"vocab\"])\n","    embed_init = config[\"embed_init\"]\n","    mask_zero = config[\"mask_zero\"]\n","    num_oov_indices = config[\"num_oov_indices\"]\n","    mask_token = config[\"mask_token\"]\n","    oov_token = config[\"oov_token\"]\n","    output_mode = config[\"output_mode\"]\n","    pad_to_max_tokens = config[\"pad_to_max_tokens\"]\n","    name = config[\"name\"]\n","    return cls(embed_dim, vocab, embed_init, mask_zero, num_oov_indices, mask_token, oov_token, output_mode, pad_to_max_tokens, name)\n","\n","  def compute_mask(self, input, mask=None):\n","    input = self.lookup(input)\n","    return self.embedding.compute_mask(input, mask=mask)\n","\n","  def call(self, inputs):\n","    x = self.lookup(inputs)\n","    x = self.embedding(x)\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGYmA3dcNyNX"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class NumericalEmbedding(keras.Layer):\n","  def __init__(\n","    self,\n","    embed_dim: int,\n","    vocab: List[float] = [],\n","    kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    bias_init: Union[str, keras.Initializer] = 'zeros',\n","    mask_token: Optional[float] = -1.0,\n","    name: Optional[str] = None,\n","  ):\n","    super().__init__(name=name)\n","\n","    self.embed_dim = embed_dim\n","    self.vocab = vocab\n","    self.kernel_init = kernel_init\n","    self.bias_init = bias_init\n","    self.mask_token = mask_token\n","\n","    if self.vocab is not None and len(self.vocab) > 0:\n","      self.norm = keras.layers.Normalization()\n","      self.norm.adapt(self.vocab)\n","    else:\n","      self.norm = keras.layers.Identity()\n","\n","    if self.mask_token is not None:\n","      self.masking = keras.layers.Masking(mask_value=self.mask_token)\n","    else:\n","      self.masking = keras.layers.Identity()\n","\n","    self.dense = keras.layers.Dense(\n","      units=self.embed_dim,\n","      activation='linear',\n","      kernel_initializer=self.kernel_init,\n","      bias_initializer=self.bias_init,\n","    )\n","\n","  def build(self, input_shape):\n","    self.built = True\n","\n","  def get_config(self):\n","    base_config = super().get_config()\n","    config = {\n","      \"embed_dim\": self.embed_dim,\n","      \"vocab\": None if self.vocab is None else json.dumps(np.array(self.vocab, dtype='float32').tolist()),\n","      \"kernel_init\": self.kernel_init,\n","      \"bias_init\": self.bias_init,\n","      \"mask_token\": self.mask_token,\n","      \"name\": self.name,\n","    }\n","    return {**base_config, **config}\n","\n","  @classmethod\n","  def from_config(cls, config):\n","    embed_dim = config[\"embed_dim\"]\n","    vocab = None if config[\"vocab\"] is None else json.loads(config[\"vocab\"])\n","    kernel_init = config[\"kernel_init\"]\n","    bias_init = config[\"bias_init\"]\n","    mask_token = config[\"mask_token\"]\n","    name = config[\"name\"]\n","    return cls(embed_dim, vocab, kernel_init, bias_init, mask_token, name)\n","\n","  def compute_mask(self, inputs, mask=None):\n","    x = keras.ops.expand_dims(inputs, axis=-1)\n","    x = self.norm(x)\n","    return self.masking.compute_mask(x, mask=mask)\n","\n","  def call(self, inputs):\n","    x = keras.ops.expand_dims(inputs, axis=-1)\n","    x = self.norm(x)\n","    x = self.masking(x)\n","    return self.dense(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcJYb8ZgQLq3"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class TemporalEncoding(keras.layers.Layer):\n","  def __init__(\n","    self,\n","    embed_init: Union[str, keras.Initializer] = 'uniform',\n","    embed_reg: Optional[keras.Regularizer] = None,\n","    embed_con: Optional[keras.constraints.Constraint] = None,\n","  ):\n","    super().__init__()\n","\n","    self.embed_init = embed_init\n","    self.embed_reg = embed_reg\n","    self.embed_con = embed_con\n","\n","    self.month_vocab_len = 13\n","    self.yearday_vocab_len = 367\n","    self.day_vocab_len = 32\n","    self.weekday_vocab_len = 7\n","    self.hour_vocab_len = 24\n","    self.min_vocab_len = 60\n","    self.sec_vocab_len = 60\n","\n","  def build(self, input_shape: List[Tuple[int, int]]):\n","    self.embed_dim = input_shape[-1]\n","\n","    self.month_embedding = keras.layers.Embedding(\n","      input_dim=self.month_vocab_len,\n","      output_dim=self.embed_dim,\n","      embeddings_initializer=self.embed_init,\n","      embeddings_regularizer=self.embed_reg,\n","      embeddings_constraint=self.embed_con,\n","      mask_zero=False,\n","      name=\"month_embedding\",\n","    )\n","\n","    self.yearday_embedding = keras.layers.Embedding(\n","      input_dim=self.yearday_vocab_len,\n","      output_dim=self.embed_dim,\n","      embeddings_initializer=self.embed_init,\n","      embeddings_regularizer=self.embed_reg,\n","      embeddings_constraint=self.embed_con,\n","      mask_zero=False,\n","      name=\"dayinyear_embedding\",\n","    )\n","\n","    self.day_embedding = keras.layers.Embedding(\n","      input_dim=self.day_vocab_len,\n","      output_dim=self.embed_dim,\n","      embeddings_initializer=self.embed_init,\n","      embeddings_regularizer=self.embed_reg,\n","      embeddings_constraint=self.embed_con,\n","      mask_zero=False,\n","      name=\"dayinmonth_embedding\",\n","    )\n","\n","    self.weekday_embedding = keras.layers.Embedding(\n","      input_dim=self.weekday_vocab_len,\n","      output_dim=self.embed_dim,\n","      embeddings_initializer=self.embed_init,\n","      embeddings_regularizer=self.embed_reg,\n","      embeddings_constraint=self.embed_con,\n","      mask_zero=False,\n","      name=\"weekday_embedding\",\n","    )\n","\n","    self.hour_embedding = keras.layers.Embedding(\n","      input_dim=self.hour_vocab_len,\n","      output_dim=self.embed_dim,\n","      embeddings_initializer=self.embed_init,\n","      embeddings_regularizer=self.embed_reg,\n","      embeddings_constraint=self.embed_con,\n","      mask_zero=False,\n","      name=\"hour_embedding\",\n","    )\n","\n","    self.min_embedding = keras.layers.Embedding(\n","      input_dim=self.min_vocab_len,\n","      output_dim=self.embed_dim,\n","      embeddings_initializer=self.embed_init,\n","      embeddings_regularizer=self.embed_reg,\n","      embeddings_constraint=self.embed_con,\n","      mask_zero=False,\n","      name=\"minute_embedding\",\n","    )\n","\n","    self.sec_embedding = keras.layers.Embedding(\n","      input_dim=self.sec_vocab_len,\n","      output_dim=self.embed_dim,\n","      embeddings_initializer=self.embed_init,\n","      embeddings_regularizer=self.embed_reg,\n","      embeddings_constraint=self.embed_con,\n","      mask_zero=False,\n","      name=\"second_embedding\",\n","    )\n","\n","  def call(self, inputs, inputs_month=None, inputs_yearday=None, inputs_day=None, inputs_weekday=None, inputs_hour=None, inputs_minute=None, inputs_second=None):\n","    x_month = 0 if inputs_month is None else self.month_embedding(inputs_month)\n","    x_yearday = 0 if inputs_yearday is None else self.yearday_embedding(inputs_yearday)\n","    x_day = 0 if inputs_day is None else self.day_embedding(inputs_day)\n","    x_weekday = 0 if inputs_weekday is None else self.weekday_embedding(inputs_weekday)\n","    x_hour = 0 if inputs_hour is None else self.hour_embedding(inputs_hour)\n","    x_minute = 0 if inputs_minute is None else self.minute_embedding(inputs_minute)\n","    x_second = 0 if inputs_second is None else self.sec_embedding(inputs_second)\n","\n","    return x_month + x_yearday + x_day + x_weekday + x_hour + x_minute + x_second"]},{"cell_type":"markdown","metadata":{"id":"zKK3wwgzkexr"},"source":["## RNN"]},{"cell_type":"markdown","metadata":{"id":"eJ9ofoPjR6l2"},"source":["### ProcessLSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqUS5D6SR5_S"},"outputs":[],"source":["class BaseProcessRNN(keras.Model, LayerNameMixin, BaseModelMixin):\n","  \"\"\"A base RNN model for process mining tasks with shared and task-specific layers. Provides a structured and more flexible reimplementation of the work by Tax et al. 2017 (https://github.com/verenich/ProcessSequencePrediction).\n","\n","  The default parameters represent the original model architecture with its hyperparameters.\n","\n","  This model implements a neural architecture for process mining that combines:\n","  - Multiple input features (activities, temporal features)\n","  - Shared RNN layers for common feature extraction\n","  - Task-specific RNN layers for different prediction tasks\n","  - Configurable normalization and feed-forward layers\n","\n","  Args:\n","      max_case_len (int): Maximum length of process cases/sequences\n","      activity_vocab (List[str]): Vocabulary of possible process activities\n","      output_dict (Dict[str, dict]): Configuration for output layers. Format: {\"task_name\": {\"units\": int, \"activation\": str}}, Example: {\"remaining_time\": {'units': 1, 'activation': 'relu'}, \"next_activity\": {'units': 10, 'activation': 'softmax'}}\n","      activity_encoding_type (str, optional): Type of activity encoding. Options: \"embedding\" or \"one_hot\". Defaults to \"one_hot\"\n","      activity_embedding_dim (int): Dimension for activity embeddings. Only used if activity_encoding_type is \"embedding\". Defaults to 16\n","      time_encoding_type (str, optional): Type of time feature encoding. Options: \"mean\", \"minmax\", or \"normal\". Defaults to \"mean\"\n","      interval_since_last_event_vocab (List[float], optional): Vocabulary for time intervals\n","      hour_of_day_vocab (List[float], optional): Vocabulary for hour features\n","      day_of_week_vocab (List[float], optional): Vocabulary for day of week features\n","      rnn_units_shared (List[int]): Units in each shared RNN layer. Defaults to [100]\n","      rnn_units_nonshared (List[int]): Units in each task-specific RNN layer. Defaults to [100]\n","      rnn_type (str): Type of RNN cell. Options: \"lstm\", \"gru\", \"rnn\". Defaults to \"lstm\"\n","      rnn_bidirectional (bool): Whether to use bidirectional RNNs. Defaults to False\n","      rnn_activation (str): Activation function for RNN cells. Defaults to \"tanh\"\n","      rnn_recurrent_activation (str): Recurrent activation for LSTM/GRU. Defaults to \"sigmoid\"\n","      rnn_go_backwards (bool): Whether to process sequence backwards. Defaults to False\n","      rnn_unroll (bool): Whether to unroll RNN computation. Defaults to False\n","      rnn_recurrent_dropout (float): Dropout rate for recurrent connections. Defaults to 0.0\n","      rnn_dropout (float): Dropout rate for RNN inputs. Defaults to 0.0\n","      ff_dims (List[int]): Units in each feed-forward layer. Defaults to []\n","      ff_activation (str): Activation for feed-forward layers. Defaults to \"relu\"\n","      ff_dropout (float): Dropout rate for feed-forward layers. Defaults to 0.0\n","      norm_type (str, optional): Type of normalization. Options: \"layer\" or \"batch\". Defaults to \"batch\"\n","      norm_epsilon (float): Small constant for numerical stability. Defaults to 0.001\n","      norm_center (bool): Whether to subtract mean in normalization. Defaults to True\n","      norm_scale (bool): Whether to multiply by gamma in normalization. Defaults to True\n","      numeric_mask_token (float): Token used for masking numeric values. Defaults to TOKEN_PAD_NUM\n","      mask_token (str): Token used for masking categorical values. Defaults to TOKEN_PAD\n","      oov_token (str): Token used for out-of-vocabulary values. Defaults to TOKEN_OOV\n","\n","  Author:\n","      Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  def __init__(\n","    self,\n","    max_case_len: int,\n","    activity_vocab: List[str],\n","    output_dict: Dict[str, dict],\n","    activity_encoding_type: Optional[Literal['embedding', 'one_hot']] = 'one_hot',\n","    activity_embedding_dim: int = 16,\n","    time_encoding_type: Optional[Literal['mean','minmax', 'normal']] = 'mean',\n","    interval_since_last_event_vocab: Optional[List[float]] = None,\n","    hour_of_day_vocab: Optional[List[float]] = None,\n","    day_of_week_vocab: Optional[List[float]] = None,\n","    rnn_units_shared: List[int] = [100],\n","    rnn_units_nonshared: List[int] = [100],\n","    rnn_type: Literal['lstm', 'gru', 'rnn'] = 'lstm',\n","    rnn_bidirectional: bool = False,\n","    rnn_activation: str = 'tanh',\n","    rnn_recurrent_activation: str = 'sigmoid',\n","    rnn_go_backwards: bool = False,\n","    rnn_unroll: bool = False,\n","    rnn_recurrent_dropout: float = 0.0,\n","    rnn_dropout: float = 0.0,\n","\n","    ff_dims: List[int] = [],\n","    ff_activation: str = 'relu',\n","    ff_dropout: float = 0.0,\n","\n","    norm_type: Optional[Literal['layer', 'batch']] = 'batch',\n","    norm_epsilon: float = 0.001,\n","    norm_center: bool = True,\n","    norm_scale: bool = True,\n","\n","    numeric_mask_token: float = TOKEN_PAD_NUM,\n","    mask_token: str = TOKEN_PAD,\n","    oov_token: str = TOKEN_OOV,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    self.max_case_len = max_case_len\n","    self.output_dict = output_dict\n","    self.activity_vocab = activity_vocab\n","    self.activity_encoding_type = activity_encoding_type\n","    self.activity_embedding_dim = activity_embedding_dim\n","\n","    self.rnn_units_shared = rnn_units_shared\n","    self.rnn_units_nonshared = rnn_units_nonshared\n","    self.rnn_type = rnn_type\n","\n","    self.rnn_activation = rnn_activation\n","    self.rnn_recurrent_activation = rnn_recurrent_activation\n","    self.rnn_bidirectional = rnn_bidirectional\n","    self.rnn_go_backwards = rnn_go_backwards\n","    self.rnn_unroll = rnn_unroll\n","    self.rnn_recurrent_dropout = rnn_recurrent_dropout\n","    self.rnn_dropout = rnn_dropout\n","\n","    self.norm_type = norm_type\n","    self.norm_epsilon = norm_epsilon\n","    self.norm_center = norm_center\n","    self.norm_scale = norm_scale\n","\n","    self.ff_dims = ff_dims\n","    self.ff_activation = ff_activation\n","    self.ff_dropout = ff_dropout\n","\n","    self.numeric_mask_token = numeric_mask_token\n","    self.mask_token = mask_token\n","    self.oov_token = oov_token\n","\n","    self.activity_encoding = self._build_activity_encoding_layer()\n","\n","    self.time_encoding_type = time_encoding_type\n","    self.interval_since_last_event_encoding = self._build_time_encoding_layer(vocab=interval_since_last_event_vocab, name=\"interval_since_last_event\")\n","    self.hour_of_day_encoding = self._build_time_encoding_layer(vocab=hour_of_day_vocab, name=\"hour_of_day\")\n","    self.day_of_week_encoding = self._build_time_encoding_layer(vocab=day_of_week_vocab, name=\"day_of_week\")\n","\n","    self.shared_rnns = self._build_hard_shared_rnn_layer()\n","\n","    self.non_shared_rnns = []\n","    self.ffns = []\n","    self.outputs = []\n","    for output_name, output_config in self.output_dict.items():\n","      self.non_shared_rnns.append(self._build_soft_shared_rnn_layer(name=output_name))\n","      self.ffns.append(self._build_ffn_layer(name=output_name))\n","      self.outputs.append(keras.layers.Dense(units=output_config['units'], activation=output_config['activation'], name=output_name))\n","\n","  @property\n","  def default_loss(self) -> Dict[str, keras.Loss]:\n","    return {k: self.find_default_loss(v['activation']) for k, v in self.output_dict.items()}\n","\n","  @property\n","  def default_metrics(self) -> Dict[str, List[keras.Metric]]:\n","    return {k: self.find_default_metrics(v['activation']) for k, v in self.output_dict.items()}\n","\n","  @property\n","  def default_learning_rate(self) -> Union[float, keras.optimizers.schedules.LearningRateSchedule]:\n","    return 0.002\n","\n","  @property\n","  def default_optimizer(self) -> keras.optimizers.Optimizer:\n","    return keras.optimizers.Nadam(learning_rate=self.default_learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, weight_decay=0.004, clipvalue=3)\n","\n","  @property\n","  def default_regression_loss(self) -> keras.Loss:\n","    return keras.losses.MeanAbsoluteError()\n","\n","  def _build_time_encoding_layer(self, vocab: Optional[List[float]] = None, name: Optional[str] = None, **kwargs) -> keras.layers.Layer:\n","    \"\"\"Builds a time encoding layer based on the specified encoding type.\n","\n","    Args:\n","        vocab (List[float], optional): Vocabulary of time values for adaptation\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layers\n","\n","    Returns:\n","        keras.layers.Layer: Sequential layer for time feature encoding\n","\n","    Raises:\n","        NotImplementedError: If time_encoding_type is not one of [None, \"mean\", \"minmax\", \"normal\"]\n","    \"\"\"\n","    name = self._generate_layer_name(f\"time_encoding_{self.time_encoding_type}\", name)\n","    layer = keras.models.Sequential([keras.layers.Masking(self.numeric_mask_token)], name=name)\n","\n","    if self.time_encoding_type == None:\n","      layer.add(keras.layers.Identity(name=name))\n","    elif 'mean' == self.time_encoding_type:\n","      lookup = MeanScaler(axis=None)\n","      lookup.adapt(vocab)\n","      layer.add(lookup)\n","    elif 'minmax' == self.time_encoding_type:\n","      lookup = MinMaxScaler(axis=None)\n","      lookup.adapt(vocab)\n","      layer.add(lookup)\n","    elif 'normal' == self.time_encoding_type:\n","      lookup = keras.layers.Normalization(axis=None)\n","      lookup.adapt(vocab)\n","      layer.add(lookup)\n","    else:\n","      raise NotImplementedError(f\"Unknown time encoding type {self.time_encoding_type}\")\n","\n","    layer.add(keras.layers.Reshape((-1, 1)))\n","\n","    return layer\n","\n","  def _build_activity_encoding_layer(self, name: Optional[str] = None, **kwargs) -> keras.layers.Layer:\n","    \"\"\"Builds an activity encoding layer based on the specified encoding type.\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layers\n","\n","    Returns:\n","        keras.layers.Layer: Layer for activity encoding (Identity, OneHot, or Embedding)\n","\n","    Raises:\n","        NotImplementedError: If activity_encoding_type is not one of [None, \"one_hot\", \"embedding\"]\n","    \"\"\"\n","    name = self._generate_layer_name(f\"activity_encoding_{self.activity_encoding_type}\", name)\n","\n","    if self.activity_encoding_type is None:\n","      layer = keras.layers.Identity(name=name)\n","    elif 'one_hot' == self.activity_encoding_type:\n","      lookup = keras.layers.StringLookup(\n","        mask_token=self.mask_token,\n","        oov_token=self.oov_token,\n","        output_mode='one_hot',\n","        pad_to_max_tokens=False,\n","      )\n","      lookup.adapt(self.activity_vocab)\n","\n","      # Workaround: https://github.com/keras-team/keras/issues/19191#issuecomment-2077711036\n","      lookup.build()\n","      layer = keras.models.Sequential([\n","        keras.layers.Reshape((-1, 1), dtype='string'),\n","        keras.layers.TimeDistributed(lookup)\n","      ], name=name)\n","\n","    elif 'embedding' == self.activity_encoding_type:\n","      lookup = keras.layers.StringLookup(\n","        mask_token=self.mask_token,\n","        oov_token=self.oov_token,\n","        output_mode='int',\n","        pad_to_max_tokens=False,\n","      )\n","      lookup.adapt(self.activity_vocab)\n","\n","      layer = keras.models.Sequential([\n","        lookup,\n","        keras.layers.Embedding(\n","          input_dim=len(lookup.get_vocabulary()),\n","          output_dim=self.activity_embedding_dim,\n","          mask_zero=True,\n","        )\n","      ], name=name)\n","    else:\n","      raise NotImplementedError(f\"Unknown activity encoding type {self.activity_encoding_type}\")\n","\n","    return layer\n","\n","  def _build_hard_shared_rnn_layer(self, name: Optional[str] = None, **kwargs) -> keras.Layer:\n","    \"\"\"Builds the shared RNN layers used across all tasks (hard parameter sharing).\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layers\n","\n","    Returns:\n","        keras.Layer: Sequential model containing shared RNN layers with normalization\n","    \"\"\"\n","    name = self._generate_layer_name(f\"hardsharing_{self.rnn_type}\", name)\n","    layer = keras.models.Sequential(name=name)\n","\n","    for units in self.rnn_units_shared:\n","      layer.add(self._build_rnn_layer(units, return_sequences=True))\n","      layer.add(self._build_norm_layer())\n","\n","    return layer\n","\n","  def _build_soft_shared_rnn_layer(self, name: Optional[str] = None, **kwargs) -> keras.Layer:\n","    \"\"\"Builds task-specific RNN layers (soft parameter sharing).\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layers\n","\n","    Returns:\n","        keras.Layer: Sequential model containing task-specific RNN layers with normalization\n","    \"\"\"\n","    name = self._generate_layer_name(f\"softsharing_{self.rnn_type}\", name)\n","    layer = keras.models.Sequential(name=name)\n","\n","    for i, units in enumerate(self.rnn_units_nonshared):\n","      return_sequences = False if i == len(self.rnn_units_nonshared) - 1 else True\n","\n","      layer.add(self._build_rnn_layer(units, return_sequences=return_sequences))\n","      layer.add(self._build_norm_layer())\n","\n","    return layer\n","\n","  def _build_rnn_layer(self, units: int, **kwargs) -> keras.Layer:\n","    \"\"\"Builds a single RNN layer based on the specified RNN type.\n","\n","    Args:\n","        units (int): Number of output units\n","        **kwargs: Additional keyword arguments passed to the RNN layer\n","\n","    Returns:\n","        keras.Layer: RNN layer (LSTM, SimpleRNN, or GRU)\n","\n","    Raises:\n","        NotImplementedError: If rnn_type is not one of [\"lstm\", \"rnn\", \"gru\"]\n","    \"\"\"\n","    if 'lstm' == self.rnn_type:\n","      layer = keras.layers.LSTM(\n","        units=units,\n","        activation=self.rnn_activation,\n","        recurrent_activation=self.rnn_recurrent_activation,\n","        dropout=self.rnn_dropout,\n","        recurrent_dropout=self.rnn_recurrent_dropout,\n","        unroll=self.rnn_unroll,\n","        go_backwards=self.rnn_go_backwards,\n","        seed=RANDOM_STATE,\n","        **kwargs\n","      )\n","    elif 'rnn' == self.rnn_type:\n","      layer = keras.layers.SimpleRNN(\n","        units=units,\n","        activation=self.rnn_activation,\n","        dropout=self.rnn_dropout,\n","        recurrent_dropout=self.rnn_recurrent_dropout,\n","        unroll=self.rnn_unroll,\n","        go_backwards=self.rnn_go_backwards,\n","        seed=RANDOM_STATE,\n","        **kwargs\n","      )\n","    elif 'gru' == self.rnn_type:\n","      layer = keras.layers.GRU(\n","        units=units,\n","        activation=self.rnn_activation,\n","        recurrent_activation=self.rnn_recurrent_activation,\n","        dropout=self.rnn_dropout,\n","        recurrent_dropout=self.rnn_recurrent_dropout,\n","        unroll=self.rnn_unroll,\n","        go_backwards=self.rnn_go_backwards,\n","        seed=RANDOM_STATE,\n","        **kwargs\n","      )\n","    else:\n","      raise NotImplementedError(f\"Unknown RNN type {self.rnn_type}\")\n","\n","    if self.rnn_bidirectional:\n","      layer = keras.layers.Bidirectional(layer)\n","\n","    return layer\n","\n","  def _build_norm_layer(self, **kwargs) -> keras.Layer:\n","    \"\"\"Builds a normalization layer based on the specified normalization type.\n","\n","    Args:\n","        **kwargs: Additional keyword arguments passed to the normalization layer\n","\n","    Returns:\n","        keras.Layer: Normalization layer (BatchNorm, LayerNorm, or Identity)\n","\n","    Raises:\n","        NotImplementedError: If norm_type is not one of [None, \"batch\", \"layer\"]\n","    \"\"\"\n","    if self.norm_type is None:\n","      layer = keras.layers.Identity()\n","    elif self.norm_type == 'batch':\n","      layer = keras.layers.BatchNormalization(\n","        epsilon=self.norm_epsilon,\n","        center=self.norm_center,\n","        scale=self.norm_scale,\n","        **kwargs\n","      )\n","    elif self.norm_type == 'layer':\n","      layer = keras.layers.LayerNormalization(\n","        epsilon=self.norm_epsilon,\n","        center=self.norm_center,\n","        scale=self.norm_scale,\n","        **kwargs\n","      )\n","    else:\n","      raise NotImplementedError(f\"Unknown normalization type {self.norm_type}\")\n","\n","    return layer\n","\n","  def _build_ffn_layer(self, name: Optional[str] = None, **kwargs) -> keras.layers.Layer:\n","    \"\"\"Builds feed-forward layers with optional dropout.\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layers\n","\n","    Returns:\n","        keras.layers.Layer: Sequential model containing feed-forward layers\n","    \"\"\"\n","    name = self._generate_layer_name(\"ffn\", name)\n","    layer = keras.models.Sequential([keras.layers.Identity()], name=name)\n","\n","    for ff_dim in self.ff_dims:\n","      layer.add(keras.layers.Dense(units=ff_dim, activation=self.ff_activation))\n","      layer.add(keras.layers.Dropout(self.ff_dropout))\n","\n","    return layer\n","\n","  def build_graph(self) -> keras.Model:\n","    \"\"\"Builds and returns a Keras Model instance with the defined architecture.\n","\n","    Returns:\n","        keras.Model: Compiled model with defined inputs and outputs\n","    \"\"\"\n","    in_activity = keras.layers.Input(shape=(self.max_case_len,), name=\"activity\", dtype='string')\n","    in_interval_since_last_event = keras.layers.Input(shape=(self.max_case_len,), name=\"time_timestamp_elapsedprev\", dtype='float32')\n","    in_hour_of_day = keras.layers.Input(shape=(self.max_case_len,), name=\"time_timestamp_hour_raw\", dtype='float32')\n","    in_day_of_week = keras.layers.Input(shape=(self.max_case_len,), name=\"time_timestamp_weekday_raw\", dtype='float32')\n","\n","    inputs = {\n","        \"activity\": in_activity,\n","        \"time_timestamp_elapsedprev\": in_interval_since_last_event,\n","        \"time_timestamp_hour_raw\": in_hour_of_day,\n","        \"time_timestamp_weekday_raw\": in_day_of_week,\n","    }\n","\n","    return keras.Model(inputs=inputs, outputs=self.call(in_activity, in_interval_since_last_event, in_hour_of_day, in_day_of_week), name=self.name)\n","\n","  def call(self, inputs, inputs_interval_since_last_event, inputs_hour_of_day, inputs_day_of_week, training=False):\n","    \"\"\"Forward pass of the model.\n","\n","    Args:\n","        inputs: Activity input tensor\n","        inputs_interval_since_last_event: Time interval input tensor\n","        inputs_hour_of_day: Hour of day input tensor\n","        inputs_day_of_week: Day of week input tensor\n","        training (bool, optional): Whether in training mode. Defaults to False\n","\n","    Returns:\n","        List[tf.Tensor]: List of output tensors for each task\n","    \"\"\"\n","    x_activity = self.activity_encoding(inputs)\n","    x_interval = self.interval_since_last_event_encoding(inputs_interval_since_last_event)\n","    x_hour = self.hour_of_day_encoding(inputs_hour_of_day)\n","    x_day = self.day_of_week_encoding(inputs_day_of_week)\n","\n","    x = keras.layers.concatenate([x_activity, x_interval, x_hour, x_day])\n","\n","    x = self.shared_rnns(x)\n","\n","    x_outputs = []\n","    for rnn, ffn, out in zip(self.non_shared_rnns, self.ffns, self.outputs, strict=True):\n","      x_out = rnn(x)\n","      x_out = ffn(x_out)\n","      x_out = out(x_out)\n","      x_outputs.append(x_out)\n","\n","    return x_outputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaJl1j0D1S7C"},"outputs":[],"source":["lstm = BaseProcessRNN(\n","    max_case_len=3,\n","    activity_vocab=['a', 'b', 'c'],\n","    interval_since_last_event_vocab=np.array([1,2,3]),\n","    hour_of_day_vocab=np.array([1,2,3]),\n","    day_of_week_vocab=np.array([1,2,3]),\n","    output_dict={\"remaining_time\": {'units': 1, 'activation': 'relu'}, \"next_activity\": {'units': 10, 'activation': 'softmax'}},\n","    activity_encoding_type='one_hot'\n",")\n","\n","lstm = lstm.build_graph()\n","lstm.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2W1wOGCBD0K"},"outputs":[],"source":["keras.utils.plot_model(\n","  lstm,\n","  show_shapes=True,\n","  show_dtype=True,\n","  show_layer_names=True,\n","  expand_nested=True,\n","  show_layer_activations=True,\n","  show_trainable=True,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"0eCiTj0T6DsK"},"source":["## Transformer"]},{"cell_type":"markdown","metadata":{"id":"F_qOua72d6Qc"},"source":["### ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1dfLVNm2PYw"},"outputs":[],"source":["class BaseProcessTransformer(keras.Model, LayerNameMixin, BaseModelMixin):\n","  \"\"\"A base Transformer model for process mining tasks with shared and task-specific layers. Provides a structured and more flexible reimplementation of the work by Bukhsh et al. 2021 (https://github.com/Zaharah/processtransformer).\n","\n","  The default parameters represent the original model architecture with its hyperparameters.\n","\n","  This model implements a neural architecture for process mining that combines:\n","  - Multiple input features (activities, temporal features)\n","  - Positional encodings for sequence information\n","  - Shared Transformer encoder layers for common feature extraction\n","  - Task-specific Transformer encoder layers for different prediction tasks\n","  - Configurable desequentialization and feed-forward layers\n","\n","  The architecture follows a multi-task learning approach where lower layers are shared\n","  across all tasks while upper layers are task-specific. The model processes sequential\n","  data using self-attention mechanisms and can handle variable-length sequences up to\n","  max_case_len.\n","\n","  Args:\n","      max_case_len (int): Maximum length of process cases/sequences\n","      output_dict (Dict[str, dict]): Configuration for output layers. Format: {\"task_name\": {\"units\": int, \"activation\": str}} Example: {\"remaining_time\": {'units': 1, 'activation': 'relu'}, \"next_activity\": {'units': 10, 'activation': 'softmax'}}\n","      activity_vocab (List[str]): Vocabulary of possible process activities\n","      activity_encoding_type (str, optional): Type of activity encoding. Currently only supports \"embedding\". Defaults to \"embedding\"\n","      time_encoding_type (str, optional): Type of time feature encoding. Options: \"mean\", \"minmax\", or \"normal\". Defaults to \"mean\"\n","      interval_since_last_event_vocab (List[float], optional): Vocabulary for time intervals\n","      hour_of_day_vocab (List[float], optional): Vocabulary for hour features\n","      day_of_week_vocab (List[float], optional): Vocabulary for day of week features\n","      activity_embedding_dim (int): Dimension for activity embeddings. Defaults to 36\n","      pos_encoding_type (str, optional): Type of positional encoding. Options: \"sincos\", \"learned\", \"rotary\", or \"temporal\". Defaults to \"learned\"\n","      enc_heads_shared (List[int]): Number of attention heads in each shared encoder layer. Defaults to [4]\n","      enc_ff_dims_shared (List[int]): Feed-forward dimensions in each shared encoder layer. Defaults to [64]\n","      enc_heads_nonshared (List[int]): Number of attention heads in task-specific layers. Defaults to []\n","      enc_ff_dims_nonshared (List[int]): Feed-forward dimensions in task-specific layers. Defaults to []\n","      enc_dropout (float): Dropout rate for encoder layers. Defaults to 0.0\n","      enc_activation (str): Activation function for encoder layers. Defaults to \"relu\"\n","      enc_normalize_first (bool): Whether to apply normalization before attention and feed-forward layers. Defaults to False\n","      deseq_type (str): Method to convert sequences to fixed-size vectors. Options: \"maxpool\", \"avgpool\", \"flatten\". Defaults to \"maxpool\"\n","      ff_dims (List[int]): Units in each final feed-forward layer. Defaults to [32, 128]\n","      ff_dropout (float): Dropout rate for final feed-forward layers. Defaults to 0.0\n","      ff_activation (str): Activation for final feed-forward layers. Defaults to \"relu\"\n","      numeric_mask_token (float): Token used for masking numeric values. Defaults to TOKEN_PAD_NUM\n","      mask_token (str): Token used for masking categorical values. Defaults to TOKEN_PAD\n","      oov_token (str): Token used for out-of-vocabulary values. Defaults to TOKEN_OOV\n","\n","  Raises:\n","      ValueError: If enc_heads_shared or enc_ff_dims_shared is empty\n","\n","  Author:\n","      Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  def __init__(\n","    self,\n","    max_case_len: int,\n","    output_dict: Dict[str, dict],\n","    activity_vocab: List[str],\n","    activity_encoding_type: Optional[Literal['embedding']] = 'embedding',\n","    time_encoding_type: Optional[Literal['mean', 'minmax', 'normal']] = 'mean',\n","    interval_since_last_event_vocab: Optional[List[float]] = None,\n","    hour_of_day_vocab: Optional[List[float]] = None,\n","    day_of_week_vocab: Optional[List[float]] = None,\n","    activity_embedding_dim: int = 36,\n","    pos_encoding_type: Optional[Literal['sincos', 'learned', 'rotary', 'temporal']] = 'learned',\n","    enc_heads_shared: List[int] = [4],\n","    enc_ff_dims_shared: List[int] = [64],\n","    enc_heads_nonshared: List[int] = [],\n","    enc_ff_dims_nonshared: List[int] = [],\n","    enc_dropout: float = 0.0,\n","    enc_activation: str = 'relu',\n","    enc_normalize_first: bool = False,\n","    deseq_type: Literal['maxpool', 'avgpool', 'flatten'] = 'maxpool',\n","    ff_dims: List[int] = [32, 128],\n","    ff_dropout: float = 0.0,\n","    ff_activation: str = 'relu',\n","    numeric_mask_token: float = TOKEN_PAD_NUM,\n","    mask_token: str = TOKEN_PAD,\n","    oov_token: str = TOKEN_OOV,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","    if not len(enc_heads_shared) > 0 or not len(enc_ff_dims_shared) > 0:\n","      raise ValueError(f\"enc_heads_nonshared and enc_ff_dim_nonshared must be at least 1\")\n","\n","    self.max_case_len = max_case_len\n","    self.output_dict = output_dict\n","\n","    self.activity_encoding_type = activity_encoding_type\n","    self.activity_vocab = activity_vocab\n","    self.activity_embedding_dim = activity_embedding_dim\n","\n","    self.time_encoding_type = time_encoding_type\n","    self.pos_encoding_type = pos_encoding_type\n","\n","    self.enc_heads_shared = enc_heads_shared\n","    self.enc_ff_dims_shared = enc_ff_dims_shared\n","    self.enc_heads_nonshared = enc_heads_nonshared\n","    self.enc_ff_dims_nonshared = enc_ff_dims_nonshared\n","    self.enc_dropout = enc_dropout\n","    self.enc_activation = enc_activation\n","    self.enc_normalize_first = enc_normalize_first\n","\n","    self.deseq_type = deseq_type\n","\n","    self.ff_dims = ff_dims\n","    self.ff_dropout = ff_dropout\n","    self.ff_activation = ff_activation\n","\n","    self.numeric_mask_token = numeric_mask_token\n","    self.mask_token = mask_token\n","    self.oov_token = oov_token\n","\n","    self.activity_encoding = self._build_activity_encoding_layer()\n","    self.interval_since_last_event_encoding = self._build_time_encoding_layer(vocab=interval_since_last_event_vocab, name=\"interval_since_last_event\")\n","    self.hour_of_day_encoding = self._build_time_encoding_layer(vocab=hour_of_day_vocab, name=\"hour_of_day\")\n","    self.day_of_week_encoding = self._build_time_encoding_layer(vocab=day_of_week_vocab, name=\"day_of_week\")\n","\n","    self.pos_encoding = self._build_positional_encoding_layer()\n","\n","    self.shared_encoder = self._build_hard_shared_transformer_encoder_layer()\n","\n","    self.non_shared_encoders = []\n","    self.deseqs = []\n","    self.ff_layers = []\n","    self.outputs = []\n","    for output_name, output_config in self.output_dict.items():\n","      self.non_shared_encoders.append(self._build_soft_shared_transformer_encoder_layer(name=output_name))\n","      self.deseqs.append(self._build_desequentialization_layer(name=output_name))\n","      self.ff_layers.append(self._build_ffn_layer(name=output_name))\n","      self.outputs.append(keras.layers.Dense(\n","          units=output_config['units'],\n","          activation=output_config['activation'],\n","          name=output_name\n","      ))\n","\n","  @property\n","  def default_loss(self) -> Dict[str, keras.Loss]:\n","    return {k: self.find_default_loss(v['activation']) for k, v in self.output_dict.items()}\n","\n","  @property\n","  def default_metrics(self) -> Dict[str, List[keras.Metric]]:\n","    return {k: self.find_default_metrics(v['activation']) for k, v in self.output_dict.items()}\n","\n","  @property\n","  def default_learning_rate(self) -> Union[float, keras.optimizers.schedules.LearningRateSchedule]:\n","    return 0.001\n","\n","  @property\n","  def default_optimizer(self) -> keras.optimizers.Optimizer:\n","    return keras.optimizers.Adam(learning_rate=self.default_learning_rate)\n","\n","  @property\n","  def default_regression_loss(self) -> keras.Loss:\n","    return keras.losses.LogCosh()\n","\n","  def _build_activity_encoding_layer(self, name: Optional[str] = None, **kwargs) -> keras.layers.Layer:\n","    \"\"\"Builds an activity encoding layer based on the specified encoding type.\n","\n","    Currently only supports embedding-based encoding, which converts activity names\n","    to dense vectors of fixed dimensionality.\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layers\n","\n","    Returns:\n","        keras.layers.Layer: Layer for activity encoding\n","\n","    Raises:\n","        NotImplementedError: If activity_encoding_type is not \"embedding\"\n","    \"\"\"\n","    name = self._generate_layer_name(f\"activity_encoding_{self.activity_encoding_type}\", name)\n","\n","    if self.activity_encoding_type is None:\n","      layer = keras.layers.Identity(name=name)\n","    elif 'embedding' == self.activity_encoding_type:\n","      lookup = keras.layers.StringLookup(\n","        mask_token=self.mask_token,\n","        oov_token=self.oov_token,\n","        output_mode='int',\n","        pad_to_max_tokens=False,\n","      )\n","      lookup.adapt(self.activity_vocab)\n","\n","      layer = keras.models.Sequential([\n","        lookup,\n","        keras.layers.Embedding(\n","          input_dim=len(lookup.get_vocabulary()),\n","          output_dim=self.activity_embedding_dim,\n","          mask_zero=True,\n","        )],\n","        name=name,\n","      )\n","    else:\n","      raise NotImplementedError(f\"Unknown activity encoding type {self.activity_encoding_type}\")\n","\n","    return layer\n","\n","  def _build_time_encoding_layer(self, vocab: Optional[List[float]] = None, name: Optional[str] = None, **kwargs) -> keras.layers.Layer:\n","    \"\"\"Builds a time encoding layer based on the specified encoding type.\n","\n","    Args:\n","        vocab (List[float], optional): Vocabulary of time values for adaptation\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layers\n","\n","    Returns:\n","        keras.layers.Layer: Sequential layer for time feature encoding\n","\n","    Raises:\n","        NotImplementedError: If time_encoding_type is not one of [None, \"mean\", \"minmax\", \"normal\"]\n","    \"\"\"\n","    name = self._generate_layer_name(f\"time_encoding_{self.time_encoding_type}\", name)\n","    layer = keras.models.Sequential([keras.layers.Masking(self.numeric_mask_token)], name=name)\n","\n","    if self.time_encoding_type is None:\n","      layer.add(keras.layers.Identity(name=name))\n","    elif 'mean' == self.time_encoding_type:\n","      lookup = MeanScaler(axis=None)\n","      lookup.adapt(vocab)\n","      layer.add(lookup)\n","    elif 'minmax' == self.time_encoding_type:\n","      lookup = MinMaxScaler(axis=None)\n","      lookup.adapt(vocab)\n","      layer.add(lookup)\n","    elif 'normal' == self.time_encoding_type:\n","      lookup = keras.layers.Normalization(axis=None)\n","      lookup.adapt(vocab)\n","      layer.add(lookup)\n","    else:\n","      raise NotImplementedError(f\"Unknown time encoding type {self.time_encoding_type}\")\n","\n","    return layer\n","\n","  def _build_positional_encoding_layer(self, **kwargs) -> keras.layers.Layer:\n","    \"\"\"Builds a positional encoding layer to inject sequence order information.\n","\n","    Supports multiple types of positional encodings:\n","    - sincos: Sinusoidal position encoding (Vaswani et al., 2017)\n","    - learned: Trainable position embeddings\n","    - rotary: Rotary position embeddings (Su et al., 2021)\n","    - temporal: Time-aware position encoding (not implemented)\n","\n","    Args:\n","        **kwargs: Additional keyword arguments passed to the position encoding layer\n","\n","    Returns:\n","        keras.layers.Layer: Positional encoding layer\n","\n","    Raises:\n","        NotImplementedError: If pos_encoding_type is \"temporal\" or unknown\n","    \"\"\"\n","    if self.pos_encoding_type is None:\n","      layer = keras.layers.Identity()\n","    elif 'sincos' == self.pos_encoding_type:\n","      layer = keras_nlp.layers.SinePositionEncoding(**kwargs)\n","    elif 'learned' == self.pos_encoding_type:\n","      layer = keras_nlp.layers.PositionEmbedding(sequence_length=self.max_case_len, **kwargs)\n","    elif 'rotary' == self.pos_encoding_type:\n","      layer = keras_nlp.layers.RotaryEmbedding(**kwargs)\n","    elif 'temporal' == self.pos_encoding_type:\n","      raise NotImplementedError(f\"Temporal positional encoding is not yet implemented\")\n","    else:\n","      raise NotImplementedError(f\"Unknown positional encoding type {self.pos_encoding_type}\")\n","\n","    return layer\n","\n","  def _build_desequentialization_layer(self, name: Optional[str] = None, **kwargs) -> keras.layers.Layer:\n","    \"\"\"Builds a layer to convert variable-length sequences to fixed-size vectors.\n","\n","    Supports multiple pooling strategies:\n","    - maxpool: Global max pooling across the sequence\n","    - avgpool: Global average pooling across the sequence\n","    - flatten: Flattens the entire sequence (preserves all information but loses\n","      sequence invariance)\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layer\n","\n","    Returns:\n","        keras.layers.Layer: Desequentialization layer\n","\n","    Raises:\n","        NotImplementedError: If deseq_type is unknown\n","    \"\"\"\n","    name = self._generate_layer_name(f\"{self.deseq_type}\", name)\n","\n","    if 'maxpool' == self.deseq_type:\n","      layer = keras.layers.GlobalMaxPooling1D(name=name, **kwargs)\n","    elif 'avgpool' == self.deseq_type:\n","      layer = keras.layers.GlobalAveragePooling1D(name=name, **kwargs)\n","    elif 'flatten' == self.deseq_type:\n","      layer = keras.layers.Flatten(name=name, **kwargs)\n","    else:\n","      raise NotImplementedError(f\"Unknown desequentialization type {self.deseq_type}\")\n","\n","    return layer\n","\n","  def _build_hard_shared_transformer_encoder_layer(self, name: Optional[str] = None, **kwargs) -> List[keras.Layer]:\n","    \"\"\"Builds the shared Transformer encoder layers used across all tasks.\n","\n","    Creates a stack of Transformer encoder layers with specified number of attention\n","    heads and feed-forward dimensions. These layers learn general process patterns\n","    shared across all prediction tasks.\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to TransformerEncoder\n","\n","    Returns:\n","        List[keras.Layer]: List of shared Transformer encoder layers\n","    \"\"\"\n","    name = self._generate_layer_name(f\"hardsharing_transformer_encoder\", name)\n","    layer = keras.models.Sequential(name=name)\n","\n","    for num_heads, ff_dim in zip(self.enc_heads_shared, self.enc_ff_dims_shared, strict=True):\n","      layer.add(keras_nlp.layers.TransformerEncoder(\n","          intermediate_dim=ff_dim,\n","          num_heads=num_heads,\n","          dropout=self.enc_dropout,\n","          activation=self.enc_activation,\n","          normalize_first=self.enc_normalize_first,\n","          **kwargs\n","      ))\n","\n","    return layer\n","\n","  def _build_soft_shared_transformer_encoder_layer(self, name: Optional[str] = None, **kwargs) -> keras.Layer:\n","    \"\"\"Builds task-specific Transformer encoder layers.\n","\n","    Creates a stack of Transformer encoder layers that are specific to each prediction\n","    task, allowing specialization for different objectives.\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to TransformerEncoder\n","\n","    Returns:\n","        keras.Layer: Sequential model containing task-specific Transformer encoder layers\n","    \"\"\"\n","    name = self._generate_layer_name(f\"softsharing_transformer_encoder\", name)\n","    layer = keras.models.Sequential([keras.layers.Identity()], name=name)\n","\n","    for num_heads, ff_dim in zip(self.enc_heads_nonshared, self.enc_ff_dims_nonshared, strict=True):\n","      layer.add(keras_nlp.layers.TransformerEncoder(\n","          intermediate_dim=ff_dim,\n","          num_heads=num_heads,\n","          dropout=self.enc_dropout,\n","          activation=self.enc_activation,\n","          normalize_first=self.enc_normalize_first,\n","          **kwargs\n","      ))\n","\n","    return layer\n","\n","  def _build_ffn_layer(self, name: Optional[str] = None, **kwargs) -> keras.layers.Layer:\n","    \"\"\"Builds feed-forward layers with optional dropout.\n","\n","    Args:\n","        name (str, optional): Name prefix for the layer\n","        **kwargs: Additional keyword arguments passed to the layers\n","\n","    Returns:\n","        keras.layers.Layer: Sequential model containing feed-forward layers\n","    \"\"\"\n","    name = self._generate_layer_name(\"ffn\", name)\n","    layer = keras.models.Sequential(name=name)\n","\n","    for ff_dim in self.ff_dims:\n","      layer.add(keras.layers.Dense(units=ff_dim, activation=self.ff_activation))\n","      layer.add(keras.layers.Dropout(self.ff_dropout))\n","\n","    return layer\n","\n","  def build_graph(self) -> keras.Model:\n","    \"\"\"Builds and returns a Keras Model instance with the defined architecture.\n","\n","    Returns:\n","        keras.Model: Compiled model with defined inputs and outputs\n","    \"\"\"\n","    in_activity = keras.layers.Input(shape=(self.max_case_len,), name=\"activity\", dtype='string')\n","    in_interval_since_last_event = keras.layers.Input(shape=(1,), name=\"time_timestamp_elapsedprev\", dtype='float32')\n","    in_hour_of_day = keras.layers.Input(shape=(1,), name=\"time_timestamp_hour_raw\", dtype='float32')\n","    in_day_of_week = keras.layers.Input(shape=(1,), name=\"time_timestamp_weekday_raw\", dtype='float32')\n","\n","    inputs = {\n","        \"activity\": in_activity,\n","        \"time_timestamp_elapsedprev\": in_interval_since_last_event,\n","        \"time_timestamp_hour_raw\": in_hour_of_day,\n","        \"time_timestamp_weekday_raw\": in_day_of_week,\n","    }\n","\n","    return keras.Model(inputs=[in_activity, in_interval_since_last_event, in_hour_of_day, in_day_of_week], outputs=self.call(in_activity, in_interval_since_last_event, in_hour_of_day, in_day_of_week), name=self.name)\n","\n","  def call(self, inputs, inputs_interval_since_last_event, inputs_hour_of_day, inputs_day_of_week, training=False):\n","    x_activity = self.activity_encoding(inputs)\n","    x_interval = self.interval_since_last_event_encoding(inputs_interval_since_last_event)\n","    x_hour = self.hour_of_day_encoding(inputs_hour_of_day)\n","    x_day = self.day_of_week_encoding(inputs_day_of_week)\n","\n","    if self.pos_encoding_type is None or 'rotary' == self.pos_encoding_type:\n","      x_activity = self.pos_encoding(x_activity)\n","    elif 'learned' == self.pos_encoding_type or 'sincos' == self.pos_encoding_type:\n","      x_activity = keras.layers.add([self.pos_encoding(x_activity), x_activity])\n","    else:\n","      raise NotImplementedError(f\"Unknown positional encoding type {self.pos_encoding_type}\")\n","\n","    x_activity = self.shared_encoder(x_activity)\n","\n","    x_outputs = []\n","    for encoder, deseq, ff_layer, output in zip(self.non_shared_encoders, self.deseqs, self.ff_layers, self.outputs, strict=True):\n","      x_nonshared = encoder(x_activity)\n","      x_nonshared = deseq(x_nonshared)\n","      x_nonshared = keras.layers.concatenate([x_nonshared, x_interval, x_hour, x_day])\n","      x_nonshared = ff_layer(x_nonshared)\n","      x_outputs.append(output(x_nonshared))\n","\n","    return x_outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WY9-k1jrnHYB"},"outputs":[],"source":["trans = BaseProcessTransformer(\n","    max_case_len=10,\n","    activity_vocab=['a', 'b', 'c'],\n","    interval_since_last_event_vocab=np.array([1,2,3]),\n","    hour_of_day_vocab=np.array([1,2,3]),\n","    day_of_week_vocab=np.array([1,2,3]),\n","    output_dict={\"remaining_time\": {'units': 1, 'activation': 'relu'}, \"next_activity\": {'units': 10, 'activation': 'softmax'}},\n",")\n","\n","trans = trans.build_graph()\n","trans.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pYj18josxsN"},"outputs":[],"source":["keras.utils.plot_model(\n","  trans,\n","  show_shapes=True,\n","  show_dtype=True,\n","  show_layer_names=True,\n","  expand_nested=True,\n","  show_layer_activations=True,\n","  show_trainable=True,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"ZrtGCOvzE0FX"},"source":["### Custom Hierarchical Transformer"]},{"cell_type":"markdown","metadata":{"id":"OqiIyZBYl7oi"},"source":["#### Dynamic Attribute Selection Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Qnq6TXcOIAF"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class DynamicAttributeSelectionNetwork(keras.Layer):\n","  \"\"\"A custom Keras layer that implements a dynamic attribute selection mechanism using\n","    transformer decoders and a variable selection network (VSN) or other flattening strategies. Based on the work of Hennig 2025.\n","\n","    This layer is designed to process sequential data and dynamically select relevant attributes\n","    based on the input sequence. It supports multiple flattening strategies, including VSN,\n","    average pooling, max pooling, and standard flattening.\n","\n","    Attributes:\n","        dec_num_heads (int): Number of attention heads in the transformer decoder.\n","        dec_ff_dim (int): Intermediate dimension of the feedforward network in the transformer decoder.\n","        var_units (int): Number of units in the variable selection network (VSN) or GRN layers.\n","        var_dropout (float): Dropout rate for the variable selection network (VSN) or GRN layers.\n","        var_activation (str): Activation function for the variable selection network (VSN) or GRN layers.\n","        dec_depth (int): Number of transformer decoder layers.\n","        dec_dropout (float): Dropout rate for the transformer decoder layers.\n","        dec_activation (str): Activation function for the transformer decoder layers.\n","        dec_layer_norm_epsilon (float): Epsilon value for layer normalization in the transformer decoder.\n","        dec_kernel_init (Union[str, keras.Initializer]): Kernel initializer for the transformer decoder.\n","        dec_bias_init (Union[str, keras.Initializer]): Bias initializer for the transformer decoder.\n","        dec_normalize_first (bool): Whether to normalize inputs before attention and feedforward layers.\n","        grn_depth (int): Number of Gated Residual Network (GRN) layers in the flattening module.\n","        grn_dropout (float): Dropout rate for the GRN layers.\n","        grn_activation (str): Activation function for the GRN layers.\n","        glu_activation (str): Activation function for the Gated Linear Unit (GLU) in GRN layers.\n","        flattening (Literal['vsn', 'avg_pool', 'max_pool', 'flatten']): Flattening strategy to use.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","    \"\"\"\n","  def __init__(\n","    self,\n","    dec_num_heads: int,\n","    dec_ff_dim: int,\n","    var_units: int,\n","    var_dropout: float = 0.0,\n","    var_activation: str = 'elu',\n","    dec_depth: int = 1,\n","    dec_dropout: float = 0.0,\n","    dec_activation: str = 'relu',\n","    dec_layer_norm_epsilon: float = 1e-05,\n","    dec_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    dec_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    dec_normalize_first: bool = False,\n","    grn_depth: int = 0,\n","    grn_dropout: float = 0.0,\n","    grn_activation: str = 'elu',\n","    glu_activation: str = 'sigmoid',\n","    flattening: Literal['vsn', 'avg_pool', 'max_pool', 'flatten'] = 'vsn',\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    self.dec_depth = dec_depth\n","    self.dec_num_heads = dec_num_heads\n","    self.dec_ff_dim = dec_ff_dim\n","    self.dec_dropout = dec_dropout\n","    self.dec_activation = dec_activation\n","    self.dec_layer_norm_epsilon = dec_layer_norm_epsilon\n","    self.dec_kernel_init = dec_kernel_init\n","    self.dec_bias_init = dec_bias_init\n","    self.dec_normalize_first = dec_normalize_first\n","\n","    self.var_units = var_units\n","    self.var_dropout = var_dropout\n","    self.var_activation = var_activation\n","\n","    self.grn_depth = grn_depth\n","    self.grn_dropout = grn_dropout\n","    self.grn_activation = grn_activation\n","    self.glu_activation = glu_activation\n","\n","    self.flattening = flattening\n","\n","  def build(self, input_shape: Tuple[int, int]):\n","    self.max_feat_len = input_shape[-1]\n","    self.max_seq_len = input_shape[-2]\n","\n","    self.decoders = []\n","    for i in range(self.dec_depth):\n","      dec = keras_nlp.layers.TransformerDecoder(\n","        num_heads=self.dec_num_heads,\n","        intermediate_dim=self.dec_ff_dim,\n","        dropout=self.dec_dropout,\n","        activation=self.dec_activation,\n","        layer_norm_epsilon=self.dec_layer_norm_epsilon,\n","        kernel_initializer=self.dec_kernel_init,\n","        bias_initializer=self.dec_bias_init,\n","        normalize_first=self.dec_normalize_first,\n","      )\n","      self.decoders.append(dec)\n","\n","    if 'vsn' == self.flattening:\n","      self.flatten = VariableSelectionNetwork(\n","        units=self.var_units,\n","        dropout=self.var_dropout,\n","        grn_depth=self.grn_depth,\n","        grn_activation=self.var_activation,\n","        glu_activation=self.glu_activation,\n","    )\n","    elif 'avg_pool' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.GlobalAveragePooling1D(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","    elif 'max_pool' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.GlobalMaxPooling1D(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","    elif 'flatten' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.Flatten(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len * self.max_seq_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","\n","  def call(self, encoder_seq, decoder_seq, encoder_padding_mask=None, decoder_padding_mask=None):\n","    x = decoder_seq\n","    for dec in self.decoders:\n","      x = dec(\n","        decoder_sequence=x,\n","        decoder_padding_mask=decoder_padding_mask,\n","        encoder_sequence=encoder_seq,\n","        encoder_padding_mask=encoder_padding_mask,\n","        use_causal_mask=False,\n","      )\n","\n","    if 'vsn' == self.flattening:\n","      x = keras.ops.unstack(x, axis=-2)\n","      x = self.flatten(x, mask=encoder_padding_mask)\n","    else:\n","      x = self.flatten(x)\n","\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"qloh6EOImArG"},"source":["#### Static Attribute Selection Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rH9vPR1qpV1N"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class StaticAttributeSelectionNetwork(keras.Layer):\n","  \"\"\"A custom Keras layer that implements a static attribute selection mechanism using\n","    Gated Residual Networks (GRNs) and a Variable Selection Network (VSN).\n","\n","    This layer is designed to process multiple input sequences (attributes) and dynamically\n","    select relevant features using a combination of GRNs and VSN. It is particularly useful\n","    for tasks involving multi-modal or multi-attribute data.\n","\n","    Attributes:\n","        var_units (int): Number of units in the Variable Selection Network (VSN) and GRNs.\n","        var_dropout (float): Dropout rate for the Variable Selection Network (VSN) and GRNs.\n","        var_activation (str): Activation function for the Variable Selection Network (VSN).\n","        grn_dim (Optional[Union[int, List[int]]]): Dimensions of the Gated Residual Networks (GRNs).\n","        grn_dropout (float): Dropout rate for the GRNs.\n","        grn_activation (str): Activation function for the GRNs.\n","        glu_activation (str): Activation function for the Gated Linear Unit (GLU) in GRNs.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  def __init__(\n","    self,\n","    var_units: int,\n","    var_dropout: float = 0.0,\n","    var_activation: str = 'elu',\n","    grn_depth: int = 0,\n","    grn_dropout: float = 0.0,\n","    grn_activation: str = 'elu',\n","    glu_activation: str = 'sigmoid',\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    self.var_units = var_units\n","    self.var_dropout = var_dropout\n","    self.var_activation = var_activation\n","\n","    self.grn_depth = grn_depth\n","    self.grn_dropout = grn_dropout\n","    self.grn_activation = grn_activation\n","    self.glu_activation = glu_activation\n","\n","  def build(self, input_shape: List[Tuple[int, int, int]]):\n","    self.num_attrs = len(input_shape)\n","\n","    self.var_selection_net = VariableSelectionNetwork(\n","      units=self.var_units,\n","      dropout=self.var_dropout,\n","      grn_depth=self.grn_depth,\n","      grn_activation=self.var_activation,\n","      glu_activation=self.glu_activation,\n","    )\n","\n","  def call(self, inputs):\n","    x = [keras.ops.squeeze(input, axis=-2) for input in inputs]\n","    x = self.var_selection_net(x, mask=keras.ops.ones(self.num_attrs, dtype=bool))\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"y0QrAILemFUA"},"source":["#### Activity & Timestamp Encoder Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvVwtOXIfVlw"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class ActivityEncoderNetwork(keras.Layer):\n","  \"\"\"A custom Keras layer that implements an activity encoder network using transformer encoders,\n","    Gated Residual Networks (GRNs), and a Variable Selection Network (VSN). Provides an implementation of the work by Hennig (2024).\n","\n","    This layer is designed to process sequential data (e.g., activity sequences) and encode it\n","    into a meaningful representation. It combines transformer encoders for sequence modeling,\n","    GRNs for feature processing, and a VSN for dynamic feature selection.\n","\n","    Attributes:\n","        enc_num_heads (Union[int, List[int]]): Number of attention heads in the transformer encoders.\n","        enc_ff_dim (Union[int, List[int]]): Intermediate dimension of the feedforward network in the transformer encoders.\n","        var_units (int): Number of units in the Variable Selection Network (VSN) and GRNs.\n","        enc_dropout (float): Dropout rate for the transformer encoders.\n","        enc_activation (str): Activation function for the transformer encoders.\n","        enc_layer_norm_epsilon (float): Epsilon value for layer normalization in the transformer encoders.\n","        enc_kernel_init (Union[str, keras.Initializer]): Kernel initializer for the transformer encoders.\n","        enc_bias_init (Union[str, keras.Initializer]): Bias initializer for the transformer encoders.\n","        enc_normalize_first (bool): Whether to normalize inputs before attention and feedforward layers.\n","        var_dropout (float): Dropout rate for the Variable Selection Network (VSN) and GRNs.\n","        var_activation (str): Activation function for the Variable Selection Network (VSN).\n","        grn_dim (Optional[Union[int, List[int]]]): Dimensions of the Gated Residual Networks (GRNs).\n","        grn_dropout (float): Dropout rate for the GRNs.\n","        grn_activation (str): Activation function for the GRNs.\n","        glu_activation (str): Activation function for the Gated Linear Unit (GLU) in GRNs.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","    \"\"\"\n","  def __init__(\n","    self,\n","    enc_num_heads: int,\n","    enc_ff_dim: int,\n","    var_units: int,\n","    enc_depth: int = 1,\n","    enc_dropout: float = 0.0,\n","    enc_activation: str ='relu',\n","    enc_layer_norm_epsilon: float = 1e-05,\n","    enc_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    enc_bias_init: Union[str, keras.Initializer] ='zeros',\n","    enc_normalize_first: bool = False,\n","    var_dropout: float = 0.0,\n","    var_activation: str = 'elu',\n","    grn_depth: int = 0,\n","    grn_dropout: float = 0.0,\n","    grn_activation: str = 'elu',\n","    glu_activation: str = 'sigmoid',\n","    flattening: Literal['vsn', 'avg_pool', 'max_pool', 'flatten'] = 'vsn',\n","    **kwargs,\n","  ):\n","    super().__init__(**kwargs)\n","\n","    self.enc_depth = enc_depth\n","    self.enc_num_heads = enc_num_heads\n","    self.enc_ff_dim = enc_ff_dim\n","    self.enc_dropout = enc_dropout\n","    self.enc_activation = enc_activation\n","    self.enc_layer_norm_epsilon = enc_layer_norm_epsilon\n","    self.enc_kernel_init = enc_kernel_init\n","    self.enc_bias_init = enc_bias_init\n","    self.enc_normalize_first = enc_normalize_first\n","\n","    self.var_units = var_units\n","    self.var_dropout = var_dropout\n","    self.var_activation = var_activation\n","\n","    self.grn_depth = grn_depth\n","    self.grn_dropout = grn_dropout\n","    self.grn_activation = grn_activation\n","    self.glu_activation = glu_activation\n","\n","    self.flattening = flattening\n","\n","  def build(self, input_shape):\n","    self.max_feat_len = input_shape[-1]\n","    self.max_seq_len = input_shape[-2]\n","\n","    self.encoders = []\n","    for _ in range(self.enc_depth):\n","      enc = keras_nlp.layers.TransformerEncoder(\n","        intermediate_dim=self.enc_ff_dim,\n","        num_heads=self.enc_num_heads,\n","        dropout=self.enc_dropout,\n","        activation=self.enc_activation,\n","        layer_norm_epsilon=self.enc_layer_norm_epsilon,\n","        kernel_initializer=self.enc_kernel_init,\n","        bias_initializer=self.enc_bias_init,\n","        normalize_first=self.enc_normalize_first,\n","      )\n","      self.encoders.append(enc)\n","\n","    if 'vsn' == self.flattening:\n","      self.flatten = VariableSelectionNetwork(\n","        units=self.var_units,\n","        dropout=self.var_dropout,\n","        grn_depth=self.grn_depth,\n","        grn_activation=self.var_activation,\n","        glu_activation=self.glu_activation,\n","    )\n","    elif 'avg_pool' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.GlobalAveragePooling1D(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","    elif 'max_pool' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.GlobalMaxPooling1D(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","    elif 'flatten' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.Flatten(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len * self.max_seq_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","\n","  def call(self, inputs, padding_mask=None, attention_mask=None):\n","    x_enc = inputs\n","    for enc in self.encoders:\n","      x_enc = enc(x_enc, padding_mask=padding_mask, attention_mask=attention_mask)\n","\n","    if 'vsn' == self.flattening:\n","      x = keras.ops.unstack(x_enc, axis=-2)\n","      x = self.flatten(x, mask=padding_mask)\n","    else:\n","      x = self.flatten(x_enc)\n","\n","    return x, x_enc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PccuD_Y0fjYm"},"outputs":[],"source":["@keras.saving.register_keras_serializable()\n","class TimestampEncoderNetwork(keras.Layer):\n","  \"\"\"Initializes the TimestampEncoderNetwork layer. Provides an implementation of the work by Hennig (2024).\n","\n","    Args:\n","        enc_num_heads (Union[int, List[int]]): Number of attention heads in the transformer encoders.\n","        enc_ff_dim (Union[int, List[int]]): Intermediate dimension of the feedforward network in the transformer encoders.\n","        var_units (int): Number of units in the Variable Selection Network (VSN) or GRNs.\n","        enc_depth (int): Number of transformer encoder layers.\n","        enc_dropout (float): Dropout rate for the transformer encoders.\n","        enc_activation (str): Activation function for the transformer encoders.\n","        enc_layer_norm_epsilon (float): Epsilon value for layer normalization in the transformer encoders.\n","        enc_kernel_init (Union[str, keras.Initializer]): Kernel initializer for the transformer encoders.\n","        enc_bias_init (Union[str, keras.Initializer]): Bias initializer for the transformer encoders.\n","        enc_normalize_first (bool): Whether to normalize inputs before attention and feedforward layers.\n","        var_dropout (float): Dropout rate for the Variable Selection Network (VSN) or GRNs.\n","        var_activation (str): Activation function for the Variable Selection Network (VSN) or GRNs.\n","        grn_depth (int): Number of Gated Residual Network (GRN) layers in the flattening module.\n","        grn_dropout (float): Dropout rate for the GRNs.\n","        grn_activation (str): Activation function for the GRNs.\n","        glu_activation (str): Activation function for the Gated Linear Unit (GLU) in GRNs.\n","        flattening (Literal['vsn', 'avg_pool', 'max_pool', 'flatten']): Flattening strategy to use.\n","        **kwargs: Additional keyword arguments passed to the parent class.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","    \"\"\"\n","  def __init__(\n","    self,\n","    enc_num_heads: Union[int, List[int]],\n","    enc_ff_dim: Union[int, List[int]],\n","    var_units: int,\n","    enc_depth: int = 1,\n","    enc_dropout: float = 0.0,\n","    enc_activation: str ='relu',\n","    enc_layer_norm_epsilon: float = 1e-05,\n","    enc_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    enc_bias_init: Union[str, keras.Initializer] ='zeros',\n","    enc_normalize_first: bool = False,\n","    var_dropout: float = 0.0,\n","    var_activation: str = 'elu',\n","    grn_depth: int = 0,\n","    grn_dropout: float = 0.0,\n","    grn_activation: str = 'elu',\n","    glu_activation: str = 'sigmoid',\n","    flattening: Literal['vsn', 'avg_pool', 'max_pool', 'flatten'] = 'vsn',\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    self.flattening = flattening\n","\n","    self.enc_depth = enc_depth\n","    self.enc_num_heads = enc_num_heads\n","    self.enc_ff_dim = enc_ff_dim\n","    self.enc_dropout = enc_dropout\n","    self.enc_activation = enc_activation\n","    self.enc_layer_norm_epsilon = enc_layer_norm_epsilon\n","    self.enc_kernel_init = enc_kernel_init\n","    self.enc_bias_init = enc_bias_init\n","    self.enc_normalize_first = enc_normalize_first\n","\n","    self.var_units = var_units\n","    self.var_dropout = var_dropout\n","    self.var_activation = var_activation\n","\n","    self.grn_depth = grn_depth\n","    self.grn_dropout = grn_dropout\n","    self.grn_activation = grn_activation\n","    self.glu_activation = glu_activation\n","\n","  def build(self, input_shape: Tuple[int, int, int]):\n","    self.max_seq_len = input_shape[-2]\n","    self.max_feat_len = input_shape[-1]\n","\n","    self.masking = keras.layers.Masking(mask_value=-1.0)\n","\n","    self.encoders = []\n","    for _ in range(self.enc_depth):\n","      enc = keras_nlp.layers.TransformerEncoder(\n","        intermediate_dim=self.enc_ff_dim,\n","        num_heads=self.enc_num_heads,\n","        dropout=self.enc_dropout,\n","        activation=self.enc_activation,\n","        layer_norm_epsilon=self.enc_layer_norm_epsilon,\n","        kernel_initializer=self.enc_kernel_init,\n","        bias_initializer=self.enc_bias_init,\n","        normalize_first=self.enc_normalize_first,\n","      )\n","      self.encoders.append(enc)\n","\n","    if 'vsn' == self.flattening:\n","      self.flatten = VariableSelectionNetwork(\n","        units=self.var_units,\n","        dropout=self.var_dropout,\n","        grn_depth=self.grn_depth,\n","        grn_activation=self.var_activation,\n","        glu_activation=self.glu_activation,\n","    )\n","    elif 'avg_pool' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.GlobalAveragePooling1D(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","    elif 'max_pool' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.GlobalMaxPooling1D(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","    elif 'flatten' == self.flattening:\n","      self.flatten = keras.models.Sequential()\n","      self.flatten.add(keras.layers.Flatten(data_format='channels_last'))\n","      for _ in range(self.grn_depth):\n","        self.flatten.add(GatedResidualNetwork(\n","            units=self.max_feat_len * self.max_seq_len,\n","            dropout=self.var_dropout,\n","            non_linear_activation=self.var_activation,\n","            glu_activation=self.glu_activation,\n","        ))\n","      self.flatten.add(GatedResidualNetwork(self.var_units, self.var_dropout, self.var_activation, self.glu_activation))\n","\n","  def call(self, inputs):\n","    inputs = self.masking(inputs)\n","\n","    x_mask = self.masking.compute_mask(inputs)\n","    x_enc = inputs\n","    for enc in self.encoders:\n","      x_enc = enc(x_enc, padding_mask=x_mask)\n","\n","    if 'vsn' == self.flattening:\n","      x = keras.ops.unstack(x_enc, axis=-2)\n","      x = self.flatten(x, mask=x_mask)\n","    else:\n","      x = self.flatten(x_enc)\n","\n","    return x, x_enc"]},{"cell_type":"markdown","metadata":{"id":"Z-Ls5jupmPT-"},"source":["#### Complete Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYwmfV1dqOds"},"outputs":[],"source":["class BaseHierarchicalProcessTransformer(keras.Model):\n","  \"\"\"A hierarchical transformer-based model for sequential data processing.\n","    This model integrates multiple input modalities, including categorical, numerical,\n","    and temporal features, to capture complex dependencies in sequential data. Provides an implementation of the work by Hennig (2024).\n","\n","    Args:\n","        max_sequence_len (int): Maximum sequence length.\n","        embed_dim (int): Embedding dimension.\n","        encoder_num_heads (int): Number of attention heads in the encoder.\n","        encoder_ff_dim (int): Feedforward dimension in the encoder.\n","        encoder_var_units (int): Variable selection units in the encoder.\n","        decoder_num_heads (int): Number of attention heads in the decoder.\n","        decoder_ff_dim (int): Feedforward dimension in the decoder.\n","        time_num_heads (int): Number of attention heads for time encoding.\n","        time_ff_dim (int): Feedforward dimension for time encoding.\n","        time_var_units (int): Variable selection units for time encoding.\n","        ff_dims (List[int]): Dimensions for feedforward layers.\n","        dynamic_var_net_dim (int): Dimensionality of the dynamic variable network.\n","        static_var_net_dim (int): Dimensionality of the static variable network.\n","        activity_vocab (List[str]): Vocabulary list for activity tokens.\n","        time_attrs (dict): Dictionary mapping time attributes to feature names.\n","        static_catergorical_attrs (dict): Dictionary of static categorical attributes.\n","        static_numerical_attrs (dict): Dictionary of static numerical attributes.\n","        dynamic_categorical_attrs (dict): Dictionary of dynamic categorical attributes.\n","        dynamic_numerical_attrs (dict): Dictionary of dynamic numerical attributes.\n","        dynamic_raw_attrs (dict): Dictionary of raw dynamic attributes.\n","        position_encoder_mode (str): Type of positional encoding to use.\n","        encoder_depth (int): Number of layers in the encoder.\n","        decoder_depth (int): Number of layers in the decoder.\n","        out_dict (dict, optional): Dictionary specifying the output layers.\n","        kwargs: Additional parameters for the Keras Model.\n","\n","    Author:\n","        Marc C. Hennig (mhennig@hm.edu)\n","  \"\"\"\n","  def __init__(\n","    self,\n","    max_sequence_len: int,\n","    embed_dim: int,\n","    encoder_num_heads: int,\n","    encoder_ff_dim: int,\n","    encoder_var_units: int,\n","    decoder_num_heads: int,\n","    decoder_ff_dim: int,\n","    time_num_heads: int,\n","    time_ff_dim: int,\n","    time_var_units: int,\n","    ff_dims: List[int],\n","    dynamic_var_net_dim: int,\n","    static_var_net_dim: int,\n","    activity_vocab: List[str],\n","    time_attrs: Dict[Literal['second', 'minute', 'hour', 'day', 'weekday', 'yearday'], str] = {},\n","    static_catergorical_attrs: Dict[str, List[str]] = {},\n","    static_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    dynamic_categorical_attrs: Dict[str, List[str]] = {},\n","    dynamic_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    dynamic_raw_attrs: Dict[str, List[str]] = {},\n","    position_encoder_mode: Literal['sincos', 'rotary', 'temporal', 'learned'] = 'learned',\n","    encoder_depth: int = 1,\n","    encoder_dropout: float = 0.0,\n","    encoder_activation: str = 'relu',\n","    encoder_layer_norm_epsilon: float = 1e-05,\n","    encoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    encoder_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    encoder_normalize_first: bool = False,\n","    encoder_var_activation: str = 'elu',\n","    encoder_var_dropout: float = 0.0,\n","    encoder_grn_depth: int = 0,\n","    encoder_grn_activation: str = 'elu',\n","    encoder_grn_dropout: float = 0.0,\n","    decoder_depth: int = 1,\n","    decoder_dropout: float = 0.0,\n","    decoder_activation: str ='relu',\n","    decoder_layer_norm_epsilon: float = 1e-05,\n","    decoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    decoder_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    decoder_normalize_first: bool = False,\n","    dynamic_var_net_dropout: float = 0.0,\n","    dynamic_var_net_activation: str = 'elu',\n","    dynamic_var_grn_depth: int = 0,\n","    dynamic_var_grn_activation: str = 'elu',\n","    dynamic_var_grn_dropout: float = 0.0,\n","    static_var_net_dropout: float = 0.0,\n","    static_var_net_activation: str = 'elu',\n","    static_var_grn_depth: int = 0,\n","    static_var_grn_activation: str = 'elu',\n","    static_var_grn_dropout: float = 0.0,\n","    time_depth: int = 1,\n","    time_dropout: float = 0.0,\n","    time_activation: str ='relu',\n","    time_layer_norm_epsilon: float = 1e-05,\n","    time_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    time_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    time_normalize_first: bool = False,\n","    time_var_activation: str = 'elu',\n","    time_var_dropout: float = 0.0,\n","    time_grn_depth: int = 0,\n","    time_grn_activation: str = 'elu',\n","    time_grn_dropout: float = 0.0,\n","    ff_dropout: float = 0.0,\n","    ff_activation: str = 'relu',\n","    ff_layer_norm_eps: float = 1e-05,\n","    flattening: Literal['vsn', 'avg_pool', 'max_pool', 'flatten'] = 'vsn',\n","    glu_activation: str = 'sigmoid',\n","    out_dict: Dict[str, dict] = None,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    self.max_seq_len = max_sequence_len\n","    self.embed_dim = embed_dim\n","    self.pos_enc_mode = position_encoder_mode\n","    self.out_dict = out_dict\n","\n","    self.flattening = flattening\n","    self.glu_activation = glu_activation\n","\n","    self.enc_num_heads = encoder_num_heads\n","    self.enc_ff_dim = encoder_ff_dim\n","    self.enc_dropout = encoder_dropout\n","    self.enc_activation = encoder_activation\n","    self.enc_layer_norm_epsilon = encoder_layer_norm_epsilon\n","    self.enc_kernel_init = encoder_kernel_init\n","    self.enc_bias_init = encoder_bias_init\n","    self.enc_normalize_first = encoder_normalize_first\n","\n","    self.enc_var_units = encoder_var_units\n","    self.enc_var_dropout = encoder_var_dropout\n","    self.enc_var_activation = encoder_var_activation\n","    self.enc_grn_depth = encoder_grn_depth\n","    self.enc_grn_dropout = encoder_grn_dropout\n","    self.enc_grn_activation = encoder_grn_activation\n","\n","    self.dec_num_heads = decoder_num_heads\n","    self.dec_ff_dim = decoder_ff_dim\n","    self.dec_dropout = decoder_dropout\n","    self.dec_activation = decoder_activation\n","    self.dec_layer_norm_epsilon = decoder_layer_norm_epsilon\n","    self.dec_kernel_init = decoder_kernel_init\n","    self.dec_bias_init = decoder_bias_init\n","    self.dec_normalize_first = decoder_normalize_first\n","\n","    self.dyn_var_dim = dynamic_var_net_dim\n","    self.dyn_var_dropout = dynamic_var_net_dropout\n","    self.dyn_var_activation = dynamic_var_net_activation\n","    self.dyn_var_grn_depth = dynamic_var_grn_depth\n","    self.dyn_var_grn_activation = dynamic_var_grn_activation\n","    self.dyn_var_grn_dropout = dynamic_var_grn_dropout\n","\n","    self.stat_var_dim = static_var_net_dim\n","    self.stat_var_dropout = static_var_net_dropout\n","    self.stat_var_activation = static_var_net_activation\n","    self.stat_var_grn_depth = static_var_grn_depth\n","    self.stat_var_grn_activation = static_var_grn_activation\n","    self.stat_var_grn_dropout = static_var_grn_dropout\n","\n","    self.time_num_heads = time_num_heads\n","    self.time_ff_dim = time_ff_dim\n","    self.time_dropout = time_dropout\n","    self.time_activation = time_activation\n","    self.time_layer_norm_epsilon = time_layer_norm_epsilon\n","    self.time_kernel_init = time_kernel_init\n","    self.time_bias_init = time_bias_init\n","    self.time_normalize_first = time_normalize_first\n","\n","    self.time_var_units = time_var_units\n","    self.time_var_dropout = time_var_dropout\n","    self.time_var_activation = time_var_activation\n","    self.time_grn_depth = time_grn_depth\n","    self.time_grn_dropout = time_grn_dropout\n","    self.time_grn_activation = time_grn_activation\n","\n","    self.ff_dims = ff_dims\n","    self.ff_dropout = ff_dropout\n","    self.ff_activation = ff_activation\n","    self.ff_layer_norm_eps = ff_layer_norm_eps\n","\n","    self.activity_vocab = activity_vocab\n","    self.time_attrs = time_attrs\n","    self.stat_cat_attrs = static_catergorical_attrs\n","    self.stat_num_attrs = static_numerical_attrs\n","    self.dyn_cat_attrs = dynamic_categorical_attrs\n","    self.dyn_num_attrs = dynamic_numerical_attrs\n","    self.dyn_raw_attrs = dynamic_raw_attrs\n","\n","  def build_positional_encoding(self, input, inputs_month=None, inputs_yearday=None, inputs_day=None, inputs_weekday=None, inputs_hour=None, inputs_minute=None, inputs_second=None):\n","    if 'sincos' == self.pos_enc_mode:\n","      return input + keras_nlp.layers.SinePositionEncoding()(input)\n","    elif 'rotary' == self.pos_enc_mode:\n","      return keras_nlp.layers.RotaryEmbedding()(input)\n","    elif 'temporal' == self.pos_enc_mode:\n","      temp_enc = TemporalEncoding()(\n","        input,\n","        inputs_month=inputs_month,\n","        inputs_yearday=inputs_yearday,\n","        inputs_day=inputs_day,\n","        inputs_weekday=inputs_weekday,\n","        inputs_hour=inputs_hour,\n","        inputs_minute=inputs_minute,\n","        inputs_second=inputs_second,\n","      )\n","      pos_enc = keras_nlp.layers.PositionEmbedding(self.max_seq_len)(input)\n","      return input + pos_enc + temp_enc\n","    elif 'learned' == self.pos_enc_mode:\n","      return input + keras_nlp.layers.PositionEmbedding(self.max_seq_len)(input)\n","    else:\n","      return input\n","\n","  def build_time_inputs(self):\n","    input_shape = (self.max_seq_len,)\n","\n","    in_month, in_yday, in_day, in_wday, in_hour, in_min, in_sec = None, None, None, None, None, None, None\n","    for time_unit, attr in self.time_attrs.items():\n","      if 'month' == time_unit:\n","        in_month = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","      elif 'yearday' == time_unit:\n","        in_yday = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","      elif 'day' == time_unit:\n","        in_day = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","      elif 'weekday' == time_unit:\n","        in_wday = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","      elif 'hour' == time_unit:\n","        in_hour = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","      elif 'minute' == time_unit:\n","        in_min = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","      elif 'second' == time_unit:\n","        in_sec = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","\n","    return in_month, in_yday, in_day, in_wday, in_hour, in_min, in_sec\n","\n","  def build_dynamic_inputs(self, inputs_activity, mask_activity, inputs_month=None, inputs_yearday=None, inputs_day=None, inputs_weekday=None, inputs_hour=None, inputs_minute=None, inputs_second=None):\n","    input_shape = (self.max_seq_len,)\n","    input_shape_raw = (self.max_seq_len, self.embed_dim)\n","\n","    inputs = []\n","    outputs = []\n","\n","    for attr, mask_val in self.dyn_raw_attrs.items():\n","      input = keras.layers.Input(input_shape_raw, name=attr, dtype='float32')\n","      inputs.append(input)\n","\n","      masking = keras.layers.Masking(mask_value=mask_val)\n","      embedding_mask = masking.compute_mask(input)\n","      masking = masking(input)\n","\n","      pos_encoding = self.build_positional_encoding(masking, inputs_month, inputs_yearday, inputs_day, inputs_weekday, inputs_hour, inputs_minute, inputs_second)\n","\n","      output = DynamicAttributeSelectionNetwork(\n","        dec_num_heads=self.dec_num_heads,\n","        dec_ff_dim=self.dec_ff_dim,\n","        dec_dropout=self.dec_dropout,\n","        dec_activation=self.dec_activation,\n","        dec_layer_norm_epsilon=self.dec_layer_norm_epsilon,\n","        dec_kernel_init=self.dec_kernel_init,\n","        dec_bias_init=self.dec_bias_init,\n","        dec_normalize_first=self.dec_normalize_first,\n","        var_units=self.dyn_var_dim,\n","        var_dropout=self.dyn_var_dropout,\n","        var_activation=self.dyn_var_activation,\n","        grn_depth=self.dyn_var_grn_depth,\n","        grn_dropout=self.dyn_var_grn_dropout,\n","        grn_activation=self.dyn_var_grn_activation,\n","        glu_activation=self.glu_activation,\n","        flattening=self.flattening,\n","      )(inputs_activity, pos_encoding, encoder_padding_mask=embedding_mask, decoder_padding_mask=mask_activity)\n","      outputs.append(output)\n","\n","    for attr, vocab in self.dyn_cat_attrs.items():\n","      input = keras.layers.Input(input_shape, name=attr, dtype='string')\n","      inputs.append(input)\n","      embedding = StringEmbedding(\n","          vocab=vocab,\n","          embed_dim=self.embed_dim,\n","      )\n","      embedding_mask = embedding.compute_mask(input)\n","      embedding = embedding(input)\n","\n","      pos_encoding = self.build_positional_encoding(embedding, inputs_month, inputs_yearday, inputs_day, inputs_weekday, inputs_hour, inputs_minute, inputs_second)\n","\n","      output = DynamicAttributeSelectionNetwork(\n","        dec_num_heads=self.dec_num_heads,\n","        dec_ff_dim=self.dec_ff_dim,\n","        dec_dropout=self.dec_dropout,\n","        dec_activation=self.dec_activation,\n","        dec_layer_norm_epsilon=self.dec_layer_norm_epsilon,\n","        dec_kernel_init=self.dec_kernel_init,\n","        dec_bias_init=self.dec_bias_init,\n","        dec_normalize_first=self.dec_normalize_first,\n","        var_units=self.dyn_var_dim,\n","        var_dropout=self.dyn_var_dropout,\n","        var_activation=self.dyn_var_activation,\n","        grn_depth=self.dyn_var_grn_depth,\n","        grn_dropout=self.dyn_var_grn_dropout,\n","        grn_activation=self.dyn_var_grn_activation,\n","        glu_activation=self.glu_activation,\n","        flattening=self.flattening,\n","      )(inputs_activity, pos_encoding, encoder_padding_mask=embedding_mask, decoder_padding_mask=mask_activity)\n","      outputs.append(output)\n","\n","    for attr, vocab in self.dyn_num_attrs.items():\n","      input = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","      inputs.append(input)\n","\n","      embedding = NumericalEmbedding(embed_dim=self.embed_dim)\n","      embedding_mask = embedding.compute_mask(input)\n","      embedding = embedding(input)\n","\n","      pos_encoding = self.build_positional_encoding(embedding, inputs_month, inputs_yearday, inputs_day, inputs_weekday, inputs_hour, inputs_minute, inputs_second)\n","\n","      output = DynamicAttributeSelectionNetwork(\n","        dec_num_heads=self.dec_num_heads,\n","        dec_ff_dim=self.dec_ff_dim,\n","        dec_dropout=self.dec_dropout,\n","        dec_activation=self.dec_activation,\n","        dec_layer_norm_epsilon=self.dec_layer_norm_epsilon,\n","        dec_kernel_init=self.dec_kernel_init,\n","        dec_bias_init=self.dec_bias_init,\n","        dec_normalize_first=self.dec_normalize_first,\n","        var_units=self.dyn_var_dim,\n","        var_dropout=self.dyn_var_dropout,\n","        var_activation=self.dyn_var_activation,\n","        grn_depth=self.dyn_var_grn_depth,\n","        grn_dropout=self.dyn_var_grn_dropout,\n","        grn_activation=self.dyn_var_grn_activation,\n","        glu_activation=self.glu_activation,\n","        flattening=self.flattening,\n","      )(inputs_activity, pos_encoding, decoder_padding_mask=mask_activity, encoder_padding_mask=embedding_mask)\n","      outputs.append(output)\n","\n","    return inputs, outputs\n","\n","  def build_static_inputs(self):\n","    input_shape = (1,)\n","\n","    cat_inputs = []\n","    cat_outputs = []\n","    for attr, vocab in self.stat_cat_attrs.items():\n","      input = keras.layers.Input(input_shape, name=attr, dtype='string')\n","      cat_inputs.append(input)\n","\n","      embedding = StringEmbedding(\n","          vocab=vocab,\n","          embed_dim=self.embed_dim,\n","      )(input)\n","      cat_outputs.append(embedding)\n","\n","    num_inputs = []\n","    num_outputs = []\n","    for attr, vocab in self.stat_num_attrs.items():\n","      input = keras.layers.Input(input_shape, name=attr, dtype='float32')\n","      num_inputs.append(input)\n","\n","      embedding = NumericalEmbedding(embed_dim=self.embed_dim)(input)\n","      num_outputs.append(embedding)\n","\n","    if len(cat_outputs + num_outputs) > 0:\n","      output = StaticAttributeSelectionNetwork(\n","        var_units=self.stat_var_dim,\n","        var_dropout=self.stat_var_dropout,\n","        var_activation=self.stat_var_activation,\n","        grn_depth=self.stat_var_grn_depth,\n","        grn_dropout=self.stat_var_grn_dropout,\n","        grn_activation=self.stat_var_activation,\n","        glu_activation=self.glu_activation,\n","      )(cat_outputs + num_outputs)\n","      outputs = [output]\n","    else:\n","      outputs = []\n","\n","    return cat_inputs + num_inputs, outputs\n","\n","  def build_outputs(self):\n","    ffns = []\n","    outputs = []\n","    for output_name, output_config in self.out_dict.items():\n","      seq = keras.models.Sequential()\n","      for ff_dim in self.ff_dims:\n","        seq.add(keras.layers.Dense(units=ff_dim, activation=self.ff_activation))\n","        seq.add(keras.layers.Dropout(self.ff_dropout))\n","\n","      outputs.append(keras.layers.Dense(units=output_config['units'], activation=output_config['activation'], name=output_name))\n","      ffns.append(seq)\n","\n","    return ffns, outputs\n","\n","  def build_graph(self):\n","    input_shape = (self.max_seq_len,)\n","\n","    in_activity = keras.layers.Input(input_shape, name=\"activity\", dtype='string')\n","    in_month, in_yday, in_day, in_wday, in_hour, in_min, in_sec = self.build_time_inputs()\n","    time_inputs = [input for input in [in_month, in_yday, in_day, in_wday, in_hour, in_min, in_sec] if input is not None]\n","\n","    activity_embed = StringEmbedding(\n","      vocab=self.activity_vocab,\n","      embed_dim=self.embed_dim,\n","    )\n","    activity_mask = activity_embed.compute_mask(in_activity)\n","    activity_embed = activity_embed(in_activity)\n","\n","    activity_pos = self.build_positional_encoding(activity_embed, in_month, in_yday, in_day, in_wday, in_hour, in_min, in_sec)\n","\n","    activity_output, activity_encoder = ActivityEncoderNetwork(\n","      enc_num_heads=self.enc_num_heads,\n","      enc_ff_dim=self.enc_ff_dim,\n","      enc_dropout=self.enc_dropout,\n","      enc_activation=self.enc_activation,\n","      enc_layer_norm_epsilon=self.enc_layer_norm_epsilon,\n","      enc_kernel_init=self.enc_kernel_init,\n","      enc_bias_init=self.enc_bias_init,\n","      enc_normalize_first=self.enc_normalize_first,\n","      var_units=self.enc_var_units,\n","      var_dropout=self.enc_var_dropout,\n","      var_activation=self.enc_var_activation,\n","      grn_depth=self.enc_grn_depth,\n","      grn_dropout=self.enc_grn_dropout,\n","      grn_activation=self.enc_grn_activation,\n","      glu_activation=self.glu_activation,\n","      flattening=self.flattening,\n","    )(activity_pos, padding_mask=activity_mask)\n","\n","    time_output, time_encoder = TimestampEncoderNetwork(\n","      enc_num_heads=self.time_num_heads,\n","      enc_ff_dim=self.time_ff_dim,\n","      enc_dropout=self.time_dropout,\n","      enc_activation=self.time_activation,\n","      enc_layer_norm_epsilon=self.time_layer_norm_epsilon,\n","      enc_kernel_init=self.time_kernel_init,\n","      enc_bias_init=self.time_bias_init,\n","      enc_normalize_first=self.time_normalize_first,\n","      var_units=self.time_var_units,\n","      var_dropout=self.time_var_dropout,\n","      var_activation=self.time_var_activation,\n","      grn_depth=self.time_grn_depth,\n","      grn_dropout=self.time_grn_dropout,\n","      grn_activation=self.time_grn_activation,\n","      glu_activation=self.glu_activation,\n","      flattening=self.flattening,\n","    )(keras.ops.stack(time_inputs, axis=-1))\n","\n","    dyn_inputs, dyn_outputs = self.build_dynamic_inputs(activity_encoder, activity_mask, in_month, in_yday, in_day, in_wday, in_hour, in_min, in_sec)\n","    stat_inputs, stat_outputs = self.build_static_inputs()\n","\n","    x = keras.layers.Concatenate()(dyn_outputs + stat_outputs + [activity_output, time_output])\n","    x = keras.layers.LayerNormalization(epsilon=self.ff_layer_norm_eps)(x)\n","\n","    ffns, outputs = self.build_outputs()\n","    x_outs = []\n","    for ffn, output in zip(ffns, outputs):\n","      x_out = ffn(x)\n","      x_out = output(x_out)\n","      x_outs.append(x_out)\n","\n","    return keras.Model(inputs=dyn_inputs + stat_inputs + time_inputs + [in_activity], outputs=x_outs, name=self.name)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xd8eI7ixyMVx"},"outputs":[],"source":["class MulticlassHierarchicalProcessTransformer(BaseHierarchicalProcessTransformer, MulticlassModelMixin):\n","  def __init__(\n","    self,\n","    max_sequence_len: int,\n","    out_dim: int,\n","    embed_dim: int,\n","    encoder_num_heads: int,\n","    encoder_ff_dim: int,\n","    encoder_var_units: int,\n","    decoder_num_heads: int,\n","    decoder_ff_dim: int,\n","    time_num_heads: int,\n","    time_ff_dim: int,\n","    time_var_units: int,\n","    ff_dims: List[int],\n","    dynamic_var_net_dim: int,\n","    static_var_net_dim: int,\n","    activity_vocab: List[str],\n","    allow_negative: bool = True,\n","    time_attrs: Dict[Literal['second', 'minute', 'hour', 'day', 'weekday', 'yearday'], str] = {},\n","    static_catergorical_attrs: Dict[str, List[str]] = {},\n","    static_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    dynamic_categorical_attrs: Dict[str, List[str]] = {},\n","    dynamic_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    position_encoder_mode: Literal['sincos', 'rotary', 'temporal', 'learned'] = 'learned',\n","    encoder_dropout: float = 0.0,\n","    encoder_activation: str = 'relu',\n","    encoder_layer_norm_epsilon: float = 1e-05,\n","    encoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    encoder_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    encoder_normalize_first: bool = False,\n","    encoder_var_activation: str = 'elu',\n","    encoder_var_dropout: float = 0.0,\n","    encoder_grn_depth: int = 0,\n","    encoder_grn_activation: str = 'elu',\n","    encoder_grn_dropout: float = 0.0,\n","    decoder_dropout: float = 0.0,\n","    decoder_activation: str ='relu',\n","    decoder_layer_norm_epsilon: float = 1e-05,\n","    decoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    decoder_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    decoder_normalize_first: bool = False,\n","    dynamic_var_net_dropout: float = 0.0,\n","    dynamic_var_net_activation: str = 'elu',\n","    dynamic_var_grn_depth: int = 0,\n","    dynamic_var_grn_activation: str = 'elu',\n","    dynamic_var_grn_dropout: float = 0.0,\n","    static_var_net_dropout: float = 0.0,\n","    static_var_net_activation: str = 'elu',\n","    static_var_grn_depth: int = 0,\n","    static_var_grn_activation: str = 'elu',\n","    static_var_grn_dropout: float = 0.0,\n","    time_dropout: float = 0.0,\n","    time_activation: str ='relu',\n","    time_layer_norm_epsilon: float = 1e-05,\n","    time_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    time_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    time_normalize_first: bool = False,\n","    time_var_activation: str = 'elu',\n","    time_var_dropout: float = 0.0,\n","    time_grn_depth: int = 0,\n","    time_grn_activation: str = 'elu',\n","    time_grn_dropout: float = 0.0,\n","    ff_dropout: float = 0.0,\n","    ff_activation: str = 'relu',\n","    ff_layer_norm_eps: float = 1e-05,\n","    glu_activation: str = 'sigmoid',\n","    flattening: Literal['vsn', 'avg_pool', 'max_pool', 'flatten'] = 'vsn',\n","    out_name: Optional[str] = None,\n","    name: Optional[str] = None,\n","    **kwargs\n","  ):\n","    super().__init__(\n","      max_sequence_len=max_sequence_len,\n","      embed_dim=embed_dim,\n","      encoder_num_heads=encoder_num_heads,\n","      encoder_ff_dim=encoder_ff_dim,\n","      encoder_var_units=encoder_var_units,\n","      decoder_num_heads=decoder_num_heads,\n","      decoder_ff_dim=decoder_ff_dim,\n","      time_num_heads=time_num_heads,\n","      time_ff_dim=time_ff_dim,\n","      time_var_units=time_var_units,\n","      ff_dims=ff_dims,\n","      dynamic_var_net_dim=dynamic_var_net_dim,\n","      static_var_net_dim=static_var_net_dim,\n","      activity_vocab=activity_vocab,\n","      time_attrs=time_attrs,\n","      static_catergorical_attrs=static_catergorical_attrs,\n","      static_numerical_attrs=static_numerical_attrs,\n","      dynamic_categorical_attrs=dynamic_categorical_attrs,\n","      dynamic_numerical_attrs=dynamic_numerical_attrs,\n","      position_encoder_mode=position_encoder_mode,\n","      encoder_dropout=encoder_dropout,\n","      encoder_activation=encoder_activation,\n","      encoder_layer_norm_epsilon=encoder_layer_norm_epsilon,\n","      encoder_kernel_init=encoder_kernel_init,\n","      encoder_bias_init=encoder_bias_init,\n","      encoder_normalize_first=encoder_normalize_first,\n","      encoder_var_activation=encoder_var_activation,\n","      encoder_var_dropout=encoder_var_dropout,\n","      encoder_grn_depth=encoder_grn_depth,\n","      encoder_grn_activation=encoder_grn_activation,\n","      encoder_grn_dropout=encoder_grn_dropout,\n","      decoder_dropout=decoder_dropout,\n","      decoder_activation=decoder_activation,\n","      decoder_layer_norm_epsilon=decoder_layer_norm_epsilon,\n","      decoder_kernel_init=decoder_kernel_init,\n","      decoder_bias_init=decoder_bias_init,\n","      decoder_normalize_first=decoder_normalize_first,\n","      dynamic_var_net_dropout=dynamic_var_net_dropout,\n","      dynamic_var_net_activation=dynamic_var_net_activation,\n","      dynamic_var_grn_depth=dynamic_var_grn_depth,\n","      dynamic_var_grn_activation=dynamic_var_grn_activation,\n","      dynamic_var_grn_dropout=dynamic_var_grn_dropout,\n","      static_var_net_dropout=static_var_net_dropout,\n","      static_var_net_activation=static_var_net_activation,\n","      static_var_grn_depth=static_var_grn_depth,\n","      static_var_grn_activation=static_var_grn_activation,\n","      static_var_grn_dropout=static_var_grn_dropout,\n","      time_dropout=time_dropout,\n","      time_activation=time_activation,\n","      time_layer_norm_epsilon=time_layer_norm_epsilon,\n","      time_kernel_init=time_kernel_init,\n","      time_bias_init=time_bias_init,\n","      time_normalize_first=time_normalize_first,\n","      time_var_activation=time_var_activation,\n","      time_var_dropout=time_var_dropout,\n","      time_grn_depth=time_grn_depth,\n","      time_grn_activation=time_grn_activation,\n","      time_grn_dropout=time_grn_dropout,\n","      ff_dropout=ff_dropout,\n","      ff_activation=ff_activation,\n","      ff_layer_norm_eps=ff_layer_norm_eps,\n","      glu_activation=glu_activation,\n","      flattening=flattening,\n","      out_dict={out_name: {'activation': 'softmax', 'units': out_dim}},\n","      name=f\"{name}_{out_name}_multiclassification\",\n","      **kwargs\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYNiI-H61P_U"},"outputs":[],"source":["class BinaryclassHierarchicalProcessTransformer(BaseHierarchicalProcessTransformer, BinaryclassModelMixin):\n","  def __init__(\n","    self,\n","    max_sequence_len: int,\n","    embed_dim: int,\n","    encoder_num_heads: int,\n","    encoder_ff_dim: int,\n","    encoder_var_units: int,\n","    decoder_num_heads: int,\n","    decoder_ff_dim: int,\n","    time_num_heads: int,\n","    time_ff_dim: int,\n","    time_var_units: int,\n","    ff_dims: List[int],\n","    dynamic_var_net_dim: int,\n","    static_var_net_dim: int,\n","    activity_vocab: List[str],\n","    time_attrs: Dict[Literal['second', 'minute', 'hour', 'day', 'weekday', 'yearday'], str] = {},\n","    static_catergorical_attrs: Dict[str, List[str]] = {},\n","    static_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    dynamic_categorical_attrs: Dict[str, List[str]] = {},\n","    dynamic_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    position_encoder_mode: Literal['sincos', 'rotary', 'temporal', 'learned'] = 'learned',\n","    encoder_dropout: float = 0.0,\n","    encoder_activation: str ='relu',\n","    encoder_layer_norm_epsilon: float = 1e-05,\n","    encoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    encoder_bias_init: Union[str, keras.Initializer] ='zeros',\n","    encoder_normalize_first: bool = False,\n","    encoder_var_activation: str = 'elu',\n","    encoder_var_dropout: float = 0.0,\n","    encoder_grn_depth: int = 0,\n","    encoder_grn_activation: str = 'elu',\n","    encoder_grn_dropout: float = 0.0,\n","    decoder_dropout: float = 0.0,\n","    decoder_activation: str ='relu',\n","    decoder_layer_norm_epsilon: float = 1e-05,\n","    decoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    decoder_bias_init: Union[str, keras.Initializer] ='zeros',\n","    decoder_normalize_first: bool = False,\n","    dynamic_var_net_dropout: float = 0.0,\n","    dynamic_var_net_activation: str = 'elu',\n","    dynamic_var_grn_depth: int = 0,\n","    dynamic_var_grn_activation: str = 'elu',\n","    dynamic_var_grn_dropout: float = 0.0,\n","    static_var_net_dropout: float = 0.0,\n","    static_var_net_activation: str = 'elu',\n","    static_var_grn_depth: int = 0,\n","    static_var_grn_activation: str = 'elu',\n","    static_var_grn_dropout: float = 0.0,\n","    time_dropout: float = 0.0,\n","    time_activation: str ='relu',\n","    time_layer_norm_epsilon: float = 1e-05,\n","    time_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    time_bias_init: Union[str, keras.Initializer] ='zeros',\n","    time_normalize_first: bool = False,\n","    time_var_activation: str = 'elu',\n","    time_var_dropout: float = 0.0,\n","    time_grn_depth: int = 0,\n","    time_grn_activation: str = 'elu',\n","    time_grn_dropout: float = 0.0,\n","    ff_dropout: float = 0.0,\n","    ff_activation: str = 'relu',\n","    ff_layer_norm_eps: float = 1e-05,\n","    out_name: Optional[str] = None,\n","    name: Optional[str] = None,\n","    **kwargs\n","  ):\n","    super().__init__(\n","      max_sequence_len=max_sequence_len,\n","      embed_dim=embed_dim,\n","      encoder_num_heads=encoder_num_heads,\n","      encoder_ff_dim=encoder_ff_dim,\n","      encoder_var_units=encoder_var_units,\n","      decoder_num_heads=decoder_num_heads,\n","      decoder_ff_dim=decoder_ff_dim,\n","      time_num_heads=time_num_heads,\n","      time_ff_dim=time_ff_dim,\n","      time_var_units=time_var_units,\n","      ff_dims=ff_dims,\n","      dynamic_var_net_dim=dynamic_var_net_dim,\n","      static_var_net_dim=static_var_net_dim,\n","      activity_vocab=activity_vocab,\n","      time_attrs=time_attrs,\n","      static_catergorical_attrs=static_catergorical_attrs,\n","      static_numerical_attrs=static_numerical_attrs,\n","      dynamic_categorical_attrs=dynamic_categorical_attrs,\n","      dynamic_numerical_attrs=dynamic_numerical_attrs,\n","      position_encoder_mode=position_encoder_mode,\n","      encoder_dropout=encoder_dropout,\n","      encoder_activation=encoder_activation,\n","      encoder_layer_norm_epsilon=encoder_layer_norm_epsilon,\n","      encoder_kernel_init=encoder_kernel_init,\n","      encoder_bias_init=encoder_bias_init,\n","      encoder_normalize_first=encoder_normalize_first,\n","      encoder_var_activation=encoder_var_activation,\n","      encoder_var_dropout=encoder_var_dropout,\n","      encoder_grn_depth=encoder_grn_depth,\n","      encoder_grn_activation=encoder_grn_activation,\n","      encoder_grn_dropout=encoder_grn_dropout,\n","      decoder_dropout=decoder_dropout,\n","      decoder_activation=decoder_activation,\n","      decoder_layer_norm_epsilon=decoder_layer_norm_epsilon,\n","      decoder_kernel_init=decoder_kernel_init,\n","      decoder_bias_init=decoder_bias_init,\n","      decoder_normalize_first=decoder_normalize_first,\n","      dynamic_var_net_dropout=dynamic_var_net_dropout,\n","      dynamic_var_net_activation=dynamic_var_net_activation,\n","      dynamic_var_grn_depth=dynamic_var_grn_depth,\n","      dynamic_var_grn_activation=dynamic_var_grn_activation,\n","      dynamic_var_grn_dropout=dynamic_var_grn_dropout,\n","      static_var_net_dropout=static_var_net_dropout,\n","      static_var_net_activation=static_var_net_activation,\n","      static_var_grn_depth=static_var_grn_depth,\n","      static_var_grn_activation=static_var_grn_activation,\n","      static_var_grn_dropout=static_var_grn_dropout,\n","      time_dropout=time_dropout,\n","      time_activation=time_activation,\n","      time_layer_norm_epsilon=time_layer_norm_epsilon,\n","      time_kernel_init=time_kernel_init,\n","      time_bias_init=time_bias_init,\n","      time_normalize_first=time_normalize_first,\n","      time_var_activation=time_var_activation,\n","      time_var_dropout=time_var_dropout,\n","      time_grn_depth=time_grn_depth,\n","      time_grn_activation=time_grn_activation,\n","      time_grn_dropout=time_grn_dropout,\n","      ff_dropout=ff_dropout,\n","      ff_activation=ff_activation,\n","      ff_layer_norm_eps=ff_layer_norm_eps,\n","      out_dict={out_name: {'units': 1, 'activation': out_activation}},\n","      name=f\"{name}_{out_name}_binaryclass\",\n","      **kwargs\n","    )\n","\n","  @property\n","  def default_monitor_metric(self):\n","    return f\"val_accuracy\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYpNVxyAEFbR"},"outputs":[],"source":["class RegressionHierarchicalProcessTransformer(BaseHierarchicalProcessTransformer, RegressionModelMixin):\n","  def __init__(\n","    self,\n","    max_sequence_len: int,\n","    embed_dim: int,\n","    encoder_num_heads: int,\n","    encoder_ff_dim: int,\n","    encoder_var_units: int,\n","    decoder_num_heads: int,\n","    decoder_ff_dim: int,\n","    time_num_heads: int,\n","    time_ff_dim: int,\n","    time_var_units: int,\n","    ff_dims: List[int],\n","    dynamic_var_net_dim: int,\n","    static_var_net_dim: int,\n","    activity_vocab: List[str],\n","    allow_negative: bool = True,\n","    time_attrs: Dict[Literal['second', 'minute', 'hour', 'day', 'weekday', 'yearday'], str] = {},\n","    static_catergorical_attrs: Dict[str, List[str]] = {},\n","    static_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    dynamic_categorical_attrs: Dict[str, List[str]] = {},\n","    dynamic_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    position_encoder_mode: Literal['sincos', 'rotary', 'temporal', 'learned'] = 'learned',\n","    encoder_dropout: float = 0.0,\n","    encoder_activation: str = 'relu',\n","    encoder_layer_norm_epsilon: float = 1e-05,\n","    encoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    encoder_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    encoder_normalize_first: bool = False,\n","    encoder_var_activation: str = 'elu',\n","    encoder_var_dropout: float = 0.0,\n","    encoder_grn_depth: int = 0,\n","    encoder_grn_activation: str = 'elu',\n","    encoder_grn_dropout: float = 0.0,\n","    decoder_dropout: float = 0.0,\n","    decoder_activation: str ='relu',\n","    decoder_layer_norm_epsilon: float = 1e-05,\n","    decoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    decoder_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    decoder_normalize_first: bool = False,\n","    dynamic_var_net_dropout: float = 0.0,\n","    dynamic_var_net_activation: str = 'elu',\n","    dynamic_var_grn_depth: int = 0,\n","    dynamic_var_grn_activation: str = 'elu',\n","    dynamic_var_grn_dropout: float = 0.0,\n","    static_var_net_dropout: float = 0.0,\n","    static_var_net_activation: str = 'elu',\n","    static_var_grn_depth: int = 0,\n","    static_var_grn_activation: str = 'elu',\n","    static_var_grn_dropout: float = 0.0,\n","    time_dropout: float = 0.0,\n","    time_activation: str ='relu',\n","    time_layer_norm_epsilon: float = 1e-05,\n","    time_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    time_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    time_normalize_first: bool = False,\n","    time_var_activation: str = 'elu',\n","    time_var_dropout: float = 0.0,\n","    time_grn_depth: int = 0,\n","    time_grn_activation: str = 'elu',\n","    time_grn_dropout: float = 0.0,\n","    ff_dropout: float = 0.0,\n","    ff_activation: str = 'relu',\n","    ff_layer_norm_eps: float = 1e-05,\n","    glu_activation: str = 'sigmoid',\n","    flattening: Literal['vsn', 'avg_pool', 'max_pool', 'flatten'] = 'vsn',\n","    out_name: Optional[str] = None,\n","    name: Optional[str] = None,\n","    **kwargs\n","  ):\n","    super().__init__(\n","      max_sequence_len=max_sequence_len,\n","      embed_dim=embed_dim,\n","      encoder_num_heads=encoder_num_heads,\n","      encoder_ff_dim=encoder_ff_dim,\n","      encoder_var_units=encoder_var_units,\n","      decoder_num_heads=decoder_num_heads,\n","      decoder_ff_dim=decoder_ff_dim,\n","      time_num_heads=time_num_heads,\n","      time_ff_dim=time_ff_dim,\n","      time_var_units=time_var_units,\n","      ff_dims=ff_dims,\n","      dynamic_var_net_dim=dynamic_var_net_dim,\n","      static_var_net_dim=static_var_net_dim,\n","      activity_vocab=activity_vocab,\n","      time_attrs=time_attrs,\n","      static_catergorical_attrs=static_catergorical_attrs,\n","      static_numerical_attrs=static_numerical_attrs,\n","      dynamic_categorical_attrs=dynamic_categorical_attrs,\n","      dynamic_numerical_attrs=dynamic_numerical_attrs,\n","      position_encoder_mode=position_encoder_mode,\n","      encoder_dropout=encoder_dropout,\n","      encoder_activation=encoder_activation,\n","      encoder_layer_norm_epsilon=encoder_layer_norm_epsilon,\n","      encoder_kernel_init=encoder_kernel_init,\n","      encoder_bias_init=encoder_bias_init,\n","      encoder_normalize_first=encoder_normalize_first,\n","      encoder_var_activation=encoder_var_activation,\n","      encoder_var_dropout=encoder_var_dropout,\n","      encoder_grn_depth=encoder_grn_depth,\n","      encoder_grn_activation=encoder_grn_activation,\n","      encoder_grn_dropout=encoder_grn_dropout,\n","      decoder_dropout=decoder_dropout,\n","      decoder_activation=decoder_activation,\n","      decoder_layer_norm_epsilon=decoder_layer_norm_epsilon,\n","      decoder_kernel_init=decoder_kernel_init,\n","      decoder_bias_init=decoder_bias_init,\n","      decoder_normalize_first=decoder_normalize_first,\n","      dynamic_var_net_dropout=dynamic_var_net_dropout,\n","      dynamic_var_net_activation=dynamic_var_net_activation,\n","      dynamic_var_grn_depth=dynamic_var_grn_depth,\n","      dynamic_var_grn_activation=dynamic_var_grn_activation,\n","      dynamic_var_grn_dropout=dynamic_var_grn_dropout,\n","      static_var_net_dropout=static_var_net_dropout,\n","      static_var_net_activation=static_var_net_activation,\n","      static_var_grn_depth=static_var_grn_depth,\n","      static_var_grn_activation=static_var_grn_activation,\n","      static_var_grn_dropout=static_var_grn_dropout,\n","      time_dropout=time_dropout,\n","      time_activation=time_activation,\n","      time_layer_norm_epsilon=time_layer_norm_epsilon,\n","      time_kernel_init=time_kernel_init,\n","      time_bias_init=time_bias_init,\n","      time_normalize_first=time_normalize_first,\n","      time_var_activation=time_var_activation,\n","      time_var_dropout=time_var_dropout,\n","      time_grn_depth=time_grn_depth,\n","      time_grn_activation=time_grn_activation,\n","      time_grn_dropout=time_grn_dropout,\n","      ff_dropout=ff_dropout,\n","      ff_activation=ff_activation,\n","      ff_layer_norm_eps=ff_layer_norm_eps,\n","      glu_activation=glu_activation,\n","      flattening=flattening,\n","      out_dict={out_name: {'units': 1, 'activation': 'linear' if allow_negative else 'relu'}},\n","      name=f\"{name}_{out_name}_regression\",\n","      **kwargs\n","    )\n","  @property\n","  def default_monitor_metric(self):\n","    return f\"val_logcosh\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8G-0GDCkdTT"},"outputs":[],"source":["class MultitaskHierarchicalProcessTransformer(BaseHierarchicalProcessTransformer, BaseModelMixin):\n","  def __init__(\n","    self,\n","    max_sequence_len: int,\n","    embed_dim: int,\n","    next_activity_out_dim: int,\n","    encoder_num_heads: int,\n","    encoder_ff_dim: int,\n","    encoder_var_units: int,\n","    decoder_num_heads: int,\n","    decoder_ff_dim: int,\n","    time_num_heads: int,\n","    time_ff_dim: int,\n","    time_var_units: int,\n","    ff_dims: List[int],\n","    dynamic_var_net_dim: int,\n","    static_var_net_dim: int,\n","    activity_vocab: List[str],\n","    allow_negative: bool = True,\n","    time_attrs: Dict[Literal['second', 'minute', 'hour', 'day', 'weekday', 'yearday'], str] = {},\n","    static_catergorical_attrs: Dict[str, List[str]] = {},\n","    static_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    dynamic_categorical_attrs: Dict[str, List[str]] = {},\n","    dynamic_numerical_attrs: Dict[str, Optional[List[float]]] = {},\n","    dynamic_raw_attrs: Dict[str, List[str]] = {},\n","    position_encoder_mode: Literal['sincos', 'rotary', 'temporal', 'learned'] = 'learned',\n","    encoder_dropout: float = 0.0,\n","    encoder_activation: str = 'relu',\n","    encoder_layer_norm_epsilon: float = 1e-05,\n","    encoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    encoder_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    encoder_normalize_first: bool = False,\n","    encoder_var_activation: str = 'elu',\n","    encoder_var_dropout: float = 0.0,\n","    encoder_grn_depth: int = 0,\n","    encoder_grn_activation: str = 'elu',\n","    encoder_grn_dropout: float = 0.0,\n","    decoder_dropout: float = 0.0,\n","    decoder_activation: str ='relu',\n","    decoder_layer_norm_epsilon: float = 1e-05,\n","    decoder_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    decoder_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    decoder_normalize_first: bool = False,\n","    dynamic_var_net_dropout: float = 0.0,\n","    dynamic_var_net_activation: str = 'elu',\n","    dynamic_var_grn_depth: int = 0,\n","    dynamic_var_grn_activation: str = 'elu',\n","    dynamic_var_grn_dropout: float = 0.0,\n","    static_var_net_dropout: float = 0.0,\n","    static_var_net_activation: str = 'elu',\n","    static_var_grn_depth: int = 0,\n","    static_var_grn_activation: str = 'elu',\n","    static_var_grn_dropout: float = 0.0,\n","    time_dropout: float = 0.0,\n","    time_activation: str ='relu',\n","    time_layer_norm_epsilon: float = 1e-05,\n","    time_kernel_init: Union[str, keras.Initializer] = 'glorot_uniform',\n","    time_bias_init: Union[str, keras.Initializer] = 'zeros',\n","    time_normalize_first: bool = False,\n","    time_var_activation: str = 'elu',\n","    time_var_dropout: float = 0.0,\n","    time_grn_depth: int = 0,\n","    time_grn_activation: str = 'elu',\n","    time_grn_dropout: float = 0.0,\n","    ff_dropout: float = 0.0,\n","    ff_activation: str = 'relu',\n","    ff_layer_norm_eps: float = 1e-05,\n","    glu_activation: str = 'sigmoid',\n","    flattening: Literal['vsn', 'avg_pool', 'max_pool', 'flatten'] = 'vsn',\n","    remaining_time_out_name: str = DEFAULT_REMAINING_TIME_OUTPUT,\n","    next_time_out_name: str = DEFAULT_NEXT_TIME_OUTPUT,\n","    next_activity_out_name: str = DEFAULT_NEXT_ACTIVITY_OUTPUT,\n","    name: Optional[str] = None,\n","    **kwargs\n","  ):\n","    super().__init__(\n","      max_sequence_len=max_sequence_len,\n","      embed_dim=embed_dim,\n","      encoder_num_heads=encoder_num_heads,\n","      encoder_ff_dim=encoder_ff_dim,\n","      encoder_var_units=encoder_var_units,\n","      decoder_num_heads=decoder_num_heads,\n","      decoder_ff_dim=decoder_ff_dim,\n","      time_num_heads=time_num_heads,\n","      time_ff_dim=time_ff_dim,\n","      time_var_units=time_var_units,\n","      ff_dims=ff_dims,\n","      dynamic_var_net_dim=dynamic_var_net_dim,\n","      static_var_net_dim=static_var_net_dim,\n","      activity_vocab=activity_vocab,\n","      time_attrs=time_attrs,\n","      static_catergorical_attrs=static_catergorical_attrs,\n","      static_numerical_attrs=static_numerical_attrs,\n","      dynamic_categorical_attrs=dynamic_categorical_attrs,\n","      dynamic_numerical_attrs=dynamic_numerical_attrs,\n","      dynamic_raw_attrs=dynamic_raw_attrs,\n","      position_encoder_mode=position_encoder_mode,\n","      encoder_dropout=encoder_dropout,\n","      encoder_activation=encoder_activation,\n","      encoder_layer_norm_epsilon=encoder_layer_norm_epsilon,\n","      encoder_kernel_init=encoder_kernel_init,\n","      encoder_bias_init=encoder_bias_init,\n","      encoder_normalize_first=encoder_normalize_first,\n","      encoder_var_activation=encoder_var_activation,\n","      encoder_var_dropout=encoder_var_dropout,\n","      encoder_grn_depth=encoder_grn_depth,\n","      encoder_grn_activation=encoder_grn_activation,\n","      encoder_grn_dropout=encoder_grn_dropout,\n","      decoder_dropout=decoder_dropout,\n","      decoder_activation=decoder_activation,\n","      decoder_layer_norm_epsilon=decoder_layer_norm_epsilon,\n","      decoder_kernel_init=decoder_kernel_init,\n","      decoder_bias_init=decoder_bias_init,\n","      decoder_normalize_first=decoder_normalize_first,\n","      dynamic_var_net_dropout=dynamic_var_net_dropout,\n","      dynamic_var_net_activation=dynamic_var_net_activation,\n","      dynamic_var_grn_depth=dynamic_var_grn_depth,\n","      dynamic_var_grn_activation=dynamic_var_grn_activation,\n","      dynamic_var_grn_dropout=dynamic_var_grn_dropout,\n","      static_var_net_dropout=static_var_net_dropout,\n","      static_var_net_activation=static_var_net_activation,\n","      static_var_grn_depth=static_var_grn_depth,\n","      static_var_grn_activation=static_var_grn_activation,\n","      static_var_grn_dropout=static_var_grn_dropout,\n","      time_dropout=time_dropout,\n","      time_activation=time_activation,\n","      time_layer_norm_epsilon=time_layer_norm_epsilon,\n","      time_kernel_init=time_kernel_init,\n","      time_bias_init=time_bias_init,\n","      time_normalize_first=time_normalize_first,\n","      time_var_activation=time_var_activation,\n","      time_var_dropout=time_var_dropout,\n","      time_grn_depth=time_grn_depth,\n","      time_grn_activation=time_grn_activation,\n","      time_grn_dropout=time_grn_dropout,\n","      ff_dropout=ff_dropout,\n","      ff_activation=ff_activation,\n","      ff_layer_norm_eps=ff_layer_norm_eps,\n","      glu_activation=glu_activation,\n","      flattening=flattening,\n","      out_dict={next_activity_out_name: {'units': next_activity_out_dim, 'activation': 'softmax'}, next_time_out_name: {'units': 1, 'activation': 'linear' if allow_negative else 'relu'}, remaining_time_out_name: {'units': 1, 'activation': 'linear' if allow_negative else 'relu'}},\n","      name=f\"{name}_multi\",\n","    )\n","\n","    self.remaining_time_out_name = remaining_time_out_name\n","    self.next_time_out_name = next_time_out_name\n","    self.next_activity_out_name = next_activity_out_name\n","\n","  @property\n","  def default_loss(self):\n","    return {\n","      self.remaining_time_out_name: keras.losses.LogCosh(),\n","      self.next_time_out_name: keras.losses.LogCosh(),\n","      self.next_activity_out_name: keras.losses.SparseCategoricalCrossentropy(),\n","    }\n","\n","  @property\n","  def default_metrics(self):\n","    return {\n","      self.remaining_time_out_name: [\n","        keras.metrics.MeanAbsoluteError(),\n","        keras.metrics.MeanSquaredError(),\n","        keras.metrics.RootMeanSquaredError(),\n","        keras.metrics.MeanSquaredLogarithmicError(),\n","        keras.metrics.MeanAbsolutePercentageError(),\n","        keras.metrics.LogCoshError(),\n","      ],\n","      self.next_time_out_name: [\n","        keras.metrics.MeanAbsoluteError(),\n","        keras.metrics.MeanSquaredError(),\n","        keras.metrics.RootMeanSquaredError(),\n","        keras.metrics.MeanSquaredLogarithmicError(),\n","        keras.metrics.MeanAbsolutePercentageError(),\n","        keras.metrics.LogCoshError(),\n","      ],\n","      self.next_activity_out_name: [\n","        keras.metrics.SparseCategoricalAccuracy(),\n","        keras.metrics.F1Score(average='macro', name=\"f1_score_macro\"),\n","        keras.metrics.F1Score(average='micro', name=\"f1_score_micro\"),\n","        keras.metrics.F1Score(average='weighted', name=\"f1_score_weighted\"),\n","        keras.metrics.SparseCategoricalCrossentropy(),\n","      ],\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"UNVWirPA_G5G"},"source":["# Dataset: Incident Management Process Enriched Event Log"]},{"cell_type":"markdown","metadata":{"id":"VkzmiuXjhXck"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EkiyeFmWWNi1"},"outputs":[],"source":["df_servicenow = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"incident_event_log_labeled.feather\"))\n","df_servicenow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vrzmhonc0ykt"},"outputs":[],"source":["df_servicenow_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"incident_event_log_train.feather\"))\n","df_servicenow_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBG4xeod02_p"},"outputs":[],"source":["df_servicenow_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"incident_event_log_test.feather\"))\n","df_servicenow_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0bg9U_UX5Ir"},"outputs":[],"source":["PROJECT_NAME = \"servicenow\"\n","MAX_SEQ_LEN = 13\n","TRAIN_STEPS_EPOCH = len(df_servicenow_train) // DEFAULT_BATCH_SIZE\n","\n","\n","TIME_ATTRS = {\n","    'month': \"time_timestamp_month\",\n","    'hour': \"time_timestamp_hour\",\n","    'day': \"time_timestamp_day\",\n","    'weekday': \"time_timestamp_weekday\",\n","    'yearday': \"time_timestamp_dayofyear\",\n","}\n","ACTIVITY_VOCAB = np.array(df_servicenow[EVENTLOG_ACTIVITY].unique().to_list() + [TOKEN_NA, TOKEN_EOC])\n","\n","INTERVAL_SINCE_LAST_EVENT_VOCAB = df_servicenow_train[\"time:timestamp:elapsedprev:seconds\"].to_numpy()\n","DAY_OF_WEEK_VOCAB = df_servicenow_train[\"time:timestamp:weekday:raw\"].to_numpy()\n","HOUR_OF_DAY_VOCAB = df_servicenow_train[\"time:timestamp:hour:raw\"].to_numpy()\n","\n","STATIC_CATEGORICAL_ATTRS = {\n","    \"case_opened_by\": df_servicenow[\"case:opened_by\"].unique().to_numpy(dtype=str),\n","    \"case_sys_created_by\": df_servicenow[\"case:sys_created_by\"].unique().to_numpy(dtype=str),\n","}\n","STATIC_NUMERICAL_ATTRS = {\n","    \"case_notify\": None,\n","}\n","DYNAMIC_CATEGORICAL_ATTRS = {\n","    \"category\": df_servicenow[\"category\"].unique().to_numpy(dtype=str),\n","    \"subcategory\": df_servicenow[\"subcategory\"].unique().to_numpy(dtype=str),\n","    \"location\": df_servicenow[\"location\"].unique().to_numpy(dtype=str),\n","    \"u_symptom\": df_servicenow[\"u_symptom\"].unique().to_numpy(dtype=str),\n","    \"caller_id\": df_servicenow[\"caller_id\"].unique().to_numpy(dtype=str),\n","    \"sys_updated_by\": df_servicenow[\"sys_updated_by\"].unique().to_numpy(dtype=str),\n","    \"org_group\": df_servicenow[\"org:group\"].unique().to_numpy(dtype=str),\n","    \"org_resource\": df_servicenow[\"org:resource\"].unique().to_numpy(dtype=str),\n","    \"contact_type\": df_servicenow[\"contact_type\"].unique().to_numpy(dtype=str),\n","}\n","\n","DYNAMIC_NUMERICAL_ATTRS = {\n","    \"sys_mod_count\": None,\n","    \"u_priority_confirmation\": None,\n","    \"knowledge\": None,\n","    \"priority\": None,\n","    \"reopen_count\": None,\n","    \"reassignment_count\": None,\n","}\n","\n","DYNAMIC_RAW_ATTRS = {\n","    \"org_resource_graph\": -99,\n","    \"org_group_graph\": -99\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pea6ILC_gNtA"},"outputs":[],"source":["ds_servicenow_train = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"incident_event_log_train_dataset\"),\n","    compression='GZIP'\n",")\n","\n","ds_servicenow_train = ds_window_dynamic_attrs(ds_servicenow_train, MAX_SEQ_LEN)\n","ds_servicenow_train_transformer = ds_desequentialize_dynamic_attrs(ds_servicenow_train, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","\n","print(len(ds_servicenow_train))\n","\n","ds_servicenow_train = ds_cache_and_batch(ds_servicenow_train, shuffle=False)\n","ds_servicenow_transformer_train = ds_cache_and_batch(ds_servicenow_train_transformer, shuffle=False)\n","\n","ds_servicenow_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDif34tZHiRL"},"outputs":[],"source":["ds_servicenow_test = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"incident_event_log_test_dataset\"),\n","    compression='GZIP'\n",")\n","\n","ds_servicenow_test = ds_window_dynamic_attrs(ds_servicenow_test, MAX_SEQ_LEN)\n","ds_servicenow_test_transformer = ds_desequentialize_dynamic_attrs(ds_servicenow_test, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","\n","print(len(ds_servicenow_test))\n","\n","ds_servicenow_test = ds_cache_and_batch(ds_servicenow_test, shuffle=False)\n","ds_servicenow_transformer_test = ds_cache_and_batch(ds_servicenow_test_transformer, shuffle=False)\n","\n","ds_servicenow_test"]},{"cell_type":"markdown","metadata":{"id":"k8yP1JMj4xdj"},"source":["## ProcessLSTM"]},{"cell_type":"markdown","metadata":{"id":"mIuY84pgxAOS"},"source":["### Multitask LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l58oGJCg4dia"},"outputs":[],"source":["model_name = f\"lstm_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y17GwipOw-lk"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessRNN(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={\"next_activity\": {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}, \"next_time\": {'units': 1, 'activation': 'linear'}, \"remaining_time\": {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1rPYWSm4YRB"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_servicenow_train,\n","  validation_data=ds_servicenow_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrRZAP4B-lXf"},"outputs":[],"source":["model_save_files(model, ds_servicenow_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RabCaqhMP93o"},"outputs":[],"source":["model_regression_report(model, ds_servicenow_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_servicenow_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_servicenow_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYdrWKJiFjEa"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"5sRm9HnswhWB"},"source":["## ProcessTransformer"]},{"cell_type":"markdown","metadata":{"id":"DTYA4Wp_C4oh"},"source":["### Remaining Time Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MutUzkuqC4oj"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKIYxlKBC4ok"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_REMAINING_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n","    activity_encoding_type='embedding'\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8r_Ko5hMC4ok"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_servicenow_transformer_train,\n","  validation_data=ds_servicenow_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUA9jlpxC4ol"},"outputs":[],"source":["model_save_files(model, ds_servicenow_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKRXCb9WC4om"},"outputs":[],"source":["model_regression_report(model, ds_servicenow_transformer_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2pRFMQgMbms"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"vk_-EyctC4xQ"},"source":["### Next Time Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcK4wkkxC4xR"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_U2yDebC4xR"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UurMguhC4xS"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_servicenow_transformer_train,\n","  validation_data=ds_servicenow_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqZ08V8rC4xS"},"outputs":[],"source":["model_save_files(model, ds_servicenow_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4CrjcSRC4xT"},"outputs":[],"source":["model_regression_report(model, ds_servicenow_transformer_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ARfaevqFqaP"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"DBdI5QFBC43J"},"source":["### Next Activity Prediction Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxBnlB4AC43K"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxsUPmPMC43K"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_ACTIVITY_OUTPUT: {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VBZ8NkRC43K"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_servicenow_transformer_train,\n","  validation_data=ds_servicenow_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"asmklOZUC43L"},"outputs":[],"source":["model_save_files(model, ds_servicenow_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yX_z5mnSC43L"},"outputs":[],"source":["model_classification_report(model, ds_servicenow_transformer_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lMxyb0tFsQL"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"KYnIDYG6Anvq"},"source":["## AST"]},{"cell_type":"markdown","metadata":{"id":"cPyLV-pcpUnA"},"source":["### Remaining Time Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iArQNoK_C84I"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPn8n91JfSdO"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=256,\n","  encoder_var_units=64,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=64,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=64,\n","  static_var_net_dim=64,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.1,\n","  static_var_net_dropout=0.1,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.1,\n","  time_depth=1,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.1,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.1,\n","  ff_dropout=0.15,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_REMAINING_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_REMAINING_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XD_MqbSKfgtB"},"outputs":[],"source":["model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_servicenow_train,\n","  validation_data=ds_servicenow_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Dne5qh33wqn"},"outputs":[],"source":["model_visualize_history(model)\n","model_save_files(model, ds_servicenow_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzzFDzC8Fzk_"},"outputs":[],"source":["model_regression_report(model, ds_servicenow_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbXR4z50CXPo"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"dHe3PAWpPe0d"},"source":["### Next Timestamp Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YI4vk2MfCTE"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wi8Uw35e-85"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = True\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=256,\n","  encoder_var_units=64,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=64,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=64,\n","  static_var_net_dim=64,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.1,\n","  static_var_net_dropout=0.1,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.1,\n","  time_depth=1,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.1,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.1,\n","  ff_dropout=0.15,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uyl5UYrN0vrg"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_servicenow_train,\n","  validation_data=ds_servicenow_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVqFB6xY5cYY"},"outputs":[],"source":["model_visualize_history(model, history)\n","model_save_files(model, ds_servicenow_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMOn50XbF4mL"},"outputs":[],"source":["model_regression_report(model, ds_servicenow_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HG2P5KstrYKa"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"fsJOCwL4Pk_B"},"source":["### Next Activity Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u90Ke_N0CL5B"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tW0RnK-nzSwT"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = True\n","\n","model = MulticlassHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=256,\n","  encoder_var_units=64,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=64,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=64,\n","  static_var_net_dim=64,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.1,\n","  static_var_net_dropout=0.1,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.1,\n","  time_depth=1,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.1,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.1,\n","  ff_dropout=0.15,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_ACTIVITY_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_ACTIVITY_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJl7XIMO0x-2"},"outputs":[],"source":["model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_servicenow_train,\n","  validation_data=ds_servicenow_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2tHRo7C5dVv"},"outputs":[],"source":["model_visualize_history(model)\n","model_save_files(model, ds_servicenow_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctVAO2iOF-_2"},"outputs":[],"source":["model_classification_report(model, ds_servicenow_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYxK2riJ-K-8"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"ZAUtPWP8AirR"},"source":["## TGN-AST"]},{"cell_type":"markdown","metadata":{"id":"PGx7KbE3USlq"},"source":["### Multitask Prediction TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zplrq5BHUSls"},"outputs":[],"source":["model_name = f\"tgnast_{PROJECT_NAME}_graph_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZeaN35dUSlt"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = False\n","\n","model = MultitaskHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  next_activity_out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=256,\n","  encoder_var_units=64,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=64,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=64,\n","  static_var_net_dim=64,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs={k: v for k, v in DYNAMIC_CATEGORICAL_ATTRS.items() if k not in ['org_group', 'org_resource']},\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  dynamic_raw_attrs=DYNAMIC_RAW_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.1,\n","  static_var_net_dropout=0.1,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.1,\n","  time_depth=1,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.1,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.1,\n","  ff_dropout=0.15,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d39pUfByUSlv"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_servicenow_train,\n","  validation_data=ds_servicenow_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70IPWU4kUSlw"},"outputs":[],"source":["model_save_files(model, ds_servicenow_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZjMKchuUSlx"},"outputs":[],"source":["model_regression_report(model, ds_servicenow_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_servicenow_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_servicenow_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIMhHMZxG-Ul"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"kzxxUVrg93w6"},"source":["# Dataset: Dataset belonging to the help desk log of an Italian Company"]},{"cell_type":"markdown","metadata":{"id":"W8vyF1IuJaQ6"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zelIEb7kJZsp"},"outputs":[],"source":["df_italy = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"finale_labeled.feather\"))\n","df_italy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXyuW6AMtn4J"},"outputs":[],"source":["df_italy_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"finale_train.feather\"))\n","df_italy_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Trzfr-YtnxA"},"outputs":[],"source":["df_italy_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"finale_test.feather\"))\n","df_italy_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlVOzDd9X1X_"},"outputs":[],"source":["PROJECT_NAME = \"italy\"\n","MAX_SEQ_LEN = 7\n","TRAIN_STEPS_EPOCH = len(df_italy_train) // DEFAULT_BATCH_SIZE\n","\n","TIME_ATTRS = {\n","    'month': \"time_timestamp_month\",\n","    'hour': \"time_timestamp_hour\",\n","    'day': \"time_timestamp_day\",\n","    'weekday': \"time_timestamp_weekday\",\n","    'yearday': \"time_timestamp_dayofyear\",\n","}\n","ACTIVITY_VOCAB = df_italy[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]\n","\n","INTERVAL_SINCE_LAST_EVENT_VOCAB = df_italy_train[\"time:timestamp:elapsedprev:seconds\"].to_numpy()\n","DAY_OF_WEEK_VOCAB = df_italy_train[\"time:timestamp:weekday:raw\"].to_numpy()\n","HOUR_OF_DAY_VOCAB = df_italy_train[\"time:timestamp:hour:raw\"].to_numpy()\n","\n","STATIC_CATEGORICAL_ATTRS = {\n","    \"case_responsible_section\": df_italy[\"case:responsible_section\"].unique().to_numpy(dtype=str),\n","    \"case_support_section\": df_italy[\"case:support_section\"].unique().to_numpy(dtype=str),\n","}\n","STATIC_NUMERICAL_ATTRS = {}\n","\n","DYNAMIC_CATEGORICAL_ATTRS = {\n","    \"org_resource\": df_italy[\"org:resource\"].unique().to_numpy(dtype=str),\n","    \"org_group\": df_italy[\"org:group\"].unique().to_numpy(dtype=str),\n","    \"customer\": df_italy[\"customer\"].unique().to_numpy(dtype=str),\n","    \"product\": df_italy[\"product\"].unique().to_numpy(dtype=str),\n","    \"service_type\": df_italy[\"service_type\"].unique().to_numpy(dtype=str),\n","}\n","DYNAMIC_NUMERICAL_ATTRS = {\n","    \"seriousness_2\": None,\n","    \"service_level\": None,\n","}\n","\n","DYNAMIC_RAW_ATTRS = {\n","    \"org_resource_graph\": -99,\n","    \"org_group_graph\": -99\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BA0DitPPJVHi"},"outputs":[],"source":["ds_italy_train = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"finale_train_dataset\"),\n","    compression='GZIP'\n",")\n","ds_italy_train = ds_window_dynamic_attrs(ds_italy_train, MAX_SEQ_LEN)\n","ds_italy_transformer_train = ds_desequentialize_dynamic_attrs(ds_italy_train, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","print(len(ds_italy_train))\n","\n","ds_italy_train = ds_cache_and_batch(ds_italy_train, shuffle=False)\n","ds_italy_transformer_train = ds_cache_and_batch(ds_italy_transformer_train, shuffle=False)\n","ds_italy_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwP9QILZJxZi"},"outputs":[],"source":["ds_italy_test = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"finale_test_dataset\"),\n","    compression='GZIP'\n",")\n","ds_italy_test = ds_window_dynamic_attrs(ds_italy_test, MAX_SEQ_LEN)\n","ds_italy_transformer_test = ds_desequentialize_dynamic_attrs(ds_italy_test, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","print(len(ds_italy_test))\n","\n","ds_italy_test = ds_cache_and_batch(ds_italy_test, shuffle=False)\n","ds_italy_transformer_test = ds_cache_and_batch(ds_italy_transformer_test, shuffle=False)\n","ds_italy_test"]},{"cell_type":"markdown","metadata":{"id":"Z91MHvZ4eKuC"},"source":["## ProcessLSTM"]},{"cell_type":"markdown","metadata":{"id":"DDWwxZLGq7oX"},"source":["### Multitask LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjBrCsYbq7oY"},"outputs":[],"source":["model_name = f\"lstm_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTL7rFxKq7oZ"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessRNN(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_ACTIVITY_OUTPUT: {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}, DEFAULT_NEXT_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}, DEFAULT_REMAINING_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8IqFZx3q7oa"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_italy_train,\n","  validation_data=ds_italy_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TX6TObHbq7oa"},"outputs":[],"source":["model_save_files(model, ds_italy_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2Cm7vpcq7oa"},"outputs":[],"source":["model_regression_report(model, ds_italy_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_italy_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_italy_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2l1Id2txHDyc"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"VrBoYLWZxhBz"},"source":["## ProcessTransformer"]},{"cell_type":"markdown","metadata":{"id":"vpA90zdCwliO"},"source":["### Remaining Time Process Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVu0SpqSwliS"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nx2xYA5iwliT"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_REMAINING_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n","    activity_encoding_type='embedding'\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcyxny2CwliT"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_italy_transformer_train,\n","  validation_data=ds_italy_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3xSjr2JwliV"},"outputs":[],"source":["model_save_files(model, ds_italy_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEYTaHfjwliV"},"outputs":[],"source":["model_regression_report(model, ds_italy_transformer_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8em1VjywHI-3"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"mxVkAtDoBtxC"},"source":["### Next Timestamp Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8ovOFl6BtxF"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jXhE2BCBtxF"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n","    activity_encoding_type='embedding'\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTBwIBZNBtxG"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_italy_transformer_train,\n","  validation_data=ds_italy_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWlJIQMeBtxG"},"outputs":[],"source":["model_save_files(model, ds_italy_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFM982FFBtxG"},"outputs":[],"source":["model_regression_report(model, ds_italy_transformer_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8uoMkGgHK4z"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"jgbtZ6PUCJ3V"},"source":["### Next Activity Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7mZnKsZCJ3X"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eA3zCZVCJ3Y"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_ACTIVITY_OUTPUT: {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csMVdW50CJ3Z"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_italy_transformer_train,\n","  validation_data=ds_italy_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEPnD3yBCJ3Z"},"outputs":[],"source":["model_save_files(model, ds_italy_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHOnnt9nCJ3a"},"outputs":[],"source":["model_classification_report(model, ds_italy_transformer_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5RuwjsfHMdU"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"Hs1PXeBUuXFb"},"source":["## AST"]},{"cell_type":"markdown","metadata":{"id":"vZcVs21dTwDO"},"source":["### Remaining Time Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gD9R43GNGbNU"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25YvjSAlWOf6"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=256,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=128,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_REMAINING_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_REMAINING_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAdNFL1J8-DA"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_italy_train,\n","  validation_data=ds_italy_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGjJXZNqVTUh"},"outputs":[],"source":["model_visualize_history(model)\n","model_save_files(model, ds_italy_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kICDFp5Ef8Wo"},"outputs":[],"source":["model_regression_report(model, ds_italy_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3wb1AdmabCQ"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"OT1RfYAvTyaW"},"source":["### Next Timestamp Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KvRwNzoHFSN"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YTPFt1Ibedz"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","#var_net_activation = 'mish'\n","var_net_activation = 'elu'\n","#transformer_activation = 'mish'\n","transformer_activation = 'mish'\n","#grn_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n"," embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=256,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=128,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  #backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hdr5qdzGXaSL"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_italy_train,\n","  validation_data=ds_italy_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxFVY8krOXq9"},"outputs":[],"source":["model_save_files(model, ds_italy_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcONEzJFISRN"},"outputs":[],"source":["model_regression_report(model, ds_italy_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ze8kIxhPbpO9"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"mbB9iNEET2hS"},"source":["### Next Activity Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQPLvIxoJ2PG"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6dXflismOtYw"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","#var_net_activation = 'mish'\n","var_net_activation = 'elu'\n","#transformer_activation = 'mish'\n","transformer_activation = 'mish'\n","#grn_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = MulticlassHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=256,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=128,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_ACTIVITY_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_ACTIVITY_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbXihF6_PcX0"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_italy_train,\n","  validation_data=ds_italy_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9V386oE-fd_1"},"outputs":[],"source":["model_save_files(model, ds_italy_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDn0fS7GIgqS"},"outputs":[],"source":["model_classification_report(model, ds_italy_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0V2zmSzXcc8F"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"H0LMz-oAf8Wj"},"source":["## TGN-AST"]},{"cell_type":"markdown","metadata":{"id":"IQy3iH4guPq9"},"source":["### Multitask Prediction TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFttuTd9f8Wm"},"outputs":[],"source":["model_name = f\"tgnast_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4Dua1j0f8Wm"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = MultitaskHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  next_activity_out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=256,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=128,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs={k: v for k, v in DYNAMIC_CATEGORICAL_ATTRS.items() if k not in ['org_group', 'org_resource']},\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  dynamic_raw_attrs=DYNAMIC_RAW_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.1,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.1,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.1,\n","  decoder_dropout=0.1,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.1,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.1,\n","  static_var_net_dropout=0.2,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.1,\n","  time_depth=2,\n","  time_dropout=0.1,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.1,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.1,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXbgRfW4f8Wn"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_italy_train,\n","  validation_data=ds_italy_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwX844lUWcAO"},"outputs":[],"source":["model_save_files(model, ds_italy_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7n4zFSGXWXVv"},"outputs":[],"source":["model_regression_report(model, ds_italy_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_italy_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_italy_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MLrVLxDJDWB"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"KwaK3XFhJ9DB"},"source":["# Dataset: BPIC 2014"]},{"cell_type":"markdown","metadata":{"id":"5MW9XP2uTYaD"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akH7W0GITXXI"},"outputs":[],"source":["df_bpic14 = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_labeled.feather\"))\n","df_bpic14"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ev5sSrdWBO-U"},"outputs":[],"source":["df_bpic14_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_train.feather\"))\n","df_bpic14_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uxsyLjkBOqX"},"outputs":[],"source":["df_bpic14_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_test.feather\"))\n","df_bpic14_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIHDeIMvsoBS"},"outputs":[],"source":["MAX_SEQ_LEN = 27\n","PROJECT_NAME = \"bpic14\"\n","TRAIN_STEPS_EPOCH = len(df_bpic14_train) // DEFAULT_BATCH_SIZE\n","\n","\n","TIME_ATTRS = {\n","    'month': \"time_timestamp_month\",\n","    'hour': \"time_timestamp_hour\",\n","    'day': \"time_timestamp_day\",\n","    'weekday': \"time_timestamp_weekday\",\n","    'yearday': \"time_timestamp_dayofyear\",\n","}\n","ACTIVITY_VOCAB = df_bpic14[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]\n","\n","INTERVAL_SINCE_LAST_EVENT_VOCAB = df_bpic14_train[\"time:timestamp:elapsedprev:seconds\"].to_numpy()\n","DAY_OF_WEEK_VOCAB = df_bpic14_train[\"time:timestamp:weekday:raw\"].to_numpy()\n","HOUR_OF_DAY_VOCAB = df_bpic14_train[\"time:timestamp:hour:raw\"].to_numpy()\n","\n","STATIC_CATEGORICAL_ATTRS = {\n","    \"case_km_number\": df_bpic14[\"case:KM number\"].unique().to_numpy(dtype=str),\n","    \"case_incident_category\": df_bpic14[\"case:incident_Category\"].unique().to_numpy(dtype=str),\n","    \"case_incident_ci_type_aff\": df_bpic14[\"case:incident_CI Type (aff)\"].unique().to_numpy(dtype=str),\n","    \"case_incident_ci_subtype_aff\": df_bpic14[\"case:incident_CI Subtype (aff)\"].unique().to_numpy(dtype=str),\n","    \"case_incident_service_component_aff\": df_bpic14[\"case:incident_Service Component WBS (aff)\"].unique().to_numpy(dtype=str),\n","    \"case_incident_ci_name_cby\": df_bpic14[\"case:incident_CI Name (CBy)\"].unique().to_numpy(dtype=str),\n","    \"case_incident_ci_type_cby\": df_bpic14[\"case:incident_CI Type (CBy)\"].unique().to_numpy(dtype=str),\n","}\n","STATIC_NUMERICAL_ATTRS = {\n","    \"case_interaction_priority\": df_bpic14[\"case:interaction_Priority\"].unique().to_numpy(dtype=str),\n","    \"case_incident_priority\": df_bpic14[\"case:incident_Priority\"].unique().to_numpy(dtype=str),\n","}\n","\n","DYNAMIC_CATEGORICAL_ATTRS = {\n","    \"org_group\": df_bpic14[\"org:group\"].unique().to_numpy(dtype=str),\n","}\n","\n","DYNAMIC_RAW_ATTRS = {\n","    \"org_group_graph\": -99\n","}\n","\n","DYNAMIC_NUMERICAL_ATTRS = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfBpCWlGTyun"},"outputs":[],"source":["ds_bpic14_train = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_train_dataset\"),\n","    compression='GZIP'\n",")\n","#ds_bpic14_train = ds_bpic14_train.unbatch()\n","ds_bpic14_train = ds_window_dynamic_attrs(ds_bpic14_train, MAX_SEQ_LEN)\n","ds_bpic14_transformer_train = ds_desequentialize_dynamic_attrs(ds_bpic14_train, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","print(len(ds_bpic14_train))\n","\n","ds_bpic14_train = ds_cache_and_batch(ds_bpic14_train, shuffle=False)\n","ds_bpic14_transformer_train = ds_cache_and_batch(ds_bpic14_transformer_train, shuffle=False)\n","ds_bpic14_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wm6cj3lYTyno"},"outputs":[],"source":["ds_bpic14_test = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_test_dataset\"),\n","    compression='GZIP'\n",")\n","\n","ds_bpic14_test = ds_window_dynamic_attrs(ds_bpic14_test, MAX_SEQ_LEN)\n","ds_bpic14_transformer_test = ds_desequentialize_dynamic_attrs(ds_bpic14_test, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","print(len(ds_bpic14_test))\n","\n","ds_bpic14_test = ds_cache_and_batch(ds_bpic14_test, shuffle=False)\n","ds_bpic14_transformer_test = ds_cache_and_batch(ds_bpic14_transformer_test, shuffle=False)\n","ds_bpic14_test"]},{"cell_type":"markdown","metadata":{"id":"N0TaeJUjC8Ji"},"source":["## ProcessLSTM"]},{"cell_type":"markdown","metadata":{"id":"9M-Z5TnHC8Jl"},"source":["### Multitask LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6tT0qA3C8Jo"},"outputs":[],"source":["model_name = f\"lstm_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGKvcez8C8Jp"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessRNN(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_ACTIVITY_OUTPUT: {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}, DEFAULT_NEXT_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}, DEFAULT_REMAINING_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGZ_w74dC8Jr"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic14_train,\n","  validation_data=ds_bpic14_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkNujamqC8Jt"},"outputs":[],"source":["model_save_files(model, ds_bpic14_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-7gTe4SC8Ju"},"outputs":[],"source":["model_regression_report(model, ds_bpic14_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_bpic14_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_bpic14_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QC6eMrxYJ08m"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"Rg04o4FkKrWr"},"source":["## ProcessTransformer"]},{"cell_type":"markdown","metadata":{"id":"O1h-jYrYKrWy"},"source":["### Remaining Time Prediction Process Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w06OwNtDKrWy"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FM_1yllrKrWy"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_REMAINING_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n","    activity_encoding_type='embedding'\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkocW1NrKrWy"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic14_transformer_train,\n","  validation_data=ds_bpic14_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-Bn3SBWKrWy"},"outputs":[],"source":["model_save_files(model, ds_bpic14_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibac4INSKrWz"},"outputs":[],"source":["model_regression_report(model, ds_bpic14_transformer_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otNEfuH1KAq9"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"m_HXT00ZEJ3P"},"source":["### Next Time Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3cq9peUEJ3R"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUf-HOEsEJ3S"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n","    activity_encoding_type='embedding'\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S17woYBQEJ3T"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic14_transformer_train,\n","  validation_data=ds_bpic14_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7FswPDgEJ3T"},"outputs":[],"source":["model_save_files(model, ds_bpic14_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXeSm_k_EJ3T"},"outputs":[],"source":["model_regression_report(model, ds_bpic14_transformer_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4Og5U7oJ-8v"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"Q8rgFioPEo1N"},"source":["### Next Activity Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73wJtLehEo1Q"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFqqgGjHEo1Q"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_ACTIVITY_OUTPUT: {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}},\n","    name=model_name,\n","    activity_encoding_type='embedding'\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9UYp06KEo1R"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic14_transformer_train,\n","  validation_data=ds_bpic14_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oR5t63yEo1S"},"outputs":[],"source":["model_save_files(model, ds_bpic14_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhoQymr7Eo1T"},"outputs":[],"source":["model_classification_report(model, ds_bpic14_transformer_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ds21sbkOJ771"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"jbp7XhyF_4ze"},"source":["## AST"]},{"cell_type":"markdown","metadata":{"id":"HnVVcxixDkdl"},"source":["### Remaining Time Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYWngzp8GtHz"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P27XmciADois"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=128,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=512,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=512,\n","  time_num_heads=2,\n","  time_ff_dim=512,\n","  time_var_units=128,\n","  ff_dims=[\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=2,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_REMAINING_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_REMAINING_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOTmmtwRJzdb"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic14_train,\n","  validation_data=ds_bpic14_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9MbtYFaAJ9M1"},"outputs":[],"source":["model_save_files(model, ds_bpic14_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaDRbQ-uMWj8"},"outputs":[],"source":["model_regression_report(model, ds_bpic14_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaDfG6_IKBhf"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"1RIqobNbDo5_"},"source":["### Next Timestamp Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yb4yPvDoHLlb"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgN9LRHzHIR7"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=128,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=512,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=512,\n","  time_num_heads=2,\n","  time_ff_dim=512,\n","  time_var_units=128,\n","  ff_dims=[\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  #backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_z2Axm5_3UGV"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic14_train,\n","  validation_data=ds_bpic14_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKfAu4BJ3XJA"},"outputs":[],"source":["model_visualize_history(model, history)\n","model_save_files(model, ds_bpic14_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceq5aWivMX0m"},"outputs":[],"source":["model_regression_report(model, ds_bpic14_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsN11T3r3eKG"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"vo_dDyQ6DrwG"},"source":["### Next Activity Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGqQf3UI27qe"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yu5YyHdM3FjO"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = False\n","\n","model = MulticlassHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=128,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=512,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=512,\n","  time_num_heads=2,\n","  time_ff_dim=512,\n","  time_var_units=128,\n","  ff_dims=[\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  out_name=DEFAULT_NEXT_ACTIVITY_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_ACTIVITY_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8_ujDEj9fDS"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic14_train,\n","  validation_data=ds_bpic14_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCr4GH_JJHXP"},"outputs":[],"source":["model_save_files(model, ds_bpic14_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tmg9vbtnJHXR"},"outputs":[],"source":["model_classification_report(model, ds_bpic14_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXGjj--09kQO"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"w69f3mZlAAvI"},"source":["## TGN-AST"]},{"cell_type":"markdown","metadata":{"id":"57seerB4MtLV"},"source":["### Multitask Prediction TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ir7NUtugMtLW"},"outputs":[],"source":["model_name = f\"tgnast_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKkuA7M6MtLX"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = MultitaskHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  next_activity_out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=256,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=128,\n","  ff_dims=[\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs={k: v for k, v in DYNAMIC_CATEGORICAL_ATTRS.items() if k not in ['org_group', 'org_resource']},\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  dynamic_raw_attrs=DYNAMIC_RAW_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  #backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgYBC9eNMtLX"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic14_train,\n","  validation_data=ds_bpic14_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z65hSFxhNPZf"},"outputs":[],"source":["model_save_files(model, ds_bpic14_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUrVJAk5NPZg"},"outputs":[],"source":["model_regression_report(model, ds_bpic14_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_bpic14_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_bpic14_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhdpAm1pMtLY"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"tYcrP6TFjucY"},"source":["# Dataset: Helpdesk"]},{"cell_type":"markdown","metadata":{"id":"0bsU8Djkmqce"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MK4iviICmp24"},"outputs":[],"source":["df_helpdesk = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"helpdesk_labeled.feather\"))\n","df_helpdesk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"So3lFH_xeh0i"},"outputs":[],"source":["df_helpdesk_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"helpdesk_train.feather\"))\n","df_helpdesk_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liSOkO4KedNI"},"outputs":[],"source":["df_helpdesk_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"helpdesk_test.feather\"))\n","df_helpdesk_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJJ_jXy7wll0"},"outputs":[],"source":["MAX_SEQ_LEN = 6\n","PROJECT_NAME = \"helpdesk\"\n","TRAIN_STEPS_EPOCH = len(df_helpdesk_train) // DEFAULT_BATCH_SIZE\n","\n","TIME_ATTRS = {\n","    'month': \"time_timestamp_month\",\n","    'hour': \"time_timestamp_hour\",\n","    'day': \"time_timestamp_day\",\n","    'weekday': \"time_timestamp_weekday\",\n","    'yearday': \"time_timestamp_dayofyear\",\n","}\n","\n","INTERVAL_SINCE_LAST_EVENT_VOCAB = df_helpdesk_train[\"time:timestamp:elapsedprev:seconds\"].to_numpy()\n","DAY_OF_WEEK_VOCAB = df_helpdesk_train[\"time:timestamp:weekday:raw\"].to_numpy()\n","HOUR_OF_DAY_VOCAB = df_helpdesk_train[\"time:timestamp:hour:raw\"].to_numpy()\n","\n","ACTIVITY_VOCAB = df_helpdesk[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]\n","\n","STATIC_CATEGORICAL_ATTRS = {}\n","STATIC_NUMERICAL_ATTRS = {}\n","\n","DYNAMIC_CATEGORICAL_ATTRS = {}\n","DYNAMIC_NUMERICAL_ATTRS = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdiz1kMKmpJN"},"outputs":[],"source":["ds_helpdesk_train = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"helpdesk_train_dataset\"),\n","    compression='GZIP'\n",")\n","ds_helpdesk_train = ds_window_dynamic_attrs(ds_helpdesk_train, MAX_SEQ_LEN)\n","ds_helpdesk_transformer_train = ds_desequentialize_dynamic_attrs(ds_helpdesk_train, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","print(len(ds_helpdesk_train))\n","\n","ds_helpdesk_train = ds_cache_and_batch(ds_helpdesk_train, shuffle=False)\n","ds_helpdesk_transformer_train = ds_cache_and_batch(ds_helpdesk_transformer_train, shuffle=False)\n","ds_helpdesk_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsvzSEV7m3-S"},"outputs":[],"source":["ds_helpdesk_test = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"helpdesk_test_dataset\"),\n","    compression='GZIP'\n",")\n","ds_helpdesk_test = ds_window_dynamic_attrs(ds_helpdesk_test, MAX_SEQ_LEN)\n","ds_helpdesk_transformer_test = ds_desequentialize_dynamic_attrs(ds_helpdesk_test, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","print(len(ds_helpdesk_test))\n","\n","ds_helpdesk_test = ds_cache_and_batch(ds_helpdesk_test, shuffle=False)\n","ds_helpdesk_transformer_test = ds_cache_and_batch(ds_helpdesk_transformer_test, shuffle=False)\n","\n","ds_helpdesk_test"]},{"cell_type":"markdown","metadata":{"id":"5Z3hbM9IeTIR"},"source":["## ProcessLSTM"]},{"cell_type":"markdown","metadata":{"id":"foaVJXKfeTIS"},"source":["### Multitask LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5TdD8ZueTIT"},"outputs":[],"source":["model_name = f\"lstm_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpgiK9M-eTIU"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessRNN(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={\"next_activity\": {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}, \"next_time\": {'units': 1, 'activation': 'linear'}, \"remaining_time\": {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsGJZnB4eTIV"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_helpdesk_train,\n","  validation_data=ds_helpdesk_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crA-YdqkeTIV"},"outputs":[],"source":["model_save_files(model, ds_helpdesk_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBytXa0YeTIW"},"outputs":[],"source":["model_regression_report(model, ds_helpdesk_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_helpdesk_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_helpdesk_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SlVhLS0JU8r"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"n51hJXraORea"},"source":["## ProcessTransformer"]},{"cell_type":"markdown","metadata":{"id":"PlZIchNxRhFs"},"source":["### Remaining Time Prediction Process Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vHlMm7oRhFs"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMAoNIToRhFs"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_REMAINING_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3hA0LjVRhFt"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_helpdesk_transformer_train,\n","  validation_data=ds_helpdesk_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6UICdzyRhFt"},"outputs":[],"source":["model_save_files(model, ds_helpdesk_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UpG16msRhFt"},"outputs":[],"source":["model_regression_report(model, ds_helpdesk_transformer_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Npo-yvwlRhFu"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"J19Ns9OmRhFu"},"source":["### Next Time Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQ0piMLARhFu"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hBciP7nRhFu"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlJ5BkctRhFv"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_helpdesk_transformer_train,\n","  validation_data=ds_helpdesk_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pe7Nzfk6RhFv"},"outputs":[],"source":["model_save_files(model, ds_helpdesk_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVGhaXUfRhFv"},"outputs":[],"source":["model_regression_report(model, ds_helpdesk_transformer_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRlTmmJpRhFw"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"ttMWeo5LRhFw"},"source":["### Next Activity Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4k-K4CsmRhFw"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTw2rw97RhFw"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_ACTIVITY_OUTPUT: {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bs00i1JBRhFw"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_helpdesk_transformer_train,\n","  validation_data=ds_helpdesk_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4umoKfjRhFx"},"outputs":[],"source":["model_save_files(model, ds_helpdesk_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kICnFVZhRhFx"},"outputs":[],"source":["model_classification_report(model, ds_helpdesk_transformer_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR7FbeoTRhFx"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"p0igzBwaBR9T"},"source":["## AST"]},{"cell_type":"markdown","metadata":{"id":"9YifudDZdTL_"},"source":["### Remaining Time Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHuWFOqRGYgb"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVlKCy8IzR0u"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = False\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=256,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=128,\n","  encoder_var_units=64,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=128,\n","  time_num_heads=2,\n","  time_ff_dim=64,\n","  time_var_units=32,\n","  ff_dims=[\n","    64,\n","  ],\n","  dynamic_var_net_dim=32,\n","  static_var_net_dim=16,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.25,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.25,\n","  encoder_grn_depth=2,\n","  encoder_grn_dropout=0.25,\n","  decoder_dropout=0.25,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.25,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=2,\n","  dynamic_var_grn_dropout=0.3,\n","  static_var_net_dropout=0.3,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=2,\n","  static_var_grn_dropout=0.3,\n","  time_depth=2,\n","  time_dropout=0.25,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.3,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=2,\n","  time_grn_dropout=0.3,\n","  ff_dropout=0.45,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_REMAINING_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  #learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaMeytITKO6e"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_helpdesk_train,\n","  validation_data=ds_helpdesk_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyMjHtPtO4eW"},"outputs":[],"source":["model_save_files(model, ds_helpdesk_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YB7XhUarO4eY"},"outputs":[],"source":["model_regression_report(model, ds_helpdesk_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7DUZvKwO4eZ"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"JMKTeVOPxHy-"},"source":["### Next Timestamp Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LG6Y_gHcmFiy"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZsNYYACxHXg"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'gelu'\n","transformer_activation = 'mish'\n","normalize_first = True\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = False\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=256,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=128,\n","  encoder_var_units=64,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=128,\n","  time_num_heads=2,\n","  time_ff_dim=64,\n","  time_var_units=32,\n","  ff_dims=[\n","    64,\n","  ],\n","  dynamic_var_net_dim=32,\n","  static_var_net_dim=16,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.25,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.25,\n","  encoder_grn_depth=2,\n","  encoder_grn_dropout=0.25,\n","  decoder_dropout=0.25,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.25,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=2,\n","  dynamic_var_grn_dropout=0.3,\n","  static_var_net_dropout=0.3,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=2,\n","  static_var_grn_dropout=0.3,\n","  time_depth=2,\n","  time_dropout=0.25,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.3,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=2,\n","  time_grn_dropout=0.3,\n","  ff_dropout=0.45,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2pwdyBoMqGR"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_helpdesk_train,\n","  validation_data=ds_helpdesk_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pl0buk4YOyBc"},"outputs":[],"source":["model_save_files(model, ds_helpdesk_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDyQG-LoOyBe"},"outputs":[],"source":["model_regression_report(model, ds_helpdesk_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5h_S0giTphIF"},"outputs":[],"source":["y_true = np.load(os.path.join(OUTPUT_DATA_DIR, \"ast_helpdesk_next_time_next_time_regression_groundtruth.npy\"))\n","y_pred = np.load(os.path.join(OUTPUT_DATA_DIR, \"ast_helpdesk_next_time_next_time_regression_predictions.npy\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6yye1cepv3m"},"outputs":[],"source":["np.abs(y_true - y_pred).argmax()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2RTZ1GKqC5z"},"outputs":[],"source":["y_true[447]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDxilprQqEr0"},"outputs":[],"source":["y_pred[447] *= 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLf7y8nTqPAk"},"outputs":[],"source":["np.save(os.path.join(OUTPUT_DATA_DIR, \"ast_helpdesk_next_time_predictions.npy\"), y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UU7aFSnQqaqB"},"outputs":[],"source":["y_pred.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTf8ASYPpv-W"},"outputs":[],"source":["evaluate_regression(y_true, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CzLIggXMzV2"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"NuZMaZaSxKjP"},"source":["### Next Activity Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CMVJ4UYSJ64"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROUcAblYSJ67"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'gelu'\n","transformer_activation = 'mish'\n","normalize_first = True\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = False\n","\n","model = MulticlassHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=256,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=128,\n","  encoder_var_units=64,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=128,\n","  time_num_heads=2,\n","  time_ff_dim=64,\n","  time_var_units=32,\n","  ff_dims=[\n","    64,\n","  ],\n","  dynamic_var_net_dim=32,\n","  static_var_net_dim=16,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.25,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.25,\n","  encoder_grn_depth=2,\n","  encoder_grn_dropout=0.25,\n","  decoder_dropout=0.25,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.25,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=2,\n","  dynamic_var_grn_dropout=0.3,\n","  static_var_net_dropout=0.3,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=2,\n","  static_var_grn_dropout=0.3,\n","  time_depth=2,\n","  time_dropout=0.25,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.3,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=2,\n","  time_grn_dropout=0.3,\n","  ff_dropout=0.45,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_ACTIVITY_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_ACTIVITY_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKEftdAPSJ67"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_helpdesk_train,\n","  validation_data=ds_helpdesk_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0f3NSTP1O736"},"outputs":[],"source":["model_save_files(model, ds_helpdesk_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZI-93E9rO738"},"outputs":[],"source":["model_classification_report(model, ds_helpdesk_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8b-lvk9O739"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"yWUSkTZH-BpI"},"source":["## TGN-AST"]},{"cell_type":"markdown","metadata":{"id":"zydOxb2rJdGQ"},"source":["### Multitask Prediction AST / TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnPJLcLJJdGS"},"outputs":[],"source":["model_name = f\"tgnast_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6hZWllUJdGS"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = False\n","\n","model = MultitaskHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  next_activity_out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=256,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=128,\n","  encoder_var_units=64,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=128,\n","  time_num_heads=2,\n","  time_ff_dim=64,\n","  time_var_units=32,\n","  ff_dims=[\n","    64,\n","  ],\n","  dynamic_var_net_dim=32,\n","  static_var_net_dim=16,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.25,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.25,\n","  encoder_grn_depth=2,\n","  encoder_grn_dropout=0.25,\n","  decoder_dropout=0.25,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.25,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=2,\n","  dynamic_var_grn_dropout=0.3,\n","  static_var_net_dropout=0.3,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=2,\n","  static_var_grn_dropout=0.3,\n","  time_depth=2,\n","  time_dropout=0.25,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.3,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=2,\n","  time_grn_dropout=0.3,\n","  ff_dropout=0.45,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SSHBXCyJdGT"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_helpdesk_train,\n","  validation_data=ds_helpdesk_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpjRZuKVOkgY"},"outputs":[],"source":["model_save_files(model, ds_helpdesk_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePELPQoCOkga"},"outputs":[],"source":["model_regression_report(model, ds_helpdesk_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_helpdesk_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_helpdesk_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZAhcPltOkgb"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"jTLykQfCSJai"},"source":["# Dataset: BPIC 2013"]},{"cell_type":"markdown","metadata":{"id":"o5mXj7gcSe9j"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_5qnTRASIw4"},"outputs":[],"source":["df_bpic13 = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_labeled.feather\"))\n","df_bpic13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRNL1H78M5rB"},"outputs":[],"source":["df_bpic13_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_train.feather\"))\n","df_bpic13_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyQGjPISM5rD"},"outputs":[],"source":["df_bpic13_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_test.feather\"))\n","df_bpic13_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mvixWwNSXBP"},"outputs":[],"source":["MAX_SEQ_LEN = 23\n","PROJECT_NAME = \"bpic13\"\n","TRAIN_STEPS_EPOCH = len(df_bpic13_train) // DEFAULT_BATCH_SIZE\n","\n","\n","TIME_ATTRS = {\n","    'month': \"time_timestamp_month\",\n","    'hour': \"time_timestamp_hour\",\n","    'day': \"time_timestamp_day\",\n","    'weekday': \"time_timestamp_weekday\",\n","    'yearday': \"time_timestamp_dayofyear\",\n","}\n","ACTIVITY_VOCAB = df_bpic13[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]\n","\n","INTERVAL_SINCE_LAST_EVENT_VOCAB = df_bpic13_train[\"time:timestamp:elapsedprev\"].to_numpy()\n","DAY_OF_WEEK_VOCAB = df_bpic13_train[\"time:timestamp:weekday:raw\"].to_numpy()\n","HOUR_OF_DAY_VOCAB = df_bpic13_train[\"time:timestamp:hour:raw\"].to_numpy()\n","\n","STATIC_CATEGORICAL_ATTRS = {\n","    \"case_product\": df_bpic13[\"case:Product\"].unique().to_numpy(dtype=str),\n","    \"case_country\": df_bpic13[\"case:Country\"].unique().to_numpy(dtype=str),\n","}\n","STATIC_NUMERICAL_ATTRS = {\n","    \"case_latest_impact\": None,\n","}\n","\n","DYNAMIC_CATEGORICAL_ATTRS = {\n","  \"status\": df_bpic13[\"Status\"].unique().to_numpy(dtype=str),\n","  \"sub_status\": df_bpic13[\"Sub Status\"].unique().to_numpy(dtype=str),\n","  \"org_role\": df_bpic13[\"org:role\"].unique().to_numpy(dtype=str),\n","  \"org_line\": df_bpic13[\"Involved Org line 3\"].unique().to_numpy(dtype=str),\n","  \"org_group\": df_bpic13[\"org:group\"].unique().to_numpy(dtype=str),\n","  \"org_resource\": df_bpic13[\"org:resource\"].unique().to_numpy(dtype=str),\n","  \"owner_country\": df_bpic13[\"Owner Country\"].unique().to_numpy(dtype=str),\n","}\n","\n","DYNAMIC_RAW_ATTRS = {\n","  \"org_group_graph\": -99,\n","  \"org_resource_graph\": -99,\n","}\n","\n","DYNAMIC_NUMERICAL_ATTRS = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05It77FdSjEK"},"outputs":[],"source":["ds_bpic13_train = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_train_dataset\"),\n","    compression='GZIP'\n",")\n","ds_bpic13_train = ds_window_dynamic_attrs(ds_bpic13_train, MAX_SEQ_LEN)\n","ds_bpic13_transformer_train = ds_desequentialize_dynamic_attrs(ds_bpic13_train, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","print(len(ds_bpic13_train))\n","\n","ds_bpic13_train = ds_cache_and_batch(ds_bpic13_train, shuffle=False)\n","ds_bpic13_transformer_train = ds_cache_and_batch(ds_bpic13_transformer_train, shuffle=False)\n","ds_bpic13_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g971vQqOSr_4"},"outputs":[],"source":["ds_bpic13_test = Dataset.load(\n","    os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_test_dataset\"),\n","    compression='GZIP'\n",")\n","\n","ds_bpic13_test = ds_window_dynamic_attrs(ds_bpic13_test, MAX_SEQ_LEN)\n","ds_bpic13_transformer_test = ds_desequentialize_dynamic_attrs(ds_bpic13_test, [\"time_timestamp_elapsedprev\", \"time_timestamp_hour_raw\", \"time_timestamp_weekday_raw\"])\n","print(len(ds_bpic13_test))\n","\n","ds_bpic13_test = ds_cache_and_batch(ds_bpic13_test, shuffle=False)\n","ds_bpic13_transformer_test = ds_cache_and_batch(ds_bpic13_transformer_test, shuffle=False)\n","\n","ds_bpic13_test"]},{"cell_type":"markdown","metadata":{"id":"YnFaof0TMvrS"},"source":["## ProcessLSTM"]},{"cell_type":"markdown","metadata":{"id":"R4UvoD9bMvrU"},"source":["### Multitask LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iM2abOOSMvrV"},"outputs":[],"source":["model_name = f\"lstm_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8IsSqjgMvrV"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessRNN(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_ACTIVITY_OUTPUT: {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}, DEFAULT_NEXT_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}, DEFAULT_REMAINING_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n"," run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0HX6_wwpMvrW"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_train,\n","  validation_data=ds_bpic13_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvD5CeXpMvrW"},"outputs":[],"source":["model_save_files(model, ds_bpic13_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d96R2-9MvrX"},"outputs":[],"source":["model_regression_report(model, ds_bpic13_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_bpic13_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_bpic13_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65Sm3nMbBhOR"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"_bRxbyhc-pPl"},"source":["## ProcessTransformer"]},{"cell_type":"markdown","metadata":{"id":"u4HZQIHtFgSH"},"source":["### Remaining Time Prediction Process Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGxE7NG4FgSH"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fr37BZPQFgSI"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_REMAINING_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEGiSQuVFgSI"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_transformer_train,\n","  validation_data=ds_bpic13_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dqAoAOOFgSI"},"outputs":[],"source":["model_save_files(model, ds_bpic13_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifORrAseFgSJ"},"outputs":[],"source":["model_regression_report(model, ds_bpic13_transformer_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyYz2w3PFgSJ"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"jsNAaBF7FgSJ"},"source":["### Next Time Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tZvCOUVFgSJ"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MN2jAcI8FgSJ"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_TIME_OUTPUT: {'units': 1, 'activation': 'linear'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DilcnV2pFgSK"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_transformer_train,\n","  validation_data=ds_bpic13_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKVNB9IDFgSK"},"outputs":[],"source":["model_save_files(model, ds_bpic13_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xp_tb9aZFgSK"},"outputs":[],"source":["model_regression_report(model, ds_bpic13_transformer_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7x9NkRE5FgSO"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"FdaxWpo8FgSO"},"source":["### Next Activity Prediction ProcessTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZVv5P3jFgSO"},"outputs":[],"source":["model_name = f\"processtransformer_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sunxox5UFgSP"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","model = BaseProcessTransformer(\n","    max_case_len=MAX_SEQ_LEN,\n","    activity_vocab=ACTIVITY_VOCAB,\n","    interval_since_last_event_vocab=INTERVAL_SINCE_LAST_EVENT_VOCAB,\n","    hour_of_day_vocab=HOUR_OF_DAY_VOCAB,\n","    day_of_week_vocab=DAY_OF_WEEK_VOCAB,\n","    output_dict={DEFAULT_NEXT_ACTIVITY_OUTPUT: {'units': len(ACTIVITY_VOCAB), 'activation': 'softmax'}},\n","    name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n","  learning_rate_warmup=None,\n","  learning_rate_decay=None,\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n","  #run_eagerly=True,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91XnXlyGFgSP"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_transformer_train,\n","  validation_data=ds_bpic13_transformer_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQGtAy-KFgSP"},"outputs":[],"source":["model_save_files(model, ds_bpic13_transformer_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1-KkfQPFgSQ"},"outputs":[],"source":["model_classification_report(model, ds_bpic13_transformer_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qGy7QhWFgSQ"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"5uZzEqwsBjI7"},"source":["## AST"]},{"cell_type":"markdown","metadata":{"id":"4DLF9fTrGAyR"},"source":["### Multitask Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFHWzP9OGAyS"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVmggVOZGAyT"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = False\n","\n","model = MultitaskHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  next_activity_out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=4,\n","  encoder_ff_dim=32,\n","  encoder_var_units=32,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=32,\n","  time_num_heads=2,\n","  time_ff_dim=32,\n","  time_var_units=32,\n","  ff_dims=[\n","    64,\n","  ],\n","  dynamic_var_net_dim=32,\n","  static_var_net_dim=16,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.1,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.1,\n","  encoder_grn_depth=2,\n","  encoder_grn_dropout=0.1,\n","  decoder_dropout=0.1,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.1,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=2,\n","  dynamic_var_grn_dropout=0.1,\n","  static_var_net_dropout=0.2,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=2,\n","  static_var_grn_dropout=0.1,\n","  time_depth=2,\n","  time_dropout=0.1,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.1,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=2,\n","  time_grn_dropout=0.1,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  name=f\"{PROJECT_NAME}_multi\",\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNhHAX_0GAyU"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_train,\n","  validation_data=ds_bpic13_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXJ0uU_LGAyU"},"outputs":[],"source":["model_save_files(model, ds_bpic13_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRIqw5asGAyV"},"outputs":[],"source":["model_regression_report(model, ds_bpic13_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_bpic13_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_bpic13_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn6ArWsgGAyV"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"KzM81bxnGAyV"},"source":["### Next Timestamp Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgX7lfU_GAyW"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpEOtFXpGAyW"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=128,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=512,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=512,\n","  time_num_heads=2,\n","  time_ff_dim=512,\n","  time_var_units=128,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9obEXXZ7GAyW"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_train,\n","  validation_data=ds_bpic13_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQ0pPDISGAyX"},"outputs":[],"source":["model_save_files(model, ds_bpic13_train)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDtL4Yr0GAyX"},"outputs":[],"source":["model_regression_report(model, ds_bpic13_test, DEFAULT_NEXT_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eq7btPZQGAyY"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"9eXA8tNdGAyY"},"source":["### Next Activity Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUS5z2KdGAyY"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_NEXT_ACTIVITY_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rV17HQrwGAyY"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = False\n","\n","model = MulticlassHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=128,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=512,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=512,\n","  time_num_heads=2,\n","  time_ff_dim=512,\n","  time_var_units=128,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_NEXT_ACTIVITY_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_NEXT_ACTIVITY_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEUnkxtIGAyZ"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_train,\n","  validation_data=ds_bpic13_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56Ec1B9KGAyZ"},"outputs":[],"source":["model_save_files(model, ds_bpic13_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgK2BLR5GAyZ"},"outputs":[],"source":["model_classification_report(model, ds_bpic13_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99Ctvs4gGAyZ"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"2YbZWQ1oS8If"},"source":["### Remaining Time Prediction AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COKCosUzS7ss"},"outputs":[],"source":["model_name = f\"ast_{PROJECT_NAME}_{DEFAULT_REMAINING_TIME_OUTPUT}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiIc4AXFV3RX"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'mish'\n","transformer_activation = 'mish'\n","grn_activation = 'mish'\n","normalize_first = True\n","\n","model = RegressionHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  embed_dim=128,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=512,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=4,\n","  decoder_ff_dim=512,\n","  time_num_heads=2,\n","  time_ff_dim=512,\n","  time_var_units=128,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs=DYNAMIC_CATEGORICAL_ATTRS,\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.15,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.15,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.15,\n","  decoder_dropout=0.15,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.15,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.15,\n","  static_var_net_dropout=0.15,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.15,\n","  time_depth=2,\n","  time_dropout=0.15,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.15,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.15,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  out_name=DEFAULT_REMAINING_TIME_OUTPUT,\n","  name=model_name,\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss={DEFAULT_REMAINING_TIME_OUTPUT: loss},\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8KXyv0BV9Ph"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_train,\n","  validation_data=ds_bpic13_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8W03xoNoWIDT"},"outputs":[],"source":["model_visualize_history(model)\n","model_save_files(model, ds_bpic13_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zIQJtnSqPsJ"},"outputs":[],"source":["model.evaluate(ds_bpic13_test, return_dict=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEXCyNXAGfdc"},"outputs":[],"source":["model_regression_report(model, ds_bpic13_test, DEFAULT_REMAINING_TIME_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q14Rfka8WIDU"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"T0mnOcTPBlIA"},"source":["## TGN-AST"]},{"cell_type":"markdown","metadata":{"id":"Tg_jOfizH8Oc"},"source":["### Multitask Prediction TGN-AST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJdOkT-HH8Od"},"outputs":[],"source":["model_name = f\"tgnast_{PROJECT_NAME}_multi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8ldjKH3H8Od"},"outputs":[],"source":["keras.utils.clear_session(free_memory=True)\n","\n","var_net_activation = 'elu'\n","transformer_activation = 'mish'\n","grn_activation = 'elu'\n","normalize_first = True\n","\n","model = MultitaskHierarchicalProcessTransformer(\n","  max_sequence_len=MAX_SEQ_LEN,\n","  next_activity_out_dim=len(ACTIVITY_VOCAB),\n","  embed_dim=64,\n","  encoder_depth=2,\n","  encoder_num_heads=2,\n","  encoder_ff_dim=256,\n","  encoder_var_units=128,\n","  decoder_depth=2,\n","  decoder_num_heads=2,\n","  decoder_ff_dim=256,\n","  time_num_heads=2,\n","  time_ff_dim=256,\n","  time_var_units=128,\n","  ff_dims=[\n","    256,\n","    128,\n","    64,\n","  ],\n","  dynamic_var_net_dim=128,\n","  static_var_net_dim=128,\n","  activity_vocab=ACTIVITY_VOCAB,\n","  time_attrs=TIME_ATTRS,\n","  static_catergorical_attrs=STATIC_CATEGORICAL_ATTRS,\n","  static_numerical_attrs=STATIC_NUMERICAL_ATTRS,\n","  dynamic_categorical_attrs={k: v for k, v in DYNAMIC_CATEGORICAL_ATTRS.items() if k not in ['org_group', 'org_resource']},\n","  dynamic_numerical_attrs=DYNAMIC_NUMERICAL_ATTRS,\n","  dynamic_raw_attrs=DYNAMIC_RAW_ATTRS,\n","  position_encoder_mode='rotary',\n","  encoder_dropout=0.1,\n","  encoder_activation=transformer_activation,\n","  encoder_normalize_first=normalize_first,\n","  encoder_var_activation=var_net_activation,\n","  encoder_var_dropout=0.1,\n","  encoder_grn_depth=1,\n","  encoder_grn_dropout=0.1,\n","  decoder_dropout=0.1,\n","  decoder_activation=transformer_activation,\n","  decoder_normalize_first=normalize_first,\n","  dynamic_var_net_dropout=0.1,\n","  dynamic_var_net_activation=var_net_activation,\n","  dynamic_var_grn_depth=1,\n","  dynamic_var_grn_dropout=0.1,\n","  static_var_net_dropout=0.2,\n","  static_var_net_activation=var_net_activation,\n","  static_var_grn_depth=1,\n","  static_var_grn_dropout=0.1,\n","  time_depth=1,\n","  time_dropout=0.1,\n","  time_activation=transformer_activation,\n","  time_var_activation=var_net_activation,\n","  time_var_dropout=0.1,\n","  time_normalize_first=normalize_first,\n","  time_grn_depth=1,\n","  time_grn_dropout=0.1,\n","  ff_dropout=0.2,\n","  flattening='vsn',\n","  ff_activation='mish',\n","  allow_negative=False,\n","  name=f\"{PROJECT_NAME}_multi\",\n",")\n","\n","loss = model.default_loss\n","metrics = model.default_metrics\n","optimizer = model.default_optimizer\n","callbacks = model.default_callbacks(\n","  log_file=os.path.join(OUTPUT_LOG_DATA_DIR, f\"{model_name}.csv\"),\n","  checkpoint_file=os.path.join(MODEL_CHECKPOINT_DIR, f\"{model_name}_best.keras\"),\n","  backup_dir=os.path.join(MODEL_BACKUP_DIR, f\"{model_name}_backup\"),\n","  tensorboard_dir=os.path.join(OUTPUT_LOG_DATA_DIR, model_name),\n",")\n","model = model.build_graph()\n","\n","model.compile(\n","  optimizer=optimizer,\n","  loss=loss,\n","  metrics=metrics,\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyAhXSMUH8Oe"},"outputs":[],"source":["history = model.fit(\n","  epochs=DEFAULT_EPOCHS,\n","  x=ds_bpic13_train,\n","  validation_data=ds_bpic13_test,\n","  callbacks=callbacks,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ey1k1BtvH8Of"},"outputs":[],"source":["model_save_files(model, ds_bpic13_test)\n","model_visualize_history(model, history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Wp4JleIH8Of"},"outputs":[],"source":["model_regression_report(model, ds_bpic13_test, DEFAULT_REMAINING_TIME_OUTPUT)\n","model_regression_report(model, ds_bpic13_test, DEFAULT_NEXT_TIME_OUTPUT)\n","model_classification_report(model, ds_bpic13_test, DEFAULT_NEXT_ACTIVITY_OUTPUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tw6PGCRH8Og"},"outputs":[],"source":["logs = os.path.join(OUTPUT_LOG_DATA_DIR, model_name)\n","%tensorboard --logdir \"$logs\""]},{"cell_type":"markdown","metadata":{"id":"tEnsomiJlYF_"},"source":["# Data Export"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbnZaIXvlpIp"},"outputs":[],"source":["output_file = f\"results_{datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S%z')}.zip\"\n","\n","!zip -r \"$output_file\" \"$DATA_DIR\" \"$GRAPHIC_DIR\" \"$MODEL_DIR\""]},{"cell_type":"markdown","metadata":{"id":"jT5qK1yAleCQ"},"source":["## A: Export to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeUyaAkGmCts"},"outputs":[],"source":["drive.mount(\"/content/drive\")\n","\n","Path(GDRIVE_OUTPUT_DIR).mkdir(exist_ok=True)\n","\n","!cp \"$output_file\" \"$GDRIVE_OUTPUT_DIR\"\n","\n","drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"A6l3vuhklh7h"},"source":["## B: Download to Local Machine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBY3BvZTl3_0"},"outputs":[],"source":["files.download(output_file)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}