{"cells":[{"cell_type":"markdown","metadata":{"id":"4Rws3ICU-8ZJ"},"source":["# Model Development\n","\n","This Notebook contains all information pertaining to the development of the models in the paper \"Leveraging Temporal Graphs for Enhancing Transformer-based Predictive Process Monitoring\" and was developed by Marc C. Hennig (mhennig@hm.edu). Requires the preprocessed event logs in the first file."]},{"cell_type":"markdown","metadata":{"id":"TQbWuHdWhhbN"},"source":["# Environment\n","\n","Initialization of the environment runnable in Google Colab and contianing all dependency installations and global variables used in the paper."]},{"cell_type":"markdown","metadata":{"id":"LKQT5Dhuhni0"},"source":["## Dependency installation"]},{"cell_type":"markdown","metadata":{"id":"RwEddQ2GrlRy"},"source":["### A. PIP Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJ_4VNSyS0jj"},"outputs":[],"source":["import packaging\n","import torch\n","\n","torch_ver = packaging.version.Version(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpBCXdYrSHRN"},"outputs":[],"source":["!pip install dgl -f https://data.dgl.ai/wheels/torch-{torch_ver.major}.{torch_ver.minor}/{torch_ver.local}/repo.html\n","!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch_ver.major}.{torch_ver.minor}.{torch_ver.micro}+{torch_ver.local}.html\n","!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{torch_ver.major}.{torch_ver.minor}.{torch_ver.micro}+{torch_ver.local}.html\n","!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{torch_ver.major}.{torch_ver.minor}.{torch_ver.micro}+{torch_ver.local}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{torch_ver.major}.{torch_ver.minor}.{torch_ver.micro}+{torch_ver.local}.html\n","!pip install torch-geometric\n","!pip install ipdb pm4py gensim h5py\n","!pip freeze > requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"HBG__Y3whra8"},"source":["## Dependency Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3h6Sq8RoTiLC"},"outputs":[],"source":["# Python dependencies\n","import os\n","import sys\n","import re\n","import math\n","import datetime\n","import random\n","import copy\n","import json\n","import time\n","import shutil\n","import pickle\n","import warnings\n","import functools\n","from pathlib import Path\n","from typing import List, Tuple, Union, Optional, Literal, Callable, Dict\n","from collections import namedtuple\n","from enum import Enum\n","\n","# Debugging\n","import ipdb\n","from tqdm.auto import tqdm, trange\n","import importlib\n","\n","# Colab dependencies\n","from google.colab import files, drive\n","\n","# Basic dependencies\n","import numpy as np\n","import pandas as pd\n","\n","# Plotting dependencies\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","# Machine learning depenencies\n","import sklearn as sl\n","import sklearn.metrics\n","\n","import torch\n","import torchdata\n","import torch_geometric as pyg\n","\n","import dgl\n","import h5py\n","import hyperopt\n","\n","# Process Mining dependencies\n","import pm4py\n","\n","# NLP dependencies\n","import gensim\n","\n","# Graph dependencies\n","import networkx as nx"]},{"cell_type":"markdown","metadata":{"id":"jSaBMrqDhuKF"},"source":["## Variables & Global Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7Tg7Nrrh1zG"},"outputs":[],"source":["# Assign a random seed for reproduceability\n","RANDOM_STATE = 1337\n","\n","os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n","random.seed(RANDOM_STATE)\n","np.random.seed(RANDOM_STATE)\n","\n","# Keras backend\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","# Show all Pandas columns\n","pd.set_option(\"display.max_columns\", None)\n","\n","# Set Matplotlib and Seaborn color scheme\n","plt.rcParams[\"image.cmap\"] = \"Blues\"\n","sns.set_palette(\"Blues\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eh37EJKuirTG"},"outputs":[],"source":["# Google Drive folders\n","GDRIVE_INPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/TGN-AST/Eventlogs\"\n","GDRIVE_OUTPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/TGN-AST/Results\"\n","\n","# Local Colab folders\n","UTIL_DIR = os.path.join(\".\", \"Util\")\n","DATA_DIR = os.path.join(\".\", \"Data\")\n","INPUT_DATA_DIR = os.path.join(DATA_DIR, \"Input\")\n","INPUT_DATA_BPIC2013_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2013\")\n","INPUT_DATA_BPIC2014_DIR = os.path.join(INPUT_DATA_DIR, \"BPIC 2014\")\n","INTERIM_DATA_DIR = os.path.join(DATA_DIR, \"Interim\")\n","OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"Output\")\n","OUTPUT_LOG_DATA_DIR = os.path.join(OUTPUT_DATA_DIR, \"Logs\")\n","\n","GRAPHIC_DIR = os.path.join(\".\", \"Graphics\")\n","MODEL_DIR = os.path.join(\".\", \"Models\")\n","MODEL_CHECKPOINT_DIR = os.path.join(MODEL_DIR, \"Checkpoints\")\n","MODEL_BACKUP_DIR = os.path.join(MODEL_DIR, \"Backups\")\n","\n","Path(DATA_DIR).mkdir(exist_ok=True)\n","Path(INTERIM_DATA_DIR).mkdir(exist_ok=True)\n","Path(OUTPUT_DATA_DIR).mkdir(exist_ok=True)\n","Path(OUTPUT_LOG_DATA_DIR).mkdir(exist_ok=True)\n","Path(GRAPHIC_DIR).mkdir(exist_ok=True)\n","Path(MODEL_DIR).mkdir(exist_ok=True)\n","Path(MODEL_BACKUP_DIR).mkdir(exist_ok=True)\n","Path(MODEL_CHECKPOINT_DIR).mkdir(exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEgmgkls_kzl"},"outputs":[],"source":["EVENTLOG_CASE = \"case:concept:name\"\n","EVENTLOG_ACTIVITY = \"concept:name\"\n","EVENTLOG_TIMESTAMP = \"time:timestamp\"\n","EVENTLOG_GROUP = \"org:group\"\n","EVENTLOG_RESOURCE = \"org:resource\"\n","EVENTLOG_CASE_PREFIX = \"case:\"\n","EVENTLOG_LABEL_PREFIX = \"label:\"\n","\n","TOKEN_PAD = \"[PAD]\"\n","TOKEN_PAD_NUM = -1.0\n","TOKEN_OOV = \"[OOV]\"\n","TOKEN_NA = \"[NA]\"\n","TOKEN_EOC = \"[EOC]\"\n","\n","DEFAULT_LEARNING_RATE = 0.00003\n","DEFAULT_MIN_LEARNING_RATE = 1e-6\n","\n","DEFAULT_WARMUP_EPOCHS = 10\n","DEFAULT_EPOCHS = 20\n","DEFAULT_BATCH_SIZE = 64\n","\n","TARGET_NEXT_ACTIVITY = \"next_activity\"\n","TARGET_NEXT_TIME = \"next_time\"\n","TARGET_REMAINING_TIME = \"remaining_time\"\n","\n","DEFAULT_REMAINING_TIME_OUTPUT = \"remaining_time\"\n","DEFAULT_NEXT_ACTIVITY_OUTPUT = \"next_activity\"\n","DEFAULT_NEXT_TIME_OUTPUT = \"next_time\""]},{"cell_type":"markdown","metadata":{"id":"r3AIDPxZlS-v"},"source":["## Data Import"]},{"cell_type":"markdown","metadata":{"id":"XAPgp73mjlFP"},"source":["### A: Import from Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4F4xUcGjkc0"},"outputs":[],"source":["drive.mount(\"/content/drive\")\n","\n","!cp -r \"$GDRIVE_INPUT_DIR\" \"$INPUT_DATA_DIR\"\n","\n","drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"FzhkKQsVj11R"},"source":["### B: Upload from Local Machine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHaQpf2gjxzL"},"outputs":[],"source":["#uploaded = files.upload()\n","\n","#for filename in uploaded.keys():\n","#  target = os.path.join(INPUT_DATA_DIR, filename)\n","#  !mv \"$filename\" \"$target\"\n","\n","#del uploaded"]},{"cell_type":"markdown","metadata":{"id":"mjH_DYlyUYgo"},"source":["## Common Functions"]},{"cell_type":"markdown","metadata":{"id":"tpOclu7boQBd"},"source":["### Model Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5HRdWiWHKkT"},"outputs":[],"source":["def evaluate_regression(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","  if y_pred.ndim > 1:\n","      y_pred = np.argmax(y_pred, axis=1)\n","\n","  def logcosh_error(y_true, y_pred):\n","    error = np.subtract(y_pred, y_true)\n","    return np.mean(np.log((np.exp(error) + np.exp(-error))/2))\n","\n","  return {\n","    'mae': sl.metrics.mean_absolute_error(y_true, y_pred),\n","    'mse': sl.metrics.mean_squared_error(y_true, y_pred),\n","    'rmse': sl.metrics.root_mean_squared_error(y_true, y_pred),\n","    'mape': sl.metrics.mean_absolute_percentage_error(y_true, y_pred),\n","    'medae': sl.metrics.median_absolute_error(y_true, y_pred),\n","    'logcosh': logcosh_error(y_true, y_pred),\n","    'max_error': sl.metrics.max_error(y_true, y_pred),\n","  }\n","\n","def evaluate_classification(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n","  if y_true.ndim > 1:\n","      y_true = np.argmax(y_true, axis=1)\n","  if y_pred.ndim > 1:\n","      y_pred = np.argmax(y_pred, axis=1)\n","\n","  return {\n","    'accuracy': sl.metrics.accuracy_score(y_true, y_pred),\n","    'accuracy_balanced': sl.metrics.balanced_accuracy_score(y_true, y_pred),\n","    'accuracy_balanced_adjusted': sl.metrics.balanced_accuracy_score(y_true, y_pred, adjusted=True),\n","    'f1_micro': sl.metrics.f1_score(y_true, y_pred, average='micro'),\n","    'f1_macro': sl.metrics.f1_score(y_true, y_pred, average='macro'),\n","    'f1_weighted': sl.metrics.f1_score(y_true, y_pred, average='weighted'),\n","    'precision_micro': sl.metrics.precision_score(y_true, y_pred, average='micro', zero_division='warn'),\n","    'precision_macro': sl.metrics.precision_score(y_true, y_pred, average='macro'),\n","    'precision_weighted': sl.metrics.precision_score(y_true, y_pred, average='weighted'),\n","    'recall_micro': sl.metrics.recall_score(y_true, y_pred, average='micro'),\n","    'recall_macro': sl.metrics.recall_score(y_true, y_pred, average='macro'),\n","    'recall_weighted': sl.metrics.recall_score(y_true, y_pred, average='weighted'),\n","  }"]},{"cell_type":"markdown","metadata":{"id":"owK83Ppa9BMB"},"source":["### PROPHET Model\n","\n","The following sections contains the implementation of the PROPHET model, mostly adapted from: https://github.com/vinspdb/PROPHET"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0cHhffY5VAq"},"outputs":[],"source":["class TextDataset(dgl.data.DGLDataset):\n","  def __init__(self, X, y):\n","    self.X_act = X\n","    self.Y = y\n","\n","  def __len__(self):\n","    return len(self.X_act)\n","\n","  def __plotgraph__(self, data):\n","    g = pyg.utils.to_networkx(data, to_undirected=False)\n","    nx.draw(g, with_labels=True)\n","    plt.show()\n","    plt.clf()\n","\n","  def __getitem__(self, idx):\n","    data = self.X_act[idx]\n","    label = self.Y[idx]\n","    return data, label\n","\n","class TextDatasetOnDisk(dgl.data.DGLDataset):\n","  def __init__(self, file: h5py.File, cache_size: int = 1024, auto_prefetch: bool = False):\n","    self.file = file\n","    self._keys = list(self.file.keys())\n","    self._cache_size = cache_size\n","    self._auto_prefetch = auto_prefetch\n","\n","    self._max_idx = 0\n","\n","    # Initialize the cached getter with the specified cache size\n","    self._cached_get_item = functools.lru_cache(maxsize=self._cache_size)(self._get_item_from_disk)\n","\n","  def key_at(self, idx: int) -> str:\n","    return self._keys[idx]\n","\n","  def prefetch(self, indices):\n","    \"\"\"Prefetch a list of indices into the cache\"\"\"\n","    for idx in indices:\n","      self._cached_get_item(idx)\n","\n","  def clear_cache(self):\n","    \"\"\"Clear the cache\"\"\"\n","    self._cached_get_item.cache_clear()\n","\n","  def __len__(self):\n","    return len(self.file.keys())\n","\n","  def __plotgraph__(self, data):\n","    g = pyg.utils.to_networkx(data, to_undirected=False)\n","    nx.draw(g, with_labels=True)\n","    plt.show()\n","    plt.clf()\n","\n","  def __getitem__(self, idx: int):\n","    if self._auto_prefetch and idx % self._cache_size == 0:\n","      self.prefetch(range(idx, min(idx + self._cache_size, len(self))))\n","    return self._cached_get_item(idx)\n","\n","  def _get_item_from_disk(self, idx: int):\n","    key = self.key_at(idx)\n","    data = pickle.loads(self.file[key][()])\n","    return data['graph'], data['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZ-ZW2W99Ai4"},"outputs":[],"source":["class RGCN(torch.nn.Module):\n","  def __init__(self, in_feats, params, rel_names):\n","    super(RGCN, self).__init__()\n","    self.convs = torch.nn.ModuleList([dgl.nn.pytorch.HeteroGraphConv({rel: dgl.nn.pytorch.GATv2Conv(in_feats=in_feats[rel], out_feats=params['hidden_dim'], num_heads=params['n_heads'], feat_drop=params['dropout'], share_weights=True, residual=True) for rel in rel_names}, aggregate='sum') for i in range(params['n_layers'])])\n","    self.conv_out = dgl.nn.pytorch.HeteroGraphConv({rel: dgl.nn.pytorch.GATv2Conv(in_feats=params['hidden_dim'] * params['n_heads'], out_feats=params['hidden_dim'], feat_drop=params['dropout'], num_heads=1, share_weights=True, residual=True) for rel in rel_names}, aggregate='sum')\n","    self.conv_out2 = dgl.nn.pytorch.HeteroGraphConv({rel: dgl.nn.pytorch.GATv2Conv(in_feats=in_feats[rel], out_feats=params['hidden_dim'], feat_drop=params['dropout'], num_heads=1, share_weights=True, residual=True) for rel in rel_names}, aggregate='sum')\n","\n","  def forward(self, graph, inputs):\n","    for f in self.convs:\n","      h = f(graph, inputs)\n","      h = {k: torch.reshape(torch.nn.functional.relu(v), (v.shape[0], -1)) for k, v in h.items()}\n","\n","      if len(self.convs) == 0:\n","        h = self.conv_out2(graph, inputs)\n","        h = {k: torch.reshape(torch.nn.functional.relu(v), (v.shape[0], -1)) for k, v in h.items()}\n","      else:\n","        h = self.conv_out(graph, h)\n","        h = {k: torch.reshape(torch.nn.functional.relu(v), (v.shape[0], -1)) for k, v in h.items()}\n","\n","    return h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItV-9YOj9A8h"},"outputs":[],"source":["class HeteroClassifier(torch.nn.Module):\n","  def __init__(self, in_dim, params, n_classes, rel_names):\n","    super().__init__()\n","\n","    self.rgcn = RGCN(in_dim, params, rel_names)\n","    self.classify = torch.nn.Linear(params['hidden_dim'], n_classes)\n","\n","  def forward(self, graph, feat, edge_weight=None):\n","    h = self.rgcn(graph, feat)\n","\n","    with graph.local_scope():\n","      graph.ndata['h'] = h\n","      hg = 0\n","      for ntype in graph.ntypes:\n","        hg = hg + dgl.sum_nodes(graph, 'h', ntype=ntype)\n","      return self.classify(hg)\n","\n","class HeteroRegressor(torch.nn.Module):\n","  def __init__(self, in_dim, params, rel_names):\n","    super().__init__()\n","\n","    self.rgcn = RGCN(in_dim, params, rel_names)\n","    self.classify = torch.nn.Linear(params['hidden_dim'], 1)\n","\n","  def forward(self, graph, feat, edge_weight=None):\n","    h = self.rgcn(graph, feat)\n","\n","    with graph.local_scope():\n","      graph.ndata['h'] = h\n","      hg = 0\n","      for ntype in graph.ntypes:\n","        hg = hg + dgl.sum_nodes(graph, 'h', ntype=ntype)\n","      return self.classify(hg)"]},{"cell_type":"markdown","metadata":{"id":"AnZrzy7O0uUa"},"source":["### PROPHET Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1eSMSpfoInpU"},"outputs":[],"source":["class GenerateTrace:\n","    def __init__(self, eventlog):\n","        self._eventlog = eventlog\n","\n","    def generate_prefix_trace(self, log, view):\n","        act = log.groupby('case:concept:name', sort=False).agg({view: lambda x: list(x)})\n","        return act\n","\n","    def get_act(self):\n","        return self.__act\n","\n","    def get_sequence(self, sequence):\n","        i = 0\n","        list_seq = []\n","        list_label = []\n","        while i < len(sequence):\n","            list_temp = []\n","            j = 0\n","            while j < (len(sequence.iat[i, 0]) - 1):\n","                list_temp.append(sequence.iat[i, 0][0 + j])\n","                list_seq.append(list_temp.copy())\n","                list_label.append(sequence.iat[i, 0][j + 1])\n","                j = j + 1\n","            i = i + 1\n","        return list_seq, list_label\n","\n","    def get_sequence_num(self, sequence):\n","        i = 0\n","        list_seq = []\n","        while i < len(sequence):\n","            list_temp = []\n","            j = 0\n","            while j < (len(sequence.iat[i, 0]) - 1):\n","                list_temp.append(sequence.iat[i, 0][0 + j])\n","                list_seq.append(list_temp.copy())\n","                j = j + 1\n","            i = i + 1\n","        return list_seq\n","\n","    @staticmethod\n","    def dataset_summary(log):\n","        print(\"Activity Distribution\\n\", log['activity'].value_counts())\n","        n_caseid = log['case'].nunique()\n","        n_activity = log['activity'].nunique()\n","        print(\"Number of CaseID\", n_caseid)\n","        print(\"Number of Unique Activities\", n_activity)\n","        print(\"Number of Activities\", log['activity'].count())\n","        cont_trace = log['case'].value_counts(dropna=False)\n","        max_trace = max(cont_trace)\n","        mean = np.mean(cont_trace)\n","        print(\"Max lenght trace\", max_trace)\n","        print(\"Mean lenght trace\", np.mean(cont_trace))\n","        print(\"Min lenght trace\", min(cont_trace))\n","        return max_trace,  int(round(mean)), n_caseid, n_activity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTIbt0w003xq"},"outputs":[],"source":["def prophet_create_triangular_matrix(columns: List[str]) -> List[Tuple[str, str, str]]:\n","    # Convert set to sorted list for consistent indexing\n","    cols = list(columns)\n","    n = len(cols)\n","\n","    # Create empty matrix filled with zeros\n","    list_relation = []\n","    # Fill upper triangular part\n","    for i in range(n):\n","        for j in range(i, n):\n","            # Create pair of column names\n","            if cols[i] == cols[j]:\n","                pair = (cols[i], 'follow', cols[j])\n","            else:\n","                pair = (cols[i], 'has', cols[j])\n","            list_relation.append(pair)\n","    return list_relation\n","\n","def apply_w2v(list_act, enc_act, mean):\n","    #list_act = clear_list(list_act)\n","    x_act_ohe = []\n","    for l in list_act:\n","        list_emb_temp = []\n","        for t in l:\n","            embed_vector = enc_act.get(t)\n","            if embed_vector is not None:\n","                list_emb_temp.append(embed_vector)\n","            else:\n","                list_emb_temp.append(np.zeros(shape=(mean,)))\n","        x_act_ohe.append(list_emb_temp)\n","    x_act_ohe = np.array(x_act_ohe)\n","    x_act_ohe = x_act_ohe.reshape(x_act_ohe.shape[0], mean)\n","    return x_act_ohe\n","\n","\n","def gen_flow(id):\n","    id.insert(0, 'START')\n","    remove_dup = list(dict.fromkeys(id))\n","    remove_dup = [[a] for a in remove_dup]\n","    id = np.array(id)\n","    node_encoder = sl.preprocessing.LabelEncoder()\n","    enc = node_encoder.fit_transform(id)\n","\n","    return remove_dup, enc\n","\n","def clear_list(prefix_list):\n","    temp_traces = []\n","    for k in prefix_list:\n","        listToStr = ' '.join([replace_char(str(elem)) for elem in k])\n","        temp_traces.append(listToStr)\n","\n","    tokenized_words = []\n","    for s in temp_traces:\n","        tokenized_words.append(s.split(' '))\n","    return tokenized_words\n","\n","def unique_edge(list1, list2):\n","    unique_tuples = []\n","    seen_tuples = set()\n","\n","    for pair in zip(list1, list2):\n","        if pair not in seen_tuples:\n","            unique_tuples.append(pair)\n","            seen_tuples.add(pair)\n","\n","    return unique_tuples\n","\n","def gen_edge_weigts(list1, list2):\n","    combined_tuples = list(zip(list1, list2))\n","    tuple_counts = {}\n","    for pair in combined_tuples:\n","        if pair in tuple_counts:\n","            tuple_counts[pair] += 1\n","        else:\n","            tuple_counts[pair] = 1\n","    return list(tuple_counts.values())\n","\n","def replace_char(ele):\n","  return re.sub(r\" -+_.:()\", \"\", ele)\n","\n","def build_list_graphs(dict_view, dict_y, dict_enc, mean, c, event_attributes, case_attributes, relation, case_ids, start: int = 0, end = -1, append: bool = True):\n","    if len(case_ids) != len(dict_view['concept:name']):\n","      raise ValueError(f\"Length {len(case_ids)} must be equal to {len(dict_view['concept:name'])}\")\n","\n","    end = len(case_ids) if end == -1 else min(len(case_ids), end)\n","\n","    mode = 'a' if append else 'w'\n","    with h5py.File(os.path.join(OUTPUT_DATA_DIR, c), mode) as f:\n","        list_graphs = []\n","        for k in (pbar := tqdm(range(start, end))):\n","                key = f\"{case_ids[k]}-{len(dict_view[EVENTLOG_ACTIVITY][k]):04d}\"\n","                if key.endswith(\"-0\"):\n","                  ipdb.set_trace()\n","                pbar.set_description(key)\n","                if key in f:\n","                    continue\n","\n","                list_node = {}\n","                list_node_comp = {}\n","                list_node_feature = {}\n","                dgl_canonical_edge = {}\n","                weight_node_follow_node = {}\n","                for v in event_attributes:\n","                    list_node[v], list_node_comp[v] = gen_flow(dict_view[v][k])\n","                    list_node_feature[v] = apply_w2v(list_node[v], dict_enc[v], mean) #W2W\n","\n","                list_att_trace = []\n","                for v in case_attributes:\n","                    embed_vector = dict_enc[v].get(dict_view[v][k][0])\n","                    res = embed_vector if embed_vector is not None else np.zeros(mean)\n","                    list_att_trace.append(res)\n","\n","                if list_att_trace:\n","                    list_node_comp['trace_att'] = [0]\n","                    list_node_feature['trace_att'] = np.array([np.concatenate(list_att_trace)])\n","\n","                for rel in relation:\n","                    if rel[1] == 'follow':\n","                        edge_res = np.array([[list_node_comp[rel[0]][i], list_node_comp[rel[0]][i + 1]] for i in range(len(list_node_comp[rel[0]]) - 1)])\n","                    elif rel[1] == 'has_ta':\n","                        list_node_comp[rel[2]] = [0]*len(np.unique(list_node_comp[rel[0]]))\n","                        edge_res = list(map(lambda X: [X[0], X[1]], list(zip(np.unique(list_node_comp[rel[0]]), list_node_comp[rel[2]]))))\n","                    else:\n","                        edge_res = list(map(lambda X: [X[0], X[1]], list(zip(list_node_comp[rel[0]], list_node_comp[rel[2]]))))\n","\n","                    src = [item[0] for item in edge_res]\n","                    dst = [item[1] for item in edge_res]\n","\n","                    tuple_src_dst = unique_edge(src, dst)\n","                    dgl_canonical_edge[rel] = tuple_src_dst\n","                    weight_node_follow_node[rel] = gen_edge_weigts(src,dst)\n","\n","                hetero_graph = dgl.heterograph(dgl_canonical_edge)\n","\n","                for nn in list_node_feature:\n","                        hetero_graph.nodes[nn].data[nn] = torch.tensor(list_node_feature[nn], dtype=torch.float)\n","\n","                for rel in weight_node_follow_node:\n","                    hetero_graph.edata['h'] = {rel:torch.tensor(weight_node_follow_node[rel])}\n","                new_g = dgl.AddReverse(copy_edata=True)(hetero_graph)\n","                list_graphs.append(new_g)\n","                pickled_graph = pickle.dumps({'graph':new_g, 'label':dict_y[k]})\n","\n","\n","                f.create_dataset(key, data=np.void(pickled_graph))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBhXBk9h1VtG"},"outputs":[],"source":["def prophet_generate_dgl_graph(\n","    df_train: pd.DataFrame,\n","    df_train_dyn: pd.DataFrame,\n","    df_test: pd.DataFrame,\n","    df_test_dyn: pd.DataFrame,\n","    name: str,\n","    dynamic_attrs: List[str] = [],\n","    static_attrs: List[str] = [],\n","    word2vec_dim: int = 100,\n","    word2vec_epochs: int = 50,\n","    case_col: str = EVENTLOG_CASE,\n","    activity_col: str = EVENTLOG_ACTIVITY,\n","    resource_col: str = EVENTLOG_RESOURCE,\n","    label_col: str = \"label:time:timestamp:last\",\n","    elapsed_since_start_col: str = \"time:timestamp:elapsedcycle:seconds\",\n","    elapsed_since_last_col: str = \"time:timestamp:elapsedprev:seconds\",\n","    random_state: int = RANDOM_STATE,\n","  ):\n","  df_train = df_train.copy()\n","  df_test = df_test.copy()\n","  df_train_dyn = df_train_dyn.copy()\n","  df_test_dyn = df_test_dyn.copy()\n","  df_complete = pd.concat([df_train, df_test]).reset_index(drop=True)\n","\n","  dynamic_attrs_mat = prophet_create_triangular_matrix(dynamic_attrs)\n","\n","  static_attrs_mat = []\n","  if len(static_attrs) > 0:\n","    static_attrs_mat = [(x, 'has_ta', 'trace_att') for x in dynamic_attrs]\n","\n","  pm = GenerateTrace(\"TEST\")\n","\n","  dict_card = {}\n","  dict_view_train = {}\n","  dict_view_test = {}\n","  dict_view_train_y = {}\n","  #dict_view_test_y = {}\n","  dict_enc = {}\n","\n","  relation = dynamic_attrs_mat + static_attrs_mat\n","\n","  if resource_col in dynamic_attrs:\n","    n_bin = (df_complete[activity_col].nunique() + df_complete[resource_col].nunique()) // 2\n","  else:\n","    n_bin = df_complete[activity_col].nunique()\n","\n","  for attr in tqdm(static_attrs):\n","    print(attr)\n","    if pd.api.types.is_numeric_dtype(df_complete[attr]):\n","      discretizer = sl.preprocessing.KBinsDiscretizer(n_bins=n_bin, encode='ordinal', strategy='quantile', random_state=random_state)\n","      discretizer.fit(df_complete[attr].dropna().unique().reshape(-1, 1))\n","      df_train[attr] = df_train[attr].map(lambda x: np.squeeze(discretizer.transform([[x]])), na_action='ignore')\n","      df_test[attr] = df_test[attr].map(lambda x: np.squeeze(discretizer.transform([[x]])), na_action='ignore')\n","\n","    df_train[attr] = df_train[attr].astype('string').fillna(TOKEN_NA)\n","    df_test[attr] = df_test[attr].astype('string').fillna(TOKEN_NA)\n","    df_complete = pd.concat([df_train, df_test]).reset_index(drop=True)\n","\n","    dict_card[attr] = list(df_complete[attr].unique())\n","\n","    dict_view_train[attr] = []\n","    for l in df_train.groupby(case_col).agg({attr: lambda x: list(x)}).to_numpy().tolist():\n","      prefix = np.tril(np.repeat(l, len(l[0]), axis=0)).tolist()\n","      prefix = [list(filter(lambda x: not pd.isna(x) and x != \"\", l)) for l in prefix]\n","      dict_view_train[attr].extend(prefix)\n","\n","    dict_view_test[attr] = []\n","    for l in df_test.groupby(case_col).agg({attr: lambda x: list(x)}).to_numpy().tolist():\n","      prefix = np.tril(np.repeat(l, len(l[0]), axis=0)).tolist()\n","      prefix = [list(filter(lambda x: not pd.isna(x) and x != \"\", l)) for l in prefix]\n","      dict_view_test[attr].extend(prefix)\n","\n","    word2vec = gensim.models.Word2Vec(vector_size=word2vec_dim, min_count=1, sg=0, workers=1, seed=random_state)\n","    word2vec.build_vocab(dict_view_train[attr], min_count=1)\n","    word2vec.train(dict_view_train[attr], total_examples=word2vec.corpus_count, epochs=50)\n","\n","    dict_enc[attr] = {}\n","    for word in word2vec.wv.index_to_key:\n","      dict_enc[attr][word] = word2vec.wv.get_vector(word).tolist()\n","\n","  for attr in tqdm(dynamic_attrs):\n","    print(attr)\n","    if pd.api.types.is_numeric_dtype(df_complete[attr]):\n","      discretizer = sl.preprocessing.KBinsDiscretizer(n_bins=n_bin, encode='ordinal', strategy='quantile', random_state=random_state)\n","      discretizer.fit(df_complete[attr].dropna().unique().reshape(-1, 1))\n","\n","      df_train[attr] = df_train[attr].map(lambda x: np.squeeze(discretizer.transform([[x]])), na_action='ignore')\n","      df_test[attr] = df_test[attr].map(lambda x: np.squeeze(discretizer.transform([[x]])), na_action='ignore')\n","      df_complete = pd.concat([df_train, df_test]).reset_index(drop=True)\n","\n","      df_train_dyn[attr] = df_train_dyn[attr].map(lambda x: np.squeeze(discretizer.transform([[x]])), na_action='ignore')\n","      df_test_dyn[attr] = df_test_dyn[attr].map(lambda x: np.squeeze(discretizer.transform([[x]])), na_action='ignore')\n","\n","    df_train[attr] = df_train[attr].astype('string').fillna(TOKEN_NA)\n","    df_test[attr] = df_test[attr].astype('string').fillna(TOKEN_NA)\n","\n","    dict_card[attr] = list(df_complete[attr].unique())\n","    dict_card[attr].insert(0, 'START')\n","\n","    dict_view_train[attr] = df_train_dyn[attr].to_numpy().tolist()\n","    dict_view_train[attr] = [list(filter(lambda x: x is not None and not pd.isna(x), l)) for l in dict_view_train[attr]]\n","    #dict_view_train_y[attr] = df_train[label_col].to_numpy().tolist()\n","\n","    dict_view_test[attr] = df_test_dyn[attr].to_numpy().tolist()\n","    dict_view_test[attr] = [list(filter(lambda x: x is not None and not pd.isna(x), l)) for l in dict_view_test[attr]]\n","    #dict_view_test_y[attr] = df_test[label_col].to_numpy().tolist()\n","\n","    word2vec = gensim.models.Word2Vec(vector_size=word2vec_dim, min_count=1, sg=0, workers=1, seed=random_state)\n","    word2vec.build_vocab(dict_view_train[attr], min_count=1)\n","    word2vec.train(dict_view_train[attr], total_examples=word2vec.corpus_count, epochs=50)\n","\n","    dict_enc[attr] = {}\n","    for word in word2vec.wv.index_to_key:\n","      dict_enc[attr][word] = word2vec.wv.get_vector(word).tolist()\n","\n","  y_train = df_train[label_col].to_numpy()\n","  y_test = df_test[label_col].to_numpy()\n","\n","  pickle.dump(dict_card, open(os.path.join(OUTPUT_DATA_DIR, f\"{name}_card.pkl\"), \"wb\"))\n","  pickle.dump(dict_enc, open(os.path.join(OUTPUT_DATA_DIR, f\"{name}_enc.pkl\"), \"wb\"))\n","  pickle.dump(dict_view_train, open(os.path.join(OUTPUT_DATA_DIR, f\"{name}_view_train.pkl\"), \"wb\"))\n","  pickle.dump(dict_view_test, open(os.path.join(OUTPUT_DATA_DIR, f\"{name}_view_test.pkl\"), \"wb\"))\n","  pickle.dump(y_train, open(os.path.join(OUTPUT_DATA_DIR, f\"{name}_train_y.pkl\"), \"wb\"))\n","  pickle.dump(y_test, open(os.path.join(OUTPUT_DATA_DIR, f\"{name}_test_y.pkl\"), \"wb\"))\n","\n","  build_list_graphs(dict_view_test, y_test, dict_enc, word2vec_dim, f'{name}_test.db', dynamic_attrs, static_attrs, relation, df_test[case_col].astype('string').values)\n","  build_list_graphs(dict_view_train, y_train, dict_enc, word2vec_dim, f'{name}_train.db', dynamic_attrs, static_attrs, relation, df_train[case_col].astype('string').values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8X9_SvUTPo2"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def train_fn(model, X_train_batch, y_train_batch, optimizer, criterion, pred_type: Literal['regression', 'classification'] = 'regression'):\n","        model.train()\n","        X_train_batch = X_train_batch.to(device)\n","        y_train_batch = y_train_batch.to(device)\n","\n","        optimizer.zero_grad()\n","        feature = {}\n","        feature = {n: X_train_batch.ndata[n][n] for n in X_train_batch.ntypes}\n","        #for n in X_train_batch.ntypes:\n","        #    feature[n] = X_train_batch.ndata[n][n]\n","        eweight = None\n","        y_train_pred = model(X_train_batch, feature, eweight)\n","\n","        if 'regression' in pred_type:\n","          y_train_pred = torch.flatten(y_train_pred)\n","          y_train_batch = torch.flatten(y_train_batch)\n","\n","        loss = criterion(y_train_pred, y_train_batch)\n","\n","        loss.backward()\n","        optimizer.step()\n","        return loss.item()\n","\n","def evaluate_fn(model, data_loader, criterion, device, pred_type: Literal['regression', 'classification'] = 'regression'):\n","    model.eval()\n","    epoch_loss = 0\n","    global y_pred\n","    global y_true\n","    y_pred = []\n","    y_true = []\n","    with torch.no_grad():\n","        for X, y in tqdm(data_loader):\n","            X = X.to(device)\n","            y = y.to(device)\n","            feature = {}\n","            for n in X.ntypes:\n","                feature[n] = X.ndata[n][n]\n","\n","            pred = model(X, feature)\n","\n","            if 'regression' == pred_type:\n","                pred = torch.flatten(pred)\n","                y = torch.flatten(y)\n","\n","            loss = criterion(pred, y)\n","\n","            if 'classification' == pred_type:\n","                pred = torch.nn.functional.softmax(pred, dim=-1)\n","\n","            y_pred.append(pred.detach().cpu().numpy())\n","            y_true.append(y.detach().cpu().numpy())\n","\n","            epoch_loss += loss.item()\n","    y_true = np.concatenate(y_true)\n","    y_pred = np.concatenate(y_pred)\n","\n","    metrics = {}\n","    if 'regression' == pred_type:\n","      metrics = evaluate_regression(y_true, y_pred)\n","    elif 'classification' == pred_type:\n","      metrics = evaluate_classification(y_true, y_pred)\n","\n","    np.save(os.path.join(OUTPUT_DATA_DIR, \"y_true.npy\"), y_true, allow_pickle=False)\n","    np.save(os.path.join(OUTPUT_DATA_DIR, \"y_pred.npy\"), y_pred, allow_pickle=False)\n","\n","    return epoch_loss / len(data_loader), metrics\n","\n","def train_gnn(model, train_data_loader, valid_data_loader, optimizer, epochs = 200, pred_type: Literal['regression', 'classification'] = 'regression'):\n","    best_valid_loss = float(\"inf\")\n","    early_stop_counter = 0\n","    patience = 10\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","    best_model = None\n","\n","    if 'regression' in pred_type:\n","      criterion = torch.nn.L1Loss()\n","    elif 'classification' in pred_type:\n","      criterion = torch.nn.CrossEntropyLoss()\n","    else:\n","      raise ValueError(f\"Invalid prediction type {pred_type}\")\n","\n","    for epoch in range(epochs):\n","        train_loss = 0\n","        for x_batch, y_batch in (pbar:= tqdm(train_data_loader)):\n","            batch_loss = train_fn(model.to(device), x_batch, y_batch, optimizer, criterion, pred_type)\n","            train_loss += batch_loss\n","            pbar.set_description(f\"Epoch Loss {epoch}: {batch_loss:.4f}\")\n","\n","        avg_train_loss = train_loss / len(train_data_loader)\n","        valid_loss, metrics = evaluate_fn(model.to(device), valid_data_loader, criterion, device, pred_type)\n","        scheduler.step(valid_loss)\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            early_stop_counter = 0  # Reset early stopping counter\n","            best_model = copy.deepcopy(model)\n","        else:\n","            early_stop_counter += 1\n","\n","        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {valid_loss:.4f}\", end=' ')\n","        print(metrics)\n","        if early_stop_counter >= patience:\n","            print(\"Validation loss hasn't improved for\", patience, \"epochs. Early stopping...\")\n","            break\n","    return best_valid_loss, best_model\n","\n","\n","def get_model(params, train_loader, pred_type: Literal['regression', 'classification'] = 'regression', n_classes=0):\n","    g, label = next(iter(train_loader))\n","    f_map = {ntype: (g.nodes[ntype[0]].data[ntype[0]].shape[1], g.nodes[ntype[2]].data[ntype[2]].shape[1]) for ntype in g.canonical_etypes}  # for SageConv\n","    if 'regression' == pred_type:\n","      model = HeteroRegressor(f_map, params, g.canonical_etypes).to(device)\n","    elif 'classification' == pred_type:\n","      model = HeteroClassifier(f_map, params, n_classes, g.canonical_etypes).to(device)\n","    else:\n","      raise ValueError(f\"Invalid prediction type {pred_type}\")\n","    return model\n","\n","def load_graphs_from_hdf5(filename):\n","    graphs = []\n","    label = []\n","    with h5py.File(filename, 'r') as f:\n","        for key in tqdm(f.keys()):\n","            pickled_graph = f[key][()]\n","            graph = pickle.loads(pickled_graph)\n","            graphs.append(graph['graph'])\n","            label.append(graph['label'])\n","    return graphs, label\n","\n","def prophet_train(name: str):\n","    # Load graphs\n","    print(\"Loading training split...\")\n","    X_train, y_train = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{name}_train.db\"))\n","    df_train = TextDataset(X_train, y_train)\n","    #file_train = h5py.File(os.path.join(OUTPUT_DATA_DIR, f\"{name}_train.db\"), 'r')\n","    #df_train = TextDatasetOnDisk(file_train)\n","    print(\"Loading test split...\")\n","    #file_test = h5py.File(os.path.join(OUTPUT_DATA_DIR, f\"{name}_test.db\"), 'r')\n","    #df_test = TextDatasetOnDisk(file_test, cache_size=20000, auto_prefetch=True)\n","    X_val, y_val = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{name}_test.db\"))\n","    df_test = TextDataset(X_val, y_val)\n","\n","    train_loader = dgl.dataloading.GraphDataLoader(df_train, batch_size=64, drop_last=False, shuffle=True)\n","    test_loader = dgl.dataloading.GraphDataLoader(df_test, batch_size=128, drop_last=False, shuffle=False)\n","\n","    params = {\n","      'hidden_dim': 64,\n","      'dropout': 0.1,\n","      'n_heads': 2,\n","      'n_layers': 4,\n","      'learning_rate': 0.001,\n","    }\n","\n","    model = get_model(params, train_loader)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n","\n","    score, best_model = train_gnn(model, train_loader, test_loader, optimizer, DEFAULT_EPOCHS)\n","\n","    #file_train.close()\n","    #file_test.close()\n","\n","    return score, best_model\n","\n","def prophet_train_classification(name: str, n_classes: int):\n","    # Load graphs\n","    print(\"Loading training split...\")\n","    X_train, y_train = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{name}_train.db\"))\n","    df_train = TextDataset(X_train, y_train)\n","    #file_train = h5py.File(os.path.join(OUTPUT_DATA_DIR, f\"{name}_train.db\"), 'r')\n","    #df_train = TextDatasetOnDisk(file_train)\n","    print(\"Loading test split...\")\n","    #file_test = h5py.File(os.path.join(OUTPUT_DATA_DIR, f\"{name}_test.db\"), 'r')\n","    #df_test = TextDatasetOnDisk(file_test, cache_size=20000, auto_prefetch=True)\n","    X_val, y_val = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{name}_test.db\"))\n","    df_test = TextDataset(X_val, y_val)\n","\n","    train_loader = dgl.dataloading.GraphDataLoader(df_train, batch_size=64, drop_last=False, shuffle=True)\n","    test_loader = dgl.dataloading.GraphDataLoader(df_test, batch_size=128, drop_last=False, shuffle=False)\n","\n","    params = {\n","      'hidden_dim': 64,\n","      'dropout': 0.1,\n","      'n_heads': 2,\n","      'n_layers': 4,\n","      'learning_rate': 0.001,\n","    }\n","\n","    model = get_model(params, train_loader, 'classification', n_classes)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n","\n","    score, best_model = train_gnn(model, train_loader, test_loader, optimizer, DEFAULT_EPOCHS, 'classification')\n","\n","    #file_train.close()\n","    #file_test.close()\n","\n","    return score, best_model\n","\n","def prophet_predict(model, graphs, label, pred_type: Literal['regression', 'classification'] = 'regression'):\n","  ds = TextDataset(graphs, label)\n","  loader = dgl.dataloading.GraphDataLoader(ds, batch_size=128, drop_last=False, shuffle=False)\n","\n","  y_pred = []\n","  y_true = []\n","\n","  model.eval()\n","  with torch.no_grad():\n","    for X, y in tqdm(loader):\n","      X = X.to(device)\n","      y = y.to(device)\n","      feature = {}\n","      for n in X.ntypes:\n","        feature[n] = X.ndata[n][n]\n","\n","      pred = model(X, feature)\n","      if 'classification' == pred_type:\n","        pred = torch.nn.functional.softmax(pred, dim=-1)\n","\n","      y_pred.append(pred.detach().cpu().numpy())\n","      y_true.append(y.detach().cpu().numpy())\n","\n","\n","  y_pred = np.concatenate(y_pred)\n","  if 'regression' == pred_type:\n","    y_pred = y_pred.flatten()\n","\n","  return np.concatenate(y_true).flatten(), y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTZMzpqP7Pm-"},"outputs":[],"source":["def prophet_replace_target(\n","    file_path: str,\n","    targets: list,\n","    name: str = \"\",\n","  ):\n","  with h5py.File(file_path, 'r+') as f:\n","    keys = list(f.keys())\n","    for key, target in tqdm(zip(keys, targets, strict=True), total=len(f.keys())):\n","      pickled_graph = f[key][()]\n","      unpickled_graph = pickle.loads(pickled_graph)\n","      unpickled_graph['label'] = target\n","      pickled_graph = pickle.dumps(unpickled_graph)\n","\n","      del f[key]\n","      f.create_dataset(key, data=np.void(pickled_graph))"]},{"cell_type":"markdown","metadata":{"id":"UNVWirPA_G5G"},"source":["# Dataset: Incident Management Process Enriched Event Log"]},{"cell_type":"markdown","metadata":{"id":"VkzmiuXjhXck"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EkiyeFmWWNi1"},"outputs":[],"source":["df_servicenow = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"incident_event_log_labeled.feather\"))\n","df_servicenow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vrzmhonc0ykt"},"outputs":[],"source":["df_servicenow_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"incident_event_log_train.feather\"))\n","df_servicenow_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0RgLDndalTR"},"outputs":[],"source":["df_servicenow_train_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"incident_event_log_train_dyn.feather\"))\n","df_servicenow_train_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBG4xeod02_p"},"outputs":[],"source":["df_servicenow_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"incident_event_log_test.feather\"))\n","df_servicenow_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhrapidfaoyT"},"outputs":[],"source":["df_servicenow_test_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"incident_event_log_test_dyn.feather\"))\n","df_servicenow_test_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0bg9U_UX5Ir"},"outputs":[],"source":["PROJECT_NAME = \"servicenow\"\n","ACTIVITY_VOCAB = df_servicenow[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]"]},{"cell_type":"markdown","metadata":{"id":"XUsdr8DnxFTg"},"source":["## Prepare PROPHET Graph"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"R87nSPHM36kJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"swAjjNJExEcN"},"outputs":[],"source":["prophet_generate_dgl_graph(\n","    df_servicenow_train,\n","    df_servicenow_train_dyn,\n","    df_servicenow_test,\n","    df_servicenow_test_dyn,\n","    dynamic_attrs=[EVENTLOG_ACTIVITY, EVENTLOG_RESOURCE, EVENTLOG_GROUP, \"category\", \"subcategory\", \"location\", \"u_symptom\", \"caller_id\", \"sys_updated_by\", \"contact_type\", \"sys_mod_count\", \"u_priority_confirmation\", \"knowledge\", \"priority\", \"reopen_count\", \"reassignment_count\", \"time:timestamp:elapsedprev:seconds\", \"time:timestamp:elapsedcycle:seconds\"],\n","    static_attrs=[\"case:notify\", \"case:opened_by\", \"case:sys_created_by\"],\n","    name=PROJECT_NAME\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aajUB8A36bpF"},"outputs":[],"source":["!gzip -c ./Data/Output/servicenow_test.db > ./Data/Output/servicenow_test.db.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOgfcGO46Omo"},"outputs":[],"source":["!gzip -c ./Data/Output/servicenow_train.db > ./servicenow_train.db.gz"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"r9Xu4YYC4LBG"}},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_test.db\")"],"metadata":{"id":"9nk5dW2C4LBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gunzip ./Data/Input/italy_train.db.gz -c > \"$new_train_file_path\"\n","!gunzip ./Data/Input/italy_test.db.gz -c > \"$new_test_file_path\""],"metadata":{"id":"nO0m6WtY4LBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_servicenow_train['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"meHIvD8V4LBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_servicenow_test['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"5sDYm2KR4LBI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"LfBkp3Sa4LBI"}},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_test.db\")"],"metadata":{"id":"A9Zb3j444LBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gunzip ./Data/Input/italy_train.db.gz -c > \"$new_train_file_path\"\n","!gunzip ./Data/Input/italy_test.db.gz -c > \"$new_test_file_path\""],"metadata":{"id":"4XKKtjtj4LBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_servicenow_train['label:concept:name:next'].to_numpy(dtype=int))\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"YjB0M2ic4LBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_servicenow_test['label:concept:name:next'].to_numpy(dtype=int))\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"zsUtIEO34LBK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nq_Mnhh2v2Qh"},"source":["## Train PROPHET Model"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"e-m4lIlC3_3n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0uJuc2mv2Qi"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pKo-E7bLv2Qj"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGlqZ8SOv2Qk"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"Ei3UwyIDhltZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMt8l904hlta"},"outputs":[],"source":["!gunzip ./Data/Input/servicenow_next_activity_train.db.gz -c > ./Data/Output/servicenow_train.db\n","!gunzip ./Data/Input/servicenow_next_activity_test.db.gz -c > ./Data/Output/servicenow_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeLBFeIHhltc"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train_classification(PROJECT_NAME, n_classes=len(ACTIVITY_VOCAB))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKxZ3I67hltc"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_65HrzgPhltd"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGqJZAbzhltd"},"outputs":[],"source":["files.download(f\"{PROJECT_NAME}_prophet_pred.npy\")\n","files.download(f\"{PROJECT_NAME}_prophet_true.npy\")"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"V-hHPkvY4gA0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_1HpFROHLl8"},"outputs":[],"source":["!gunzip ./Data/Input/servicenow_next_time_train.db.gz -c > ./Data/Output/servicenow_train.db\n","!gunzip ./Data/Input/servicenow_next_time_test.db.gz -c > ./Data/Output/servicenow_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gniE09YgHLl-"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqtSITudHLl_"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OKYtzs8HLl_"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOS2WNLmHLmA"},"outputs":[],"source":["files.download(f\"{PROJECT_NAME}_prophet_pred.npy\")\n","files.download(f\"{PROJECT_NAME}_prophet_true.npy\")"]},{"cell_type":"markdown","metadata":{"id":"kzxxUVrg93w6"},"source":["# Dataset: Dataset belonging to the help desk log of an Italian Company"]},{"cell_type":"markdown","metadata":{"id":"W8vyF1IuJaQ6"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zelIEb7kJZsp"},"outputs":[],"source":["df_italy = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"finale_labeled.feather\"))\n","df_italy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXyuW6AMtn4J"},"outputs":[],"source":["df_italy_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"Splits\", \"Italy\", \"finale_train.feather\"))\n","df_italy_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Trzfr-YtnxA"},"outputs":[],"source":["df_italy_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"Splits\", \"Italy\", \"finale_test.feather\"))\n","df_italy_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wAwEs9iiMDMd"},"outputs":[],"source":["df_italy_train_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"finale_train_dyn.feather\"))\n","df_italy_train_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OOA8amBLstr"},"outputs":[],"source":["df_italy_test_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"finale_test_dyn.feather\"))\n","df_italy_test_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlVOzDd9X1X_"},"outputs":[],"source":["PROJECT_NAME = \"italy\"\n","ACTIVITY_VOCAB = df_italy[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]"]},{"cell_type":"markdown","metadata":{"id":"Nnu_lMoIxfed"},"source":["## Prepare PROPHET Graph"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"FngLF9G7ujN-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7xo_26YxiAp"},"outputs":[],"source":["prophet_generate_dgl_graph(\n","    df_italy_train,\n","    df_italy_train_dyn,\n","    df_italy_test,\n","    df_italy_test_dyn,\n","    dynamic_attrs=[EVENTLOG_ACTIVITY, EVENTLOG_RESOURCE, EVENTLOG_GROUP, \"customer\", \"product\", \"service_type\", \"seriousness_2\", \"service_level\", \"time:timestamp:elapsedprev:seconds\", \"time:timestamp:elapsedcycle:seconds\"],\n","    static_attrs=[\"case:responsible_section\", \"case:support_section\"],\n","    name=PROJECT_NAME\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHntFqLyRATU"},"outputs":[],"source":["!gzip -c ./Data/Output/italy_test.db > ./Data/Output/italy_test.db.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQfZH4k-Q-sD"},"outputs":[],"source":["!gzip -c ./Data/Output/italy_train.db > ./Data/Output/italy_train.db.gz"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"7mxr1PckurFJ"}},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_test.db\")"],"metadata":{"id":"a0DunJoGurFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gunzip ./Data/Input/italy_train.db.gz -c > \"$new_train_file_path\"\n","!gunzip ./Data/Input/italy_test.db.gz -c > \"$new_test_file_path\""],"metadata":{"id":"T_UfwFQGurFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_italy_train['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"y-vdyhM9urFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_italy_test['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"sefw803_urFO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"1G0gk69CurFP"}},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_test.db\")"],"metadata":{"id":"QfjPjecjvGbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gunzip ./Data/Input/italy_train.db.gz -c > \"$new_train_file_path\"\n","!gunzip ./Data/Input/italy_test.db.gz -c > \"$new_test_file_path\""],"metadata":{"id":"n1nLKp-JurFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_italy_train['label:concept:name:next'].to_numpy(dtype=int))\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"9Q4y6HYPurFR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_italy_test['label:concept:name:next'].to_numpy(dtype=int))\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"ImaAaL3zurFR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1yLk3McPFrb"},"source":["## Train PROPHET Model"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"IUnyTg90xzBN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaLtim5ZPFre"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKLiJnLW7dT2"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzyNBDLnPFrf"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"xi6YL8Fqx2_s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2_eouh_x2_w"},"outputs":[],"source":["!gunzip ./Data/Input/italy_next_activity_train.db.gz -c > ./Data/Output/italy_train.db\n","!gunzip ./Data/Input/italy_next_activity_test.db.gz -c > ./Data/Output/italy_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLGmLES4x2_x"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train_classification(PROJECT_NAME, n_classes=len(ACTIVITY_VOCAB))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XPDJ6eA8x2_y"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQNw_KlUx2_z"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"al40uLrux2_0"},"outputs":[],"source":["files.download(f\"{PROJECT_NAME}_prophet_pred.npy\")\n","files.download(f\"{PROJECT_NAME}_prophet_true.npy\")"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"LF2PnfHRx2_0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQgW8rzbx2_1"},"outputs":[],"source":["!gunzip ./Data/Input/italy_next_time_train.db.gz -c > ./Data/Output/italy_train.db\n","!gunzip ./Data/Input/italy_next_time_test.db.gz -c > ./Data/Output/italy_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rvp-bbk6x2_1"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1z-ewrYtx2_2"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXZYIEIHx2_2"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYhI3irtx2_2"},"outputs":[],"source":["files.download(f\"{PROJECT_NAME}_prophet_pred.npy\")\n","files.download(f\"{PROJECT_NAME}_prophet_true.npy\")"]},{"cell_type":"markdown","metadata":{"id":"KwaK3XFhJ9DB"},"source":["# Dataset: BPIC 2014"]},{"cell_type":"markdown","metadata":{"id":"5MW9XP2uTYaD"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akH7W0GITXXI"},"outputs":[],"source":["df_bpic14 = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"Detail_Incident_Activity_labeled.feather\"))\n","df_bpic14"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ev5sSrdWBO-U"},"outputs":[],"source":["df_bpic14_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"Detail_Incident_Activity_train.feather\"))\n","df_bpic14_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBwR53HyA7pH"},"outputs":[],"source":["df_bpic14_train_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"Detail_Incident_Activity_train_dyn.feather\"))\n","df_bpic14_train_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uxsyLjkBOqX"},"outputs":[],"source":["df_bpic14_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"Detail_Incident_Activity_test.feather\"))\n","df_bpic14_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQndsIayA34I"},"outputs":[],"source":["df_bpic14_test_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"Detail_Incident_Activity_test_dyn.feather\"))\n","df_bpic14_test_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIHDeIMvsoBS"},"outputs":[],"source":["PROJECT_NAME = \"bpic2014\"\n","\n","ACTIVITY_VOCAB = df_bpic14[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]"]},{"cell_type":"markdown","metadata":{"id":"nwTSmDgr0CZ1"},"source":["## Prepare PROPHET Graph"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"91i4EsluouEj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8X2wpdBF0CZ4"},"outputs":[],"source":["prophet_generate_dgl_graph(\n","    df_bpic14_train,\n","    df_bpic14_train_dyn,\n","    df_bpic14_test,\n","    df_bpic14_test_dyn,\n","    dynamic_attrs=[EVENTLOG_ACTIVITY, EVENTLOG_GROUP, \"time:timestamp:elapsedprev:seconds\", \"time:timestamp:elapsedcycle:seconds\"],\n","    static_attrs=[\"case:KM number\", \"case:incident_Category\", \"case:incident_CI Type (aff)\", \"case:incident_CI Subtype (aff)\", \"case:incident_Service Component WBS (aff)\", \"case:incident_CI Name (CBy)\", \"case:incident_CI Type (CBy)\", \"case:incident_Priority\", \"case:interaction_Priority\"],\n","    name=PROJECT_NAME,\n",")"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"aFJHTNKVoz_Z"}},{"cell_type":"code","source":["!gunzip ./Data/Input/bpic2014_train.db.gz -c > ./Data/Output/bpic2014_train.db\n","!gunzip ./Data/Input/bpic2014_test.db.gz -c > ./Data/Output/bpic2014_test.db"],"metadata":{"id":"wt0uPIFblzzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_test.db\")"],"metadata":{"id":"GqEV578DpJMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shutil.copyfile(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_train.db\"), new_train_file_path)\n","shutil.copyfile(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"), new_test_file_path)"],"metadata":{"id":"kCu8fjfuo3oN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_bpic14_train['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"ku8uIpFy9k8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_bpic14_test['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"qxP2iN8HyBRC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"t33J8Z7Mp3tQ"}},{"cell_type":"code","source":["!gunzip ./Data/Input/bpic2014_train.db.gz -c > ./Data/Output/bpic2014_train.db\n","!gunzip ./Data/Input/bpic2014_test.db.gz -c > ./Data/Output/bpic2014_test.db"],"metadata":{"id":"igR8SYkRme9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_test.db\")"],"metadata":{"id":"zrknuoWYtW33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shutil.copyfile(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_train.db\"), new_train_file_path)\n","shutil.copyfile(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"), new_test_file_path)"],"metadata":{"id":"pSDJ-2uetW33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_bpic14_train['label:concept:name:next'].to_numpy(dtype=int))\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"zg86CVNGvXd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_bpic14_test['label:concept:name:next'].to_numpy(dtype=int))\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"2qN3JC8DvXd7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SPMpCGJYvzvR"},"source":["## Train PROPHET Model"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"hnFBYXSa5-W0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9MRjyady4A8"},"outputs":[],"source":["!gunzip ./Data/Input/bpic2014_train.db.gz -c > ./Data/Output/bpic2014_train.db\n","!gunzip ./Data/Input/bpic2014_test.db.gz -c > ./Data/Output/bpic2014_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRGAxeYVvzvS"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XePaGoiDvzvT"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvF7y_ATvzvU"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUwn7b-1vzvV"},"outputs":[],"source":["files.download(f\"{PROJECT_NAME}_prophet_pred.npy\")\n","files.download(f\"{PROJECT_NAME}_prophet_true.npy\")"]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"b1OD76RS6Ea1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uH-iBZvN6HVf"},"outputs":[],"source":["!gunzip ./Data/Input/bpic2014_next_activity_train.db.gz -c > ./Data/Output/bpic2014_train.db\n","!gunzip ./Data/Input/bpic2014_next_activity_test.db.gz -c > ./Data/Output/bpic2014_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPvphzPp6HVg"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train_classification(PROJECT_NAME, n_classes=len(ACTIVITY_VOCAB))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xMgLtOC6HVg"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H71PFH8w6HVh"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aur4uHrK6HVi"},"outputs":[],"source":["files.download(f\"{PROJECT_NAME}_prophet_pred.npy\")\n","files.download(f\"{PROJECT_NAME}_prophet_true.npy\")"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"NUbsDTMTXwyT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3vJnLmgXwyX"},"outputs":[],"source":["!gunzip ./Data/Input/bpic2014_next_time_train.db.gz -c > ./Data/Output/bpic2014_train.db\n","!gunzip ./Data/Input/bpic2014_next_time_test.db.gz -c > ./Data/Output/bpic2014_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YorQ7c3cXwyX"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flckzxAmXwyY"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjI8GR4eXwyY"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTjfMZlaXwyY"},"outputs":[],"source":["files.download(f\"{PROJECT_NAME}_prophet_pred.npy\")\n","files.download(f\"{PROJECT_NAME}_prophet_true.npy\")"]},{"cell_type":"markdown","metadata":{"id":"tYcrP6TFjucY"},"source":["# Dataset: Helpdesk"]},{"cell_type":"markdown","metadata":{"id":"0bsU8Djkmqce"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MK4iviICmp24"},"outputs":[],"source":["df_helpdesk = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"helpdesk_labeled.feather\"))\n","df_helpdesk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"So3lFH_xeh0i"},"outputs":[],"source":["df_helpdesk_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"helpdesk_train.feather\"))\n","df_helpdesk_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liSOkO4KedNI"},"outputs":[],"source":["df_helpdesk_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"Splits\", \"Helpdesk\", \"helpdesk_test.feather\"))\n","df_helpdesk_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yDkVQn-_VOb"},"outputs":[],"source":["df_helpdesk_train_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"helpdesk_train_dyn.feather\"))\n","df_helpdesk_train_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ds9lq7qd_VGX"},"outputs":[],"source":["df_helpdesk_test_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"helpdesk_test_dyn.feather\"))\n","df_helpdesk_test_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJJ_jXy7wll0"},"outputs":[],"source":["PROJECT_NAME = \"helpdesk\"\n","\n","ACTIVITY_VOCAB = df_helpdesk[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]"]},{"cell_type":"markdown","metadata":{"id":"33MtIo9TyawO"},"source":["## Prepare PROPHET Graph"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"5VPDgPb_rAB9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5YstfYxyawQ"},"outputs":[],"source":["prophet_generate_dgl_graph(\n","    df_helpdesk_train,\n","    df_helpdesk_train_dyn,\n","    df_helpdesk_test,\n","    df_helpdesk_test_dyn,\n","    dynamic_attrs=[EVENTLOG_ACTIVITY, \"time:timestamp:elapsedprev:seconds\", \"time:timestamp:elapsedcycle:seconds\"],\n","    static_attrs=[],\n","    name=PROJECT_NAME\n",")"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"wNQqmkRKrEfu"}},{"cell_type":"code","source":["!gunzip ./Data/Input/helpdesk_train.db.gz -c > ./Data/Output/helpdesk_train.db\n","!gunzip ./Data/Input/helpdesk_test.db.gz -c > ./Data/Output/helpdesk_test.db"],"metadata":{"id":"Uo_HoO-jwaxU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_test.db\")"],"metadata":{"id":"dnSNta9IrNOJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shutil.copyfile(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_train.db\"), new_train_file_path)\n","shutil.copyfile(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"), new_test_file_path)"],"metadata":{"id":"idXQ6CrJrNOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_helpdesk_train['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"vx8ECBfDrNON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_helpdesk_test['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"TLCqX5_qrNOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"ugaDDE0yrG5u"}},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_test.db\")"],"metadata":{"id":"_4-8n34hriDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shutil.copyfile(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_train.db\"), new_train_file_path)\n","shutil.copyfile(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"), new_test_file_path)"],"metadata":{"id":"CqL_WfQxriDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_helpdesk_train['label:concept:name:next'].to_numpy(dtype=int))\n","\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"ftpYjx8jriDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_helpdesk_test['label:concept:name:next'].to_numpy(dtype=int))\n","\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"LSSzsEuvrxEr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfG5jcmVU6n9"},"source":["## Train PROPHET Model"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"rsxy1V13x8yT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PKVwU5uU6Hg"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHQBPoFrM_Bv"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzfQRjh9OWie"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"sEYEh3dLdR4u"}},{"cell_type":"code","source":["!gunzip ./Data/Output/helpdesk_next_time_train.db.gz -c > ./Data/Output/helpdesk_train.db\n","!gunzip ./Data/Output/helpdesk_next_time_test.db.gz -c > ./Data/Output/helpdesk_test.db"],"metadata":{"id":"fF9zS8lmdbmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-0zKwjjeUmq"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zZ50ETTd9_g"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzlBvXBHd9_m"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"oHAk_VW3yAj8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lG0sCbhfyJZt"},"outputs":[],"source":["!gunzip ./Data/Output/helpdesk_next_activity_train.db.gz -c > ./Data/Output/helpdesk_train.db\n","!gunzip ./Data/Output/helpdesk_next_activity_test.db.gz -c > ./Data/Output/helpdesk_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TL6gGxjkyJZv"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train_classification(PROJECT_NAME, n_classes=len(ACTIVITY_VOCAB))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjnXvkgDyJZv"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels, 'classification')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNxNN-2YyJZw"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"markdown","metadata":{"id":"jTLykQfCSJai"},"source":["# Dataset: BPIC 2013"]},{"cell_type":"markdown","metadata":{"id":"o5mXj7gcSe9j"},"source":["## Input Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_5qnTRASIw4"},"outputs":[],"source":["df_bpic13 = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"BPI_Challenge_2013_incidents_labeled.feather\"))\n","df_bpic13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRNL1H78M5rB"},"outputs":[],"source":["df_bpic13_train = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"BPI_Challenge_2013_incidents_train.feather\"))\n","df_bpic13_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5y7KtrAhEEgm"},"outputs":[],"source":["df_bpic13_train_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"BPI_Challenge_2013_incidents_train_dyn.feather\"))\n","df_bpic13_train_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyQGjPISM5rD"},"outputs":[],"source":["df_bpic13_test = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"BPI_Challenge_2013_incidents_test.feather\"))\n","df_bpic13_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoQ0ER2cEE3e"},"outputs":[],"source":["df_bpic13_test_dyn = pd.read_feather(os.path.join(INPUT_DATA_DIR, \"CAiSE 2025\", \"BPI_Challenge_2013_incidents_test_dyn.feather\"))\n","df_bpic13_test_dyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mvixWwNSXBP"},"outputs":[],"source":["PROJECT_NAME = \"bpic2013\"\n","ACTIVITY_VOCAB = df_bpic13[EVENTLOG_ACTIVITY].unique().tolist() + [TOKEN_EOC]"]},{"cell_type":"markdown","metadata":{"id":"oFgKkfBiy77Y"},"source":["## Prepare PROPHET Graph"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"CidViajhW5Fm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5TaMcrry77Z"},"outputs":[],"source":["prophet_generate_dgl_graph(\n","    df_bpic13_train,\n","    df_bpic13_train_dyn,\n","    df_bpic13_test,\n","    df_bpic13_test_dyn,\n","    dynamic_attrs=[EVENTLOG_ACTIVITY, EVENTLOG_RESOURCE, EVENTLOG_GROUP, \"org:role\", \"Involved Org line 3\", \"Owner Country\", \"Status\", \"Sub Status\", \"time:timestamp:elapsedprev\", \"time:timestamp:elapsedcycle\"],\n","    static_attrs=[\"case:Product\", \"case:Country\", \"case:SR Latest Impact\"],\n","    name=PROJECT_NAME\n",")"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"gSZvxbeUXJOl"}},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_time_test.db\")"],"metadata":{"id":"LkZ1g12HXJOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gunzip ./Data/Input/bpic2013_train.db.gz -c > \"$new_train_file_path\"\n","!gunzip ./Data/Input/bpic2013_test.db.gz -c > \"$new_test_file_path\""],"metadata":{"id":"CCFxDgBrXJOn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_bpic13_train['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"aFZPySqtXJOn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_bpic13_test['label:time:timestamp:next'].to_numpy())\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"s2Gc64jAXJOo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Next Activity Prediction\n","\n","---\n","\n"],"metadata":{"id":"YDF4s5o6W9Xr"}},{"cell_type":"code","source":["new_train_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_train.db\")\n","new_test_file_path = os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_next_activity_test.db\")"],"metadata":{"id":"KzS0lBmhXg_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gunzip ./Data/Input/bpic2013_train.db.gz -c > \"$new_train_file_path\"\n","!gunzip ./Data/Input/bpic2013_test.db.gz -c > \"$new_test_file_path\""],"metadata":{"id":"BYzhpyOSXg_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_train_file_path, df_bpic13_train['label:concept:name:next'].to_numpy(dtype=int))\n","!gzip -c \"$new_train_file_path\" > \"$new_train_file_path\".gz"],"metadata":{"id":"45JfHhpYXg_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prophet_replace_target(new_test_file_path, df_bpic13_test['label:concept:name:next'].to_numpy(dtype=int))\n","!gzip -c \"$new_test_file_path\" > \"$new_test_file_path\".gz"],"metadata":{"id":"WFY1q2a6Xg_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIKDbuZSaEOB"},"source":["## Train PROPHET Model"]},{"cell_type":"markdown","source":["### Remaining Time Prediction"],"metadata":{"id":"mXD6QdOCXqK7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qVujl2GaHtO"},"outputs":[],"source":["!gunzip ./Data/Input/bpic13_train.db.gz\n","!mv ./Data/Input/bpic13_train.db ./Data/Output/bpic13_train.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IIdk-4daHtQ"},"outputs":[],"source":["!gunzip ./Data/Input/bpic13_test.db.gz\n","!mv ./Data/Input/bpic13_test.db ./Data/Output/bpic13_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XNqXyfJaEOC"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdX1DeHIaEOD"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRmlO79JaEOE"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"markdown","source":["### Next Time Prediction"],"metadata":{"id":"3DMmDhxqfMu4"}},{"cell_type":"code","source":["!gunzip ./Data/Input/bpic2013_next_time_train.db.gz -c > ./Data/Output/bpic2013_train.db\n","!gunzip ./Data/Input/bpic2013_next_time_test.db.gz -c > ./Data/Output/bpic2013_test.db"],"metadata":{"id":"WkyaCbrpfMu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7vn78s-fMu7"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train(PROJECT_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAzbwpGefMu8"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khftEod3fMu8"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"markdown","source":["### Next Activity Prediction"],"metadata":{"id":"nFTiGQAJfMu8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oROuURZJfMu9"},"outputs":[],"source":["!gunzip ./Data/Input/bpic2013_next_activity_train.db.gz -c > ./Data/Output/bpic2013_train.db\n","!gunzip ./Data/Input/bpic2013_next_activity_test.db.gz -c > ./Data/Output/bpic2013_test.db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6psr_BuqfMu-"},"outputs":[],"source":["%pdb off\n","best_loss, best_model = prophet_train_classification(PROJECT_NAME, n_classes=len(ACTIVITY_VOCAB))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcGV8ZhwfMu-"},"outputs":[],"source":["graphs, labels = load_graphs_from_hdf5(os.path.join(OUTPUT_DATA_DIR, f\"{PROJECT_NAME}_test.db\"))\n","y_true, y_pred = prophet_predict(best_model, graphs, labels, 'classification')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BnzLJDq7fMu_"},"outputs":[],"source":["np.save(f\"{PROJECT_NAME}_prophet_pred.npy\", y_pred, allow_pickle=False)\n","np.save(f\"{PROJECT_NAME}_prophet_true.npy\", y_true, allow_pickle=False)"]},{"cell_type":"markdown","metadata":{"id":"tEnsomiJlYF_"},"source":["# Data Export"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbnZaIXvlpIp"},"outputs":[],"source":["output_file = f\"results_{datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S%z')}.zip\"\n","\n","!zip -r \"$output_file\" \"$DATA_DIR\" \"$GRAPHIC_DIR\" \"$MODEL_DIR\""]},{"cell_type":"markdown","metadata":{"id":"jT5qK1yAleCQ"},"source":["## A: Export to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeUyaAkGmCts"},"outputs":[],"source":["drive.mount(\"/content/drive\")\n","\n","Path(GDRIVE_OUTPUT_DIR).mkdir(exist_ok=True)\n","\n","!cp \"$output_file\" \"$GDRIVE_OUTPUT_DIR\"\n","\n","drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"A6l3vuhklh7h"},"source":["## B: Download to Local Machine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBY3BvZTl3_0"},"outputs":[],"source":["files.download(output_file)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}